{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa3071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e750b2f9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8dae2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import nnx\n",
    "from jax import random\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f12893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flax version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import flax\n",
    "from flax.core import FrozenDict\n",
    "\n",
    "print(\"Flax version:\", flax.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f24af3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRT_CPU_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CpuDevice(id=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dev in jax.devices(): print(dev)\n",
    "\n",
    "try: # choose gpu if avaliable\n",
    "    device = jax.devices('gpu')[0]\n",
    "except: # otherwise default to cpu\n",
    "    device = jax.devices('cpu')[0]\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234448a",
   "metadata": {},
   "source": [
    "## 1. The Buffer-Stock Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bdf06f",
   "metadata": {},
   "source": [
    "$$\n",
    "v_t(m_t,p_t) = \\max_{a_t} u(c_t)+\\beta E_t[v_{t+1}(m_{t+1},p_{t+1})] \\\\\n",
    "\\text{s.t.} \\\\\n",
    "c_t = (1-a_t)m_t \\\\\n",
    "m_{t+1} = (1+r)(m_t-c_t) + \\text{income}_{t+1} \\\\\n",
    "\\text{income}_{t+1} = \\kappa_{t+1} \\psi_{t+1}p_{t+1} \\\\\n",
    "p_{t+1} = \\xi_{t+1}p_t^{\\rho} \\\\\n",
    "a_t \\in [0,1]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f8a11",
   "metadata": {},
   "source": [
    "$a_t$ = $\\pi(s_t)$ is approximated with neural network $\\pi(s_t) \\approx \\pi^{DNN}(t,s_t;\\theta_\\pi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451cec89",
   "metadata": {},
   "source": [
    "## 1. Create neural network using FLAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5792d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nnx.Module):\n",
    "\n",
    "  # layers should be flax compatible list: syntax for parameter updates in list\n",
    "  layers: list[nnx.Linear]\n",
    "  \n",
    "  def __init__(self, din: int, dout: int, neurons: list, rngs: nnx.Rngs):\n",
    "\n",
    "    # 1. initialize\n",
    "    layers = []\n",
    "\n",
    "    # 2. 1st layer\n",
    "    layers.append(nnx.Linear(din, neurons[0], rngs=rngs))\n",
    "    \n",
    "    # 3. hidden layers\n",
    "    for layer in range(len(neurons)-1):\n",
    "      \n",
    "      layers.append(nnx.Linear(neurons[layer], neurons[layer+1], rngs=rngs))\n",
    "\n",
    "    # 4. output layer\n",
    "    layers.append(nnx.Linear(neurons[-1], dout, rngs=rngs))\n",
    "\n",
    "    # 5. assign to neural network\n",
    "    self.layers = nnx.List(layers)\n",
    "\n",
    "  def __call__(self, x: jax.Array):\n",
    "\n",
    "    # 1. 1st and hiden layers\n",
    "    for layer in self.layers[:-1]:\n",
    "\n",
    "      x = nnx.relu(layer(x)) # ReLU activation for input and all hidden layers\n",
    "\n",
    "    # 2. output layer\n",
    "    layer = self.layers[-1] \n",
    "    y = jax.nn.sigmoid(layer(x)) # sigmoid activation for output layer\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224d4fa",
   "metadata": {},
   "source": [
    "We could include time as a continuous input in the network, i.e. $\\pi(s_t) \\approx \\pi^{DNN}(0,s_t;\\theta_\\pi)$ for evaluating the policy in $t=0$. However, in our experience, it is better handled as time dummies: $\\pi(s_t) \\approx \\pi^{DNN}((1,0,0,\\dots),s_t;\\theta_\\pi)$ for $t=0$.\n",
    "\n",
    "Implementing time dummies implies that we have to add $T$ extra inputs to the neural network and construct a wrapper function that easily makes time dummies, combines them with states, passes everything to the neural network and returns actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ec0a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_nn(model):\n",
    "  \"\"\"\n",
    "  Function that takes economic model object and returns suitable neural network\n",
    "  \"\"\"\n",
    "\n",
    "  par = model.par\n",
    "  train = model.train\n",
    "\n",
    "  # 1. unpack\n",
    "  T = par.get(\"T\")\n",
    "  Nstates = par.get(\"Nstates\")\n",
    "  Nactions = par.get(\"Nactions\")\n",
    "\n",
    "  # 2. compute in- and output dimensions and retrieve list of neurons\n",
    "  din = Nstates + T\n",
    "  dout = Nactions\n",
    "  neurons = train.get(\"neurons\")\n",
    "\n",
    "  # 3. call policy class\n",
    "  nn = Policy(din, dout, neurons, rngs=nnx.Rngs(params=0)) # last kwarg set seed=0 for bias and weight initialization\n",
    "\n",
    "  return nn\n",
    "\n",
    "def eval_policy(nn,x,t):\n",
    "  \"\"\"\n",
    "  Function that takes neural network, states and the time period and transforms time input to dummy variables and passes everything to neural network\n",
    "  \"\"\"\n",
    "\n",
    "  # 1. infer attributes of states\n",
    "  Nx = x.shape[0]\n",
    "  Nstates = x.shape[1]\n",
    "\n",
    "  # 2. infer total number of time periods from network structure\n",
    "  T = nn.layers[0].in_features - Nstates\n",
    "\n",
    "  # 3. construct time dummies and concat with states x\n",
    "  time_dummies = jax.nn.one_hot(t, T) # shape (T,)\n",
    "  time_dummies = jnp.broadcast_to(time_dummies, (Nx, T))\n",
    "  x = jnp.concatenate((x,time_dummies),axis=-1)\n",
    "  \n",
    "  # 4. pass to network\n",
    "  action = nn(x)\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ed6c4",
   "metadata": {},
   "source": [
    "### 1.1 Test policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a70eaab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [5,5,10] # 5 neurons in input and first hidden layer and 10 in second hidden layer\n",
    "din = 10 # number of states and time periods\n",
    "dout = 1 # number of actions\n",
    "\n",
    "neural_net_test = Policy(din, dout, hidden_layers, rngs=nnx.Rngs(params=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36f32a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.5701341]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. draw random states for 1 individual\n",
    "x_test = random.normal(random.key(0), shape=(1,din))\n",
    "\n",
    "neural_net_test(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec2292e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.5701341 ],\n",
       "       [0.562211  ],\n",
       "       [0.5975553 ],\n",
       "       [0.65444136],\n",
       "       [0.72693014],\n",
       "       [0.5660716 ],\n",
       "       [0.5       ],\n",
       "       [0.50856334],\n",
       "       [0.6697087 ],\n",
       "       [0.6374486 ],\n",
       "       [0.6104777 ],\n",
       "       [0.5677598 ],\n",
       "       [0.5970321 ],\n",
       "       [0.5883862 ],\n",
       "       [0.72699654],\n",
       "       [0.6646006 ],\n",
       "       [0.60491925],\n",
       "       [0.5688924 ],\n",
       "       [0.56430894],\n",
       "       [0.5246854 ]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. draw random states for 20 individuals\n",
    "N = 20\n",
    "x_test = random.normal(random.key(0), shape=(N,din))\n",
    "\n",
    "neural_net_test(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588f904",
   "metadata": {},
   "source": [
    "## 2. Create (economic) model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12d38550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "\n",
    "    par: dict\n",
    "    train: dict\n",
    "    sim: dict\n",
    "    policy: Policy\n",
    "    opt: nnx.Optimizer\n",
    "\n",
    "    def __init__(self, device, **kwargs):\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        par = {}\n",
    "        train = {}\n",
    "        sim = {}\n",
    "\n",
    "        # a. model\n",
    "        par[\"T\"] = 5                     # number of periods\n",
    "\n",
    "        # preferences\n",
    "        par[\"beta\"] = 1/1.01             # discount factor\n",
    "\n",
    "        # income\n",
    "        par[\"kappa_base\"] = 1.0\n",
    "        par[\"rho_p\"] = 0.95              # shock persistence\n",
    "        par[\"sigma_xi\"] = 0.1            # permanent shock std\n",
    "        par[\"sigma_psi\"] = 0.1           # transitory shock std\n",
    "\n",
    "        # return\n",
    "        par[\"R\"] = 1.01                  # gross return\n",
    "\n",
    "        # initial states\n",
    "        par[\"mu_m0\"] = 1.0               # initial cash-on-hand mean\n",
    "        par[\"sigma_m0\"] = 0.1            # initial cash-on-hand std\n",
    "\n",
    "        # initial permanent income\n",
    "        par[\"mu_p0\"] = 1.0               # initial permanent income mean\n",
    "        par[\"sigma_p0\"] = 0.1            # initial permanent income std\n",
    "\n",
    "        # b. solver settings\n",
    "        par[\"Nstates\"] = 2               # number of state variables\n",
    "        par[\"Nactions\"] = 1              # number of action variables\n",
    "        par[\"Nshocks\"] = 2               # number of shocks\n",
    "\n",
    "        # c. simulation \n",
    "        sim[\"N\"] = 50_000                # number of agents\n",
    "\n",
    "        # d. neural network and training\n",
    "        train[\"neurons\"] = [100, 100]     # number of neurons in hidden layers\n",
    "        train[\"N\"] = 3000                 # number of agents for training\n",
    "        train[\"seed\"] = 0                 # random seed\n",
    "        train[\"learning_rate_policy\"] = 1e-3   # learning rate for policy\n",
    "        train['K'] = 1000\n",
    "\n",
    "        # overwrite with specified parameters\n",
    "        for key, val in kwargs.items():\n",
    "            if key in par:\n",
    "                par[key] = val\n",
    "            elif key in train:\n",
    "                train[key] = val\n",
    "            elif key in sim:\n",
    "                sim[key] = val\n",
    "                \n",
    "        self.par = par\n",
    "        self.train = train\n",
    "        self.sim = sim\n",
    "        self.dtype = jnp.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f5777",
   "metadata": {},
   "source": [
    "### 2.1 Test policy through class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "400bdb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = Model(device, N=1000)\n",
    "model_nn_test =  setup_nn(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbac8540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.62973243]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = random.normal(random.key(0), shape=(1, model_test.par.get(\"Nstates\")))\n",
    "eval_policy(model_nn_test,x_test,t=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3c4b4",
   "metadata": {},
   "source": [
    "### 2.2 Test jitted policy through class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d146d1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.62973243]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jit(eval_policy)(model_nn_test,x_test,t=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c2186",
   "metadata": {},
   "source": [
    "## 3. Draw before runs (work-around to avoid JAX random keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf026bd",
   "metadata": {},
   "source": [
    "Draw training and test data for all $K$ iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d54b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_all(par, train, sim):\n",
    "    \n",
    "    # 1. set seed\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # 2. unpack\n",
    "    sigma_m0 = par['sigma_m0']\n",
    "    sigma_p0 = par['sigma_p0']\n",
    "    sigma_xi = par['sigma_xi']\n",
    "    sigma_psi = par['sigma_psi']\n",
    "    K = train['K']\n",
    "    T = par['T']\n",
    "    N = train['N']\n",
    "    N_sim = sim['N']\n",
    "\n",
    "    # 3. set global\n",
    "    global initial_states_train\n",
    "    global initial_states_test\n",
    "    global shocks_train\n",
    "    global shocks_test\n",
    "\n",
    "    # 4. draw training data\n",
    "    m0_train = np.exp(np.random.normal(-0.5*sigma_m0**2,sigma_m0,size=(K, N)))\n",
    "    p0_train = np.exp(np.random.normal(-0.5*sigma_p0**2,sigma_p0,size=(K, N)))\n",
    "\n",
    "    initial_states_np = np.stack((m0_train,p0_train),axis=-1)\n",
    "    initial_states_train = jnp.array(initial_states_np)\n",
    "\n",
    "    psi_train = np.exp(np.random.normal(-0.5*sigma_psi**2,sigma_psi,size=(K, T, N)))\n",
    "    xi_train = np.exp(np.random.normal(-0.5*sigma_xi**2,sigma_xi,size=(K, T, N)))\n",
    "\n",
    "    shocks_np = np.stack((psi_train,xi_train),axis=-1)\n",
    "    shocks_train = jnp.array(shocks_np)\n",
    "\n",
    "    # 5. draw test data\n",
    "    m0_train = np.exp(np.random.normal(-0.5*sigma_m0**2,sigma_m0,size=(N_sim,)))\n",
    "    p0_train = np.exp(np.random.normal(-0.5*sigma_p0**2,sigma_p0,size=(N_sim,)))\n",
    "\n",
    "    initial_states_np = np.stack((m0_train,p0_train),axis=-1)\n",
    "    initial_states_test = jnp.array(initial_states_np)\n",
    "\n",
    "    psi_train = np.exp(np.random.normal(-0.5*sigma_psi**2,sigma_psi,size=(T, N_sim)))\n",
    "    xi_train = np.exp(np.random.normal(-0.5*sigma_xi**2,sigma_xi,size=(T, N_sim)))\n",
    "\n",
    "    shocks_np = np.stack((psi_train,xi_train),axis=-1)\n",
    "    shocks_test = jnp.array(shocks_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ec3c6",
   "metadata": {},
   "source": [
    "## 4. Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e12f8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcomes(states,actions):\n",
    "\t\"\"\" outcomes - just consumption here \"\"\"\n",
    "\n",
    "\tm = states[...,0] # cash-on-hand\n",
    "\ta = actions[...,0] # savings rate\n",
    "\tc = m*(1-a) # consumption\n",
    "\n",
    "\treturn jnp.stack((c,),axis=-1) # (T,N,Noutcomes)\n",
    "\n",
    "def state_trans(par,states,outcomes,shocks):\n",
    "\t\"\"\" transition to future state \"\"\"\n",
    "\n",
    "\t# a. unpack\n",
    "\txi = shocks[...,0] # permanent income shock\n",
    "\tpsi = shocks[...,1] # transitory income shock\n",
    "\tm = states[...,0]\n",
    "\tp = states[...,1]\n",
    "\t\n",
    "\t# b. outcomes\n",
    "\tc = outcomes[...,0]\n",
    "\n",
    "\t# c. post-decision\n",
    "\tm_pd = m-c\n",
    "\t\n",
    "    # d. persistent income\n",
    "\tp_plus = p**par['rho_p'] * xi # permanent income\n",
    "\t\n",
    "    # e. income\n",
    "\tincome = par['kappa_base'] * p_plus * psi # income\n",
    "\t\n",
    "    # f. future cash-on-hand\n",
    "\tm_plus = par['R'] * m_pd + income # future cash-on-hand\n",
    "\n",
    "\t# g. finalize\n",
    "\tstates_pd = jnp.stack((m_plus,p_plus),axis=-1)\n",
    "\treturn states_pd\n",
    "\n",
    "def utility(c):\n",
    "\t\"\"\" utility \"\"\"\n",
    "\n",
    "\t#c_ = jnp.clip(c, 1e-2, None) # avoid log(0)\n",
    "\n",
    "\treturn jnp.log(c)\n",
    "\n",
    "def reward(outcomes):\n",
    "\t\"\"\" reward \"\"\"\n",
    "\n",
    "\t# a. consumption\n",
    "\tc = outcomes[...,0]\n",
    "\n",
    "\t# b. utility\n",
    "\tu = utility(c)\n",
    "\n",
    "\treturn u "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae65622",
   "metadata": {},
   "source": [
    "## 5. Simulate loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1186aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_loss(par, nn, initial_states, shocks):\n",
    "\n",
    "    # 1. define compiler-friendly function to be used inside every loop step\n",
    "    def scan_step(states_t, t):\n",
    "        \"\"\"One period transition in simulation.\"\"\"\n",
    "\n",
    "        # 1.1 policy\n",
    "        actions_t = eval_policy(nn, states_t, t)\n",
    "\n",
    "        # 1.2 outcomes & reward\n",
    "        outcomes_t = outcomes(states_t, actions_t)\n",
    "        reward_t = reward(outcomes_t)\n",
    "\n",
    "        # 1.3 Transition to next state\n",
    "        next_states = state_trans(par, states_t, outcomes_t, shocks[t])\n",
    "\n",
    "        # 1.4 return next period states for next iteration in loop and return rewards to be saved\n",
    "        return next_states, (reward_t,)\n",
    "\n",
    "    # 2. construct manual iterator\n",
    "    ts = jnp.arange(par['T'])\n",
    "\n",
    "    # 3. call scanner and pass initial states as first states_t and manual iterator\n",
    "    _, (reward_seq,) = jax.lax.scan(scan_step, initial_states, ts)\n",
    "\n",
    "    # 4. discount rewards\n",
    "    discounts = par['beta']**jnp.arange(par['T'])\n",
    "    discounted_sum = jnp.sum(discounts[:, None] * reward_seq, axis=0)\n",
    "\n",
    "    # 5. Average objective over training samples\n",
    "    loss = -jnp.mean(discounted_sum)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c9811",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b5115cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(par, nn, initial_states, shocks):\n",
    "\n",
    "    # 1. construct inner function that only takes neural network as argument\n",
    "    def loss_fn(nn_):\n",
    "        return simulate_loss(par, nn_, initial_states, shocks)\n",
    "\n",
    "    # 2. compute loss and gradients wrt. parameters\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(nn)\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "def train_policy(model):\n",
    "\n",
    "    # 1. unpack\n",
    "    par = FrozenDict(model.par) # freeze dict such that it can be passed around inside jitted functions\n",
    "    train = model.train\n",
    "    sim = model.sim\n",
    "\n",
    "    # 2. start timer\n",
    "    t0 = time.perf_counter()\n",
    "    train['times'] = jnp.zeros(train['K']) + jnp.nan\n",
    "\n",
    "    # 3. draw all initial states and shocks (globally)\n",
    "    draw_all(par, train, sim)\n",
    "    \n",
    "    # 4. create optimizer and neural network\n",
    "    nn = setup_nn(model)\n",
    "    policy_opt = nnx.ModelAndOptimizer(nn, optax.adam(learning_rate=train['learning_rate_policy']))\n",
    "    \n",
    "    # 5. jit training step\n",
    "    train_step_jit = jax.jit(train_step, static_argnums=(0))\n",
    "    \n",
    "    # 6. training loop\n",
    "    for k in range(train['K']):\n",
    "\n",
    "        # 6.1. unpack initial states and shocks for this interation (remember they are global variables)\n",
    "        initial_states = initial_states_train[k]\n",
    "        shocks = shocks_train[k]\n",
    "\n",
    "        # 6.2. training step\n",
    "        loss, grads = train_step_jit(par, nn, initial_states, shocks)\n",
    "\n",
    "        # 6.3. update neural network parameters based on gradients\n",
    "        policy_opt.update(grads)\n",
    "\n",
    "        # 6.4. print progress\n",
    "        if k % 10 == 0:\n",
    "            print(f\"Iteration {k}: Loss {loss.item()}\")\n",
    "\n",
    "        # 6.5. time iterations\n",
    "        train['times'] = train['times'].at[k].set(time.perf_counter()-t0)\n",
    "\n",
    "Model.train_policy = train_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c997ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(device, N=1000, T=5, K=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6bd475aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss 1.4694324731826782\n",
      "Iteration 10: Loss 0.29048025608062744\n",
      "Iteration 20: Loss 0.1550261527299881\n",
      "Iteration 30: Loss 0.11706811934709549\n",
      "Iteration 40: Loss 0.11824722588062286\n",
      "Iteration 50: Loss 0.06109444797039032\n",
      "Iteration 60: Loss 0.12793615460395813\n",
      "Iteration 70: Loss 0.11850355565547943\n",
      "Iteration 80: Loss 0.07880781590938568\n",
      "Iteration 90: Loss 0.07671257108449936\n",
      "Iteration 100: Loss 0.04370598495006561\n",
      "Iteration 110: Loss 0.07431963831186295\n",
      "Iteration 120: Loss 0.06638626754283905\n",
      "Iteration 130: Loss 0.08880484849214554\n",
      "Iteration 140: Loss 0.06633350998163223\n",
      "Iteration 150: Loss 0.06561187654733658\n",
      "Iteration 160: Loss 0.11270808428525925\n",
      "Iteration 170: Loss 0.1200365200638771\n",
      "Iteration 180: Loss 0.07401444762945175\n",
      "Iteration 190: Loss 0.10888688266277313\n",
      "Iteration 200: Loss 0.09709599614143372\n",
      "Iteration 210: Loss 0.07013393938541412\n",
      "Iteration 220: Loss 0.09133089333772659\n",
      "Iteration 230: Loss 0.09908413887023926\n",
      "Iteration 240: Loss 0.10418576002120972\n",
      "Iteration 250: Loss 0.0760338306427002\n",
      "Iteration 260: Loss 0.1046496257185936\n",
      "Iteration 270: Loss 0.10119296610355377\n",
      "Iteration 280: Loss 0.07924816757440567\n",
      "Iteration 290: Loss 0.06129422411322594\n",
      "Iteration 300: Loss 0.035378359258174896\n",
      "Iteration 310: Loss 0.08602346479892731\n",
      "Iteration 320: Loss 0.08475448191165924\n",
      "Iteration 330: Loss 0.07453718781471252\n",
      "Iteration 340: Loss 0.08079101890325546\n",
      "Iteration 350: Loss 0.10125129669904709\n",
      "Iteration 360: Loss 0.10268623381853104\n",
      "Iteration 370: Loss 0.08596251904964447\n",
      "Iteration 380: Loss 0.08188322931528091\n",
      "Iteration 390: Loss 0.07366866618394852\n",
      "Iteration 400: Loss 0.0711783766746521\n",
      "Iteration 410: Loss 0.12889225780963898\n",
      "Iteration 420: Loss 0.07737784832715988\n",
      "Iteration 430: Loss 0.11080542206764221\n",
      "Iteration 440: Loss 0.09032329171895981\n",
      "Iteration 450: Loss 0.05907084792852402\n",
      "Iteration 460: Loss 0.09785290062427521\n",
      "Iteration 470: Loss 0.0885263979434967\n",
      "Iteration 480: Loss 0.10038341581821442\n",
      "Iteration 490: Loss 0.09264472126960754\n",
      "Iteration 500: Loss 0.09076821058988571\n",
      "Iteration 510: Loss 0.07346037030220032\n",
      "Iteration 520: Loss 0.04957522824406624\n",
      "Iteration 530: Loss 0.08766742795705795\n",
      "Iteration 540: Loss 0.10922844707965851\n",
      "Iteration 550: Loss 0.09510432928800583\n",
      "Iteration 560: Loss 0.0644979402422905\n",
      "Iteration 570: Loss 0.09580037742853165\n",
      "Iteration 580: Loss 0.07404079288244247\n",
      "Iteration 590: Loss 0.09237547963857651\n",
      "Iteration 600: Loss 0.07786710560321808\n",
      "Iteration 610: Loss 0.037897758185863495\n",
      "Iteration 620: Loss 0.07011039555072784\n",
      "Iteration 630: Loss 0.036721982061862946\n",
      "Iteration 640: Loss 0.11245256662368774\n",
      "Iteration 650: Loss 0.11034973710775375\n",
      "Iteration 660: Loss 0.1009150817990303\n",
      "Iteration 670: Loss 0.0855037197470665\n",
      "Iteration 680: Loss 0.10252569615840912\n",
      "Iteration 690: Loss 0.1097414419054985\n",
      "Iteration 700: Loss 0.09277743101119995\n",
      "Iteration 710: Loss 0.0807923674583435\n",
      "Iteration 720: Loss 0.07640715688467026\n",
      "Iteration 730: Loss 0.08297547698020935\n",
      "Iteration 740: Loss 0.10557100921869278\n",
      "Iteration 750: Loss 0.10524488240480423\n",
      "Iteration 760: Loss 0.08393307030200958\n",
      "Iteration 770: Loss 0.06247136741876602\n",
      "Iteration 780: Loss 0.08962038159370422\n",
      "Iteration 790: Loss 0.08707232028245926\n",
      "Iteration 800: Loss 0.08688753098249435\n",
      "Iteration 810: Loss 0.10508923977613449\n",
      "Iteration 820: Loss 0.06309740990400314\n",
      "Iteration 830: Loss 0.054229672998189926\n",
      "Iteration 840: Loss 0.04071289300918579\n",
      "Iteration 850: Loss 0.046307336539030075\n",
      "Iteration 860: Loss 0.07249745726585388\n",
      "Iteration 870: Loss 0.044928066432476044\n",
      "Iteration 880: Loss 0.08841482549905777\n",
      "Iteration 890: Loss 0.08644913136959076\n",
      "Iteration 900: Loss 0.08943198621273041\n",
      "Iteration 910: Loss 0.07276193052530289\n",
      "Iteration 920: Loss 0.06612598896026611\n",
      "Iteration 930: Loss 0.08523496240377426\n",
      "Iteration 940: Loss 0.08141769468784332\n",
      "Iteration 950: Loss 0.09224127978086472\n",
      "Iteration 960: Loss 0.03156432509422302\n",
      "Iteration 970: Loss 0.1133168637752533\n",
      "Iteration 980: Loss 0.052437327802181244\n",
      "Iteration 990: Loss 0.051988162100315094\n"
     ]
    }
   ],
   "source": [
    "model.train_policy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
