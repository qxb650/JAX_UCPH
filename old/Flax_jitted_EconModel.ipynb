{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e8dae2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import nnx\n",
    "from jax import random\n",
    "\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "import time\n",
    "from EconModel import EconModelClass, jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "89f12893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flax version: 0.12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import flax\n",
    "from flax.core import FrozenDict\n",
    "\n",
    "print(\"Flax version:\", flax.__version__)\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451cec89",
   "metadata": {},
   "source": [
    "## 1. Create neural network using FLAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f5792d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nnx.Module):\n",
    "  layers: list[nnx.Linear]\n",
    "  \n",
    "  def __init__(self, din: int, dout: int, neurons: list, rngs: nnx.Rngs):\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    # 1st layer\n",
    "    layers.append(nnx.Linear(din, neurons[0], rngs=rngs))\n",
    "    \n",
    "    # hidden layers\n",
    "    for layer in range(len(neurons)-1):\n",
    "      \n",
    "      #setattr(self, f\"layer{layer+2}\", nnx.Linear(neurons[layer], neurons[layer+1], rngs=rngs))\n",
    "      layers.append(nnx.Linear(neurons[layer], neurons[layer+1], rngs=rngs))\n",
    "\n",
    "    # last layer\n",
    "    layers.append(nnx.Linear(neurons[-1], dout, rngs=rngs))\n",
    "\n",
    "    self.layers = nnx.List(layers)\n",
    "\n",
    "    #self.din, self.dout, self.hidden_layers = din, dout, neurons\n",
    "  \n",
    "  def __call__(self, x: jax.Array):\n",
    "\n",
    "    # 1st + hiden layers\n",
    "    for layer in self.layers[:-1]:\n",
    "\n",
    "      # unpack layer\n",
    "      #layer = getattr(self, f\"layer{i+1}\")\n",
    "\n",
    "      # forward x\n",
    "      x = nnx.relu(layer(x))\n",
    "\n",
    "    # last layer\n",
    "    layer = self.layers[-1]\n",
    "\n",
    "    y = jax.nn.sigmoid(layer(x))\n",
    "\n",
    "    return y\n",
    "\n",
    "def setup_nn(model):\n",
    "\n",
    "  par = model.par\n",
    "  train = model.sol\n",
    "\n",
    "  T = par.T\n",
    "  Nstates = par.Nstates\n",
    "  Nactions = par.Nactions\n",
    "\n",
    "  din = Nstates + T\n",
    "  dout = Nactions\n",
    "  neurons = train.neurons\n",
    "\n",
    "  nn = Policy(din, dout, neurons, rngs=nnx.Rngs(params=0))\n",
    "\n",
    "  return nn\n",
    "\n",
    "def eval_policy(nn,x,t):\n",
    "\n",
    "  # time dummies\n",
    "  Nx = x.shape[0]\n",
    "  Nstates = x.shape[1]\n",
    "  T = nn.layers[0].in_features - Nstates\n",
    "  \n",
    "  time_dummies = jax.nn.one_hot(t, T) # shape (T,)\n",
    "  time_dummies = jnp.broadcast_to(time_dummies, (Nx, T))\n",
    "  \n",
    "  x = jnp.concatenate((x,time_dummies),axis=-1)\n",
    "  \n",
    "  # evaluate\n",
    "  action = nn(x)\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ed6c4",
   "metadata": {},
   "source": [
    "### Test policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "36f32a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.5598746]], dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# direcly evaluate Policy network\n",
    "hidden_layers = [5,5,10]\n",
    "din = 10\n",
    "dout = 1\n",
    "\n",
    "model = Policy(din, dout, hidden_layers, rngs=nnx.Rngs(params=0))\n",
    "model(x=jnp.ones((1, din)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588f904",
   "metadata": {},
   "source": [
    "## 2. Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "12d38550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "\n",
    "    par: dict\n",
    "    train: dict\n",
    "    sim: dict\n",
    "    policy: Policy\n",
    "    opt: nnx.Optimizer\n",
    "\n",
    "    def __init__(self, N=None, T=None):\n",
    "\n",
    "        par = {}\n",
    "        train = {}\n",
    "        sim = {}\n",
    "\n",
    "        # a. model\n",
    "        par[\"T\"] = 5                     # number of periods\n",
    "\n",
    "        # preferences\n",
    "        par[\"beta\"] = 1/1.01             # discount factor\n",
    "\n",
    "        # income\n",
    "        par[\"kappa_base\"] = 1.0\n",
    "        par[\"rho_p\"] = 0.95              # shock persistence\n",
    "        par[\"sigma_xi\"] = 0.1            # permanent shock std\n",
    "        par[\"sigma_psi\"] = 0.1           # transitory shock std\n",
    "\n",
    "        # return\n",
    "        par[\"R\"] = 1.01                  # gross return\n",
    "\n",
    "        # b. solver settings\n",
    "        par[\"Nstates\"] = 2               # number of state variables\n",
    "        par[\"Nactions\"] = 1              # number of action variables\n",
    "        par[\"Nshocks\"] = 2               # number of shocks\n",
    "\n",
    "        # c. simulation \n",
    "        sim = {}\n",
    "        sim[\"N\"] = 50_000                # number of agents\n",
    "\n",
    "        # initial states\n",
    "        par[\"mu_m0\"] = 1.0               # initial cash-on-hand mean\n",
    "        par[\"sigma_m0\"] = 0.1            # initial cash-on-hand std\n",
    "\n",
    "        # initial permanent income\n",
    "        par[\"mu_p0\"] = 1.0               # initial permanent income mean\n",
    "        par[\"sigma_p0\"] = 0.1            # initial permanent income std\n",
    "\n",
    "        if T: par['T'] = T\n",
    "\n",
    "        self.par = par\n",
    "\n",
    "        sim['N'] = 100_000\n",
    "\n",
    "        self.sim = sim\n",
    "\n",
    "        # a. neural network\n",
    "\n",
    "        train[\"neurons\"] = [100, 100]     # number of neurons in hidden layers\n",
    "        train[\"N\"] = 3000                 # number of agents for training\n",
    "        train[\"seed\"] = 0                 # random seed\n",
    "        train[\"learning_rate_policy\"] = 1e-3   # learning rate for policy\n",
    "        train['K'] = 1000\n",
    "\n",
    "        if N: train['N'] = N\n",
    "\n",
    "        self.train = train\n",
    "        self.dtype = jnp.float32\n",
    "        self.device = jax.devices(\"cpu\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7253bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(EconModelClass):\n",
    "    \n",
    "    # __init__ is inherited from EconModelClass\n",
    "    \n",
    "    def settings(self): # required\n",
    "        \"\"\" choose settings \"\"\"\n",
    "            \n",
    "        pass # nothing chosen here\n",
    "    \n",
    "    def setup(self): # required\n",
    "        \"\"\" set free parameters \"\"\"\n",
    "        \n",
    "        par = self.par\n",
    "        train = self.sol\n",
    "        sim = self.sim\n",
    "\n",
    "        par.T = 5                     # number of periods\n",
    "\n",
    "        # preferences\n",
    "        par.beta = 1/1.01             # discount factor\n",
    "\n",
    "        # income\n",
    "        par.kappa_base = 1.0\n",
    "        par.rho_p = 0.95              # shock persistence\n",
    "        par.sigma_xi = 0.1            # permanent shock std\n",
    "        par.sigma_psi = 0.1           # transitory shock std\n",
    "\n",
    "        # return\n",
    "        par.R = 1.01                  # gross return\n",
    "\n",
    "        # solver settings\n",
    "        par.Nstates = 2               # number of state variables\n",
    "        par.Nactions = 1              # number of action variables\n",
    "        par.Nshocks = 2               # number of shocks\n",
    "\n",
    "        par.mu_m0 = 1.0        # initial cash-on-hand mean\n",
    "        par.sigma_m0 = 0.1     # initial cash-on-hand std\n",
    "\n",
    "        # initial permanent income\n",
    "        par.mu_p0 = 1.0        # initial permanent income mean\n",
    "        par.sigma_p0 = 0.1     # initial permanent income std\n",
    "\n",
    "        train.neurons = [100, 100]            # number of neurons in hidden layers\n",
    "        train.N = 3000                        # number of agents for training\n",
    "        train.seed = 0                        # random seed\n",
    "        train.learning_rate_policy = 1e-3     # learning rate for policy\n",
    "        train.K = 1000\n",
    "\n",
    "        sim.N = 10_000\n",
    "\n",
    "    def allocate(self): # required\n",
    "        \"\"\" set compound parameters and allocate arrays \"\"\"\n",
    "        \n",
    "        train = self.sol\n",
    "        \n",
    "        train.initial_states_train, train.shocks_train = self.draw_all()\n",
    "        train.iter_time = np.zeros(train.K) + np.nan\n",
    "\n",
    "    def solve(self):\n",
    "\n",
    "        with jit(self) as model_jit:\n",
    "            model_jit.train_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0648305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_all(self):\n",
    "    \n",
    "    par = self.par\n",
    "    train = self.sol\n",
    "    sim = self.sim\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    sigma_m0 = par.sigma_m0\n",
    "    sigma_p0 = par.sigma_p0\n",
    "    sigma_xi = par.sigma_xi\n",
    "    sigma_psi = par.sigma_psi\n",
    "\n",
    "    K = train.K\n",
    "    T = par.T\n",
    "    N = train.N\n",
    "\n",
    "    N_sim = sim.N\n",
    "\n",
    "    # set global\n",
    "    #global initial_states_train\n",
    "    #global initial_states_test\n",
    "    #global shocks_train\n",
    "    #global shocks_test\n",
    "\n",
    "    # draw training data\n",
    "    m0_train = np.exp(np.random.normal(-0.5*sigma_m0**2,sigma_m0,size=(K, N)))\n",
    "    p0_train = np.exp(np.random.normal(-0.5*sigma_p0**2,sigma_p0,size=(K, N)))\n",
    "\n",
    "    initial_states_np = np.stack((m0_train,p0_train),axis=-1)\n",
    "    initial_states_train = jnp.array(initial_states_np)\n",
    "\n",
    "    psi_train = np.exp(np.random.normal(-0.5*sigma_psi**2,sigma_psi,size=(K, T, N)))\n",
    "    xi_train = np.exp(np.random.normal(-0.5*sigma_xi**2,sigma_xi,size=(K, T, N)))\n",
    "\n",
    "    shocks_np = np.stack((psi_train,xi_train),axis=-1)\n",
    "    shocks_train = jnp.array(shocks_np)\n",
    "\n",
    "    # draw test data\n",
    "    m0_train = np.exp(np.random.normal(-0.5*sigma_m0**2,sigma_m0,size=(N_sim,)))\n",
    "    p0_train = np.exp(np.random.normal(-0.5*sigma_p0**2,sigma_p0,size=(N_sim,)))\n",
    "\n",
    "    initial_states_np = np.stack((m0_train,p0_train),axis=-1)\n",
    "    initial_states_test = jnp.array(initial_states_np)\n",
    "\n",
    "    psi_train = np.exp(np.random.normal(-0.5*sigma_psi**2,sigma_psi,size=(T, N_sim)))\n",
    "    xi_train = np.exp(np.random.normal(-0.5*sigma_xi**2,sigma_xi,size=(T, N_sim)))\n",
    "\n",
    "    shocks_np = np.stack((psi_train,xi_train),axis=-1)\n",
    "    shocks_test = jnp.array(shocks_np)\n",
    "\n",
    "    return initial_states_train, shocks_train\n",
    "\n",
    "MyModel.draw_all = draw_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f5777",
   "metadata": {},
   "source": [
    "### Test policy through class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bbac8540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.9248638]], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel()\n",
    "nn =  setup_nn(model)\n",
    "\n",
    "x = 20*jnp.ones((1,model.par.Nstates))\n",
    "eval_policy(nn,x,t=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3c4b4",
   "metadata": {},
   "source": [
    "### Test jitted policy through class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d146d1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.9248638]], dtype=float32)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jit(eval_policy)(nn,x,t=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ec3c6",
   "metadata": {},
   "source": [
    "## 4. Model functions and simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e12f8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcomes(states,actions):\n",
    "\t\"\"\" outcomes - just consumption here \"\"\"\n",
    "\n",
    "\tm = states[...,0] # cash-on-hand\n",
    "\ta = actions[...,0] # savings rate\n",
    "\tc = m*(1-a) # consumption\n",
    "\n",
    "\treturn jnp.stack((c,),axis=-1) # (T,N,Noutcomes)\n",
    "\n",
    "def state_trans(par,states,outcomes,shocks):\n",
    "\t\"\"\" transition to future state \"\"\"\n",
    "\n",
    "\t# a. unpack\n",
    "\txi = shocks[...,0] # permanent income shock\n",
    "\tpsi = shocks[...,1] # transitory income shock\n",
    "\tm = states[...,0]\n",
    "\tp = states[...,1]\n",
    "\tc = outcomes[...,0]\n",
    "\n",
    "\t# c. post-decision\n",
    "\tm_pd = m-c\n",
    "\t\n",
    "    # d. persistent income\n",
    "\tp_plus = p**par.rho_p * xi # permanent income\n",
    "\t\n",
    "    # d. income\n",
    "\tincome = par.kappa_base * p_plus * psi # income\n",
    "\t\n",
    "    # e. future cash-on-hand\n",
    "\tm_plus = par.R * m_pd + income # future cash-on-hand\n",
    "\n",
    "\t# d. finalize\n",
    "\tstates_pd = jnp.stack((m_plus,p_plus),axis=-1)\n",
    "\treturn states_pd\n",
    "\n",
    "def utility(c):\n",
    "\t\"\"\" utility \"\"\"\n",
    "\n",
    "\t#c_ = jnp.clip(c, 1e-2, None) # avoid log(0)\n",
    "\n",
    "\treturn jnp.log(c)\n",
    "\n",
    "def reward(outcomes):\n",
    "\t\"\"\" reward \"\"\"\n",
    "\n",
    "\t# b. consumption\n",
    "\tc = outcomes[...,0]\n",
    "\n",
    "\t# c. utility\n",
    "\tu = utility(c)\n",
    "\n",
    "\treturn u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1186aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_loss(par, nn, initial_states, shocks):\n",
    "\n",
    "    def scan_step(states_t, t):\n",
    "        \"\"\"One period transition in simulation.\"\"\"\n",
    "\n",
    "        # Policy\n",
    "        actions_t = eval_policy(nn, states_t, t)\n",
    "\n",
    "        # Outcomes & reward\n",
    "        outcomes_t = outcomes(states_t, actions_t)\n",
    "        reward_t = reward(outcomes_t)\n",
    "\n",
    "        # Transition to next state\n",
    "        next_states = state_trans(par, states_t, outcomes_t, shocks[t])\n",
    "\n",
    "        return next_states, (reward_t,)\n",
    "\n",
    "    # construct manual iterator\n",
    "    ts = jnp.arange(par.T)\n",
    "\n",
    "    # call scanner and pass initial states as first states_t\n",
    "    _, (reward_seq,) = jax.lax.scan(scan_step, initial_states, ts) # discard states in simulation\n",
    "\n",
    "    # discount\n",
    "    discounts = par.beta**jnp.arange(par.T)\n",
    "    discounted_sum = jnp.sum(discounts[:, None] * reward_seq, axis=0)\n",
    "\n",
    "    # Average objective over training samples\n",
    "    loss = -jnp.mean(discounted_sum)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c9811",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b5115cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(par, nn, initial_states, shocks):\n",
    "\n",
    "    def loss_fn(nn_):\n",
    "        return simulate_loss(par, nn_, initial_states, shocks)\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(nn)\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "def train_policy(model):\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    par = model.par\n",
    "    train = model.sol\n",
    "    \n",
    "    # create optimizer and neural network\n",
    "    nn = setup_nn(model)\n",
    "    lr = train.learning_rate_policy\n",
    "\n",
    "    policy_opt = nnx.ModelAndOptimizer(nn, optax.adam(learning_rate=lr))\n",
    "    train_step_jit = jax.jit(train_step, static_argnums=(0))\n",
    "    \n",
    "    initial_states_train, shocks_train = train.initial_states_train, train.shocks_train\n",
    "\n",
    "    # training loop\n",
    "    for k in range(train.K):\n",
    "\n",
    "        initial_states = initial_states_train[k]\n",
    "        shocks = shocks_train[k]\n",
    "\n",
    "        loss, grads = train_step_jit(par, nn, initial_states, shocks)\n",
    "\n",
    "        policy_opt.update(grads)\n",
    "\n",
    "        # iv. print progress\n",
    "        if k % 10 == 0:\n",
    "            print(f\"Iteration {k}: Loss {loss.item()}\")\n",
    "\n",
    "        train.iter_time[k] = time.perf_counter()-t0\n",
    "\n",
    "MyModel.train_policy = train_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c997ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6bd475aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss 1.4592036008834839\n",
      "Iteration 10: Loss 0.3410794138908386\n",
      "Iteration 20: Loss 0.16268207132816315\n",
      "Iteration 30: Loss 0.10668126493692398\n",
      "Iteration 40: Loss 0.10078403353691101\n",
      "Iteration 50: Loss 0.11616367101669312\n",
      "Iteration 60: Loss 0.09111995995044708\n",
      "Iteration 70: Loss 0.09416574984788895\n",
      "Iteration 80: Loss 0.10953684151172638\n",
      "Iteration 90: Loss 0.0902084931731224\n",
      "Iteration 100: Loss 0.07315880060195923\n",
      "Iteration 110: Loss 0.10105960816144943\n",
      "Iteration 120: Loss 0.08488869667053223\n",
      "Iteration 130: Loss 0.08187302201986313\n",
      "Iteration 140: Loss 0.08261433243751526\n",
      "Iteration 150: Loss 0.06982419639825821\n",
      "Iteration 160: Loss 0.0655754879117012\n",
      "Iteration 170: Loss 0.07263153046369553\n",
      "Iteration 180: Loss 0.0900285467505455\n",
      "Iteration 190: Loss 0.10107329487800598\n",
      "Iteration 200: Loss 0.09637176245450974\n",
      "Iteration 210: Loss 0.08995111286640167\n",
      "Iteration 220: Loss 0.08638092130422592\n",
      "Iteration 230: Loss 0.07697977125644684\n",
      "Iteration 240: Loss 0.07011783123016357\n",
      "Iteration 250: Loss 0.10789823532104492\n",
      "Iteration 260: Loss 0.10277961939573288\n",
      "Iteration 270: Loss 0.07577798515558243\n",
      "Iteration 280: Loss 0.07187514752149582\n",
      "Iteration 290: Loss 0.08358866721391678\n",
      "Iteration 300: Loss 0.09806603938341141\n",
      "Iteration 310: Loss 0.10870514065027237\n",
      "Iteration 320: Loss 0.07960494607686996\n",
      "Iteration 330: Loss 0.0771644115447998\n",
      "Iteration 340: Loss 0.06576048582792282\n",
      "Iteration 350: Loss 0.08547214418649673\n",
      "Iteration 360: Loss 0.07369320839643478\n",
      "Iteration 370: Loss 0.09463395923376083\n",
      "Iteration 380: Loss 0.06861749291419983\n",
      "Iteration 390: Loss 0.09738858789205551\n",
      "Iteration 400: Loss 0.08136428892612457\n",
      "Iteration 410: Loss 0.08791693300008774\n",
      "Iteration 420: Loss 0.08366774767637253\n",
      "Iteration 430: Loss 0.08907260745763779\n",
      "Iteration 440: Loss 0.09141696244478226\n",
      "Iteration 450: Loss 0.08187499642372131\n",
      "Iteration 460: Loss 0.07579529285430908\n",
      "Iteration 470: Loss 0.08199743926525116\n",
      "Iteration 480: Loss 0.06749077141284943\n",
      "Iteration 490: Loss 0.08814187347888947\n",
      "Iteration 500: Loss 0.07667417824268341\n",
      "Iteration 510: Loss 0.0825415775179863\n",
      "Iteration 520: Loss 0.07755862176418304\n",
      "Iteration 530: Loss 0.06682968884706497\n",
      "Iteration 540: Loss 0.08389869332313538\n",
      "Iteration 550: Loss 0.08077337592840195\n",
      "Iteration 560: Loss 0.09746485948562622\n",
      "Iteration 570: Loss 0.06673900783061981\n",
      "Iteration 580: Loss 0.08552298694849014\n",
      "Iteration 590: Loss 0.07905305176973343\n",
      "Iteration 600: Loss 0.06077442318201065\n",
      "Iteration 610: Loss 0.09787622839212418\n",
      "Iteration 620: Loss 0.06466268002986908\n",
      "Iteration 630: Loss 0.04829327017068863\n",
      "Iteration 640: Loss 0.0720176175236702\n",
      "Iteration 650: Loss 0.08306429535150528\n",
      "Iteration 660: Loss 0.07076152414083481\n",
      "Iteration 670: Loss 0.08600017428398132\n",
      "Iteration 680: Loss 0.08074706792831421\n",
      "Iteration 690: Loss 0.0685095340013504\n",
      "Iteration 700: Loss 0.06542573124170303\n",
      "Iteration 710: Loss 0.07413139939308167\n",
      "Iteration 720: Loss 0.07567711919546127\n",
      "Iteration 730: Loss 0.06624983996152878\n",
      "Iteration 740: Loss 0.08247198164463043\n",
      "Iteration 750: Loss 0.084465891122818\n",
      "Iteration 760: Loss 0.08664491027593613\n",
      "Iteration 770: Loss 0.0660654827952385\n",
      "Iteration 780: Loss 0.0948687419295311\n",
      "Iteration 790: Loss 0.10913798958063126\n",
      "Iteration 800: Loss 0.07798148691654205\n",
      "Iteration 810: Loss 0.07287648320198059\n",
      "Iteration 820: Loss 0.07570349425077438\n",
      "Iteration 830: Loss 0.07190750539302826\n",
      "Iteration 840: Loss 0.08249913156032562\n",
      "Iteration 850: Loss 0.0647796243429184\n",
      "Iteration 860: Loss 0.07602672278881073\n",
      "Iteration 870: Loss 0.056915830820798874\n",
      "Iteration 880: Loss 0.07693428546190262\n",
      "Iteration 890: Loss 0.07884115725755692\n",
      "Iteration 900: Loss 0.07725068926811218\n",
      "Iteration 910: Loss 0.07579364627599716\n",
      "Iteration 920: Loss 0.0642472580075264\n",
      "Iteration 930: Loss 0.06982193887233734\n",
      "Iteration 940: Loss 0.08309201151132584\n",
      "Iteration 950: Loss 0.08988533169031143\n",
      "Iteration 960: Loss 0.08996813744306564\n",
      "Iteration 970: Loss 0.07759915292263031\n",
      "Iteration 980: Loss 0.08576522767543793\n",
      "Iteration 990: Loss 0.07123449444770813\n"
     ]
    }
   ],
   "source": [
    "model.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "51918095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss 0.7371875047683716\n",
      "Iteration 10: Loss 0.08829497545957565\n",
      "Iteration 20: Loss 0.025199443101882935\n",
      "Iteration 30: Loss 0.012974199838936329\n",
      "Iteration 40: Loss 0.007960982620716095\n",
      "Iteration 50: Loss 0.008080583065748215\n",
      "Iteration 60: Loss 0.006515211891382933\n",
      "Iteration 70: Loss 0.005166305229067802\n",
      "Iteration 80: Loss 0.005398693028837442\n",
      "Iteration 90: Loss 0.0055967457592487335\n",
      "Iteration 100: Loss 0.0061430661007761955\n",
      "Iteration 110: Loss 0.005670497659593821\n",
      "Iteration 120: Loss 0.005313219968229532\n",
      "Iteration 130: Loss 0.005506480112671852\n",
      "Iteration 140: Loss 0.004100146237760782\n",
      "Iteration 150: Loss 0.006662364117801189\n",
      "Iteration 160: Loss 0.004660042002797127\n",
      "Iteration 170: Loss 0.005811668932437897\n",
      "Iteration 180: Loss 0.0048632314428687096\n",
      "Iteration 190: Loss 0.004583041649311781\n",
      "Iteration 200: Loss 0.004771729931235313\n",
      "Iteration 210: Loss 0.0059467884711921215\n",
      "Iteration 220: Loss 0.004975947551429272\n",
      "Iteration 230: Loss 0.005416672211140394\n",
      "Iteration 240: Loss 0.00520733930170536\n",
      "Iteration 250: Loss 0.004774475935846567\n",
      "Iteration 260: Loss 0.004002994857728481\n",
      "Iteration 270: Loss 0.004342650528997183\n",
      "Iteration 280: Loss 0.006905917078256607\n",
      "Iteration 290: Loss 0.005433043465018272\n",
      "Iteration 300: Loss 0.0057037221267819405\n",
      "Iteration 310: Loss 0.004469104111194611\n",
      "Iteration 320: Loss 0.0053552077151834965\n",
      "Iteration 330: Loss 0.004579260013997555\n",
      "Iteration 340: Loss 0.0054525998421013355\n",
      "Iteration 350: Loss 0.003024485893547535\n",
      "Iteration 360: Loss 0.004167358856648207\n",
      "Iteration 370: Loss 0.004350016824901104\n",
      "Iteration 380: Loss 0.006251533515751362\n",
      "Iteration 390: Loss 0.0055090938694775105\n",
      "Iteration 400: Loss 0.005269934888929129\n",
      "Iteration 410: Loss 0.0049676597118377686\n",
      "Iteration 420: Loss 0.003955805208534002\n",
      "Iteration 430: Loss 0.004922288469970226\n",
      "Iteration 440: Loss 0.006225035060197115\n",
      "Iteration 450: Loss 0.007024568039923906\n",
      "Iteration 460: Loss 0.004414896946400404\n",
      "Iteration 470: Loss 0.005271148402243853\n",
      "Iteration 480: Loss 0.003153972327709198\n",
      "Iteration 490: Loss 0.004412906710058451\n",
      "Iteration 500: Loss 0.0039332835003733635\n",
      "Iteration 510: Loss 0.0051419115625321865\n",
      "Iteration 520: Loss 0.007261781021952629\n",
      "Iteration 530: Loss 0.004246459808200598\n",
      "Iteration 540: Loss 0.0050521306693553925\n",
      "Iteration 550: Loss 0.004231455735862255\n",
      "Iteration 560: Loss 0.006183065474033356\n",
      "Iteration 570: Loss 0.004955080337822437\n",
      "Iteration 580: Loss 0.004688117187470198\n",
      "Iteration 590: Loss 0.006134286057204008\n",
      "Iteration 600: Loss 0.005874736700206995\n",
      "Iteration 610: Loss 0.004846570547670126\n",
      "Iteration 620: Loss 0.0031429999507963657\n",
      "Iteration 630: Loss 0.003301400225609541\n",
      "Iteration 640: Loss 0.006348772440105677\n",
      "Iteration 650: Loss 0.006644261535257101\n",
      "Iteration 660: Loss 0.004224906675517559\n",
      "Iteration 670: Loss 0.004763007164001465\n",
      "Iteration 680: Loss 0.0038331267423927784\n",
      "Iteration 690: Loss 0.0046258969232439995\n",
      "Iteration 700: Loss 0.005506477784365416\n",
      "Iteration 710: Loss 0.004599760286509991\n",
      "Iteration 720: Loss 0.005920725408941507\n",
      "Iteration 730: Loss 0.005608280189335346\n",
      "Iteration 740: Loss 0.005473943892866373\n",
      "Iteration 750: Loss 0.003989782650023699\n",
      "Iteration 760: Loss 0.00499742990359664\n",
      "Iteration 770: Loss 0.00527946325019002\n",
      "Iteration 780: Loss 0.004795335233211517\n",
      "Iteration 790: Loss 0.003771055955439806\n",
      "Iteration 800: Loss 0.004796971101313829\n",
      "Iteration 810: Loss 0.005009061191231012\n",
      "Iteration 820: Loss 0.005695399362593889\n",
      "Iteration 830: Loss 0.0052418699488043785\n",
      "Iteration 840: Loss 0.002967552514746785\n",
      "Iteration 850: Loss 0.0037036773283034563\n",
      "Iteration 860: Loss 0.0061153266578912735\n",
      "Iteration 870: Loss 0.005685213953256607\n",
      "Iteration 880: Loss 0.006503683514893055\n",
      "Iteration 890: Loss 0.004874232225120068\n",
      "Iteration 900: Loss 0.005684956908226013\n",
      "Iteration 910: Loss 0.005001137964427471\n",
      "Iteration 920: Loss 0.004723312333226204\n",
      "Iteration 930: Loss 0.004174983594566584\n",
      "Iteration 940: Loss 0.005760280415415764\n",
      "Iteration 950: Loss 0.004007973242551088\n",
      "Iteration 960: Loss 0.004930933937430382\n",
      "Iteration 970: Loss 0.006001533940434456\n",
      "Iteration 980: Loss 0.0067221783101558685\n",
      "Iteration 990: Loss 0.005922598298639059\n",
      "Iteration 0: Loss 1.363918662071228\n",
      "Iteration 10: Loss 0.23524805903434753\n",
      "Iteration 20: Loss 0.07772711664438248\n",
      "Iteration 30: Loss 0.03813880681991577\n",
      "Iteration 40: Loss 0.032508574426174164\n",
      "Iteration 50: Loss 0.027263134717941284\n",
      "Iteration 60: Loss 0.02145310863852501\n",
      "Iteration 70: Loss 0.02069748565554619\n",
      "Iteration 80: Loss 0.022538814693689346\n",
      "Iteration 90: Loss 0.01984092965722084\n",
      "Iteration 100: Loss 0.019815856590867043\n",
      "Iteration 110: Loss 0.018551018089056015\n",
      "Iteration 120: Loss 0.022079668939113617\n",
      "Iteration 130: Loss 0.021941954270005226\n",
      "Iteration 140: Loss 0.01806514337658882\n",
      "Iteration 150: Loss 0.023727934807538986\n",
      "Iteration 160: Loss 0.02121531404554844\n",
      "Iteration 170: Loss 0.019603347405791283\n",
      "Iteration 180: Loss 0.021548006683588028\n",
      "Iteration 190: Loss 0.016893083229660988\n",
      "Iteration 200: Loss 0.019348032772541046\n",
      "Iteration 210: Loss 0.02117825485765934\n",
      "Iteration 220: Loss 0.021573975682258606\n",
      "Iteration 230: Loss 0.019241832196712494\n",
      "Iteration 240: Loss 0.02113451436161995\n",
      "Iteration 250: Loss 0.017992474138736725\n",
      "Iteration 260: Loss 0.019893428310751915\n",
      "Iteration 270: Loss 0.020470228046178818\n",
      "Iteration 280: Loss 0.024721620604395866\n",
      "Iteration 290: Loss 0.021846294403076172\n",
      "Iteration 300: Loss 0.02005048841238022\n",
      "Iteration 310: Loss 0.020966963842511177\n",
      "Iteration 320: Loss 0.014402871020138264\n",
      "Iteration 330: Loss 0.018966862931847572\n",
      "Iteration 340: Loss 0.019571328535676003\n",
      "Iteration 350: Loss 0.016107778996229172\n",
      "Iteration 360: Loss 0.022588539868593216\n",
      "Iteration 370: Loss 0.02218962274491787\n",
      "Iteration 380: Loss 0.018211491405963898\n",
      "Iteration 390: Loss 0.019925881177186966\n",
      "Iteration 400: Loss 0.021315502002835274\n",
      "Iteration 410: Loss 0.019296720623970032\n",
      "Iteration 420: Loss 0.01932629942893982\n",
      "Iteration 430: Loss 0.021365385502576828\n",
      "Iteration 440: Loss 0.022768964990973473\n",
      "Iteration 450: Loss 0.018967559561133385\n",
      "Iteration 460: Loss 0.020951543003320694\n",
      "Iteration 470: Loss 0.021420177072286606\n",
      "Iteration 480: Loss 0.018952568992972374\n",
      "Iteration 490: Loss 0.017884591594338417\n",
      "Iteration 500: Loss 0.01649855077266693\n",
      "Iteration 510: Loss 0.02178012579679489\n",
      "Iteration 520: Loss 0.02139171026647091\n",
      "Iteration 530: Loss 0.01672094129025936\n",
      "Iteration 540: Loss 0.02092711813747883\n",
      "Iteration 550: Loss 0.01765204779803753\n",
      "Iteration 560: Loss 0.02148045226931572\n",
      "Iteration 570: Loss 0.019479574635624886\n",
      "Iteration 580: Loss 0.02029595896601677\n",
      "Iteration 590: Loss 0.02061878889799118\n",
      "Iteration 600: Loss 0.01965140365064144\n",
      "Iteration 610: Loss 0.01996980980038643\n",
      "Iteration 620: Loss 0.01958348974585533\n",
      "Iteration 630: Loss 0.020921267569065094\n",
      "Iteration 640: Loss 0.023263324052095413\n",
      "Iteration 650: Loss 0.02079041860997677\n",
      "Iteration 660: Loss 0.017965810373425484\n",
      "Iteration 670: Loss 0.019441992044448853\n",
      "Iteration 680: Loss 0.017002301290631294\n",
      "Iteration 690: Loss 0.021543562412261963\n",
      "Iteration 700: Loss 0.019387666136026382\n",
      "Iteration 710: Loss 0.01916852407157421\n",
      "Iteration 720: Loss 0.01967979036271572\n",
      "Iteration 730: Loss 0.0211014486849308\n",
      "Iteration 740: Loss 0.019136596471071243\n",
      "Iteration 750: Loss 0.019262058660387993\n",
      "Iteration 760: Loss 0.020894186571240425\n",
      "Iteration 770: Loss 0.020735222846269608\n",
      "Iteration 780: Loss 0.01703016273677349\n",
      "Iteration 790: Loss 0.02059297449886799\n",
      "Iteration 800: Loss 0.017964916303753853\n",
      "Iteration 810: Loss 0.018787838518619537\n",
      "Iteration 820: Loss 0.02058437466621399\n",
      "Iteration 830: Loss 0.01939866878092289\n",
      "Iteration 840: Loss 0.018519211560487747\n",
      "Iteration 850: Loss 0.02189084328711033\n",
      "Iteration 860: Loss 0.022490350529551506\n",
      "Iteration 870: Loss 0.02020265720784664\n",
      "Iteration 880: Loss 0.020116344094276428\n",
      "Iteration 890: Loss 0.019086206331849098\n",
      "Iteration 900: Loss 0.01816159300506115\n",
      "Iteration 910: Loss 0.02011818252503872\n",
      "Iteration 920: Loss 0.020752036944031715\n",
      "Iteration 930: Loss 0.01912507973611355\n",
      "Iteration 940: Loss 0.022820284590125084\n",
      "Iteration 950: Loss 0.02146065980195999\n",
      "Iteration 960: Loss 0.019962674006819725\n",
      "Iteration 970: Loss 0.022953785955905914\n",
      "Iteration 980: Loss 0.019185401499271393\n",
      "Iteration 990: Loss 0.019473761320114136\n",
      "Iteration 0: Loss 1.4597572088241577\n",
      "Iteration 10: Loss 0.335274875164032\n",
      "Iteration 20: Loss 0.1656496822834015\n",
      "Iteration 30: Loss 0.12971334159374237\n",
      "Iteration 40: Loss 0.09839663654565811\n",
      "Iteration 50: Loss 0.10587829351425171\n",
      "Iteration 60: Loss 0.09996578097343445\n",
      "Iteration 70: Loss 0.08789398521184921\n",
      "Iteration 80: Loss 0.093805231153965\n",
      "Iteration 90: Loss 0.08445684611797333\n",
      "Iteration 100: Loss 0.08326934278011322\n",
      "Iteration 110: Loss 0.0918700248003006\n",
      "Iteration 120: Loss 0.08590937405824661\n",
      "Iteration 130: Loss 0.09207670390605927\n",
      "Iteration 140: Loss 0.08178678154945374\n",
      "Iteration 150: Loss 0.0867958813905716\n",
      "Iteration 160: Loss 0.08611131459474564\n",
      "Iteration 170: Loss 0.08622369170188904\n",
      "Iteration 180: Loss 0.0875457152724266\n",
      "Iteration 190: Loss 0.08740741014480591\n",
      "Iteration 200: Loss 0.08387444913387299\n",
      "Iteration 210: Loss 0.08742672950029373\n",
      "Iteration 220: Loss 0.09229175746440887\n",
      "Iteration 230: Loss 0.08210714906454086\n",
      "Iteration 240: Loss 0.08639520406723022\n",
      "Iteration 250: Loss 0.08317860960960388\n",
      "Iteration 260: Loss 0.09090108424425125\n",
      "Iteration 270: Loss 0.09991457313299179\n",
      "Iteration 280: Loss 0.09579584747552872\n",
      "Iteration 290: Loss 0.08679276704788208\n",
      "Iteration 300: Loss 0.08299122005701065\n",
      "Iteration 310: Loss 0.08655735850334167\n",
      "Iteration 320: Loss 0.08615658432245255\n",
      "Iteration 330: Loss 0.08495650440454483\n",
      "Iteration 340: Loss 0.08825374394655228\n",
      "Iteration 350: Loss 0.07828173786401749\n",
      "Iteration 360: Loss 0.09095701575279236\n",
      "Iteration 370: Loss 0.09523764252662659\n",
      "Iteration 380: Loss 0.0992073342204094\n",
      "Iteration 390: Loss 0.08512363582849503\n",
      "Iteration 400: Loss 0.089478999376297\n",
      "Iteration 410: Loss 0.08901015669107437\n",
      "Iteration 420: Loss 0.08107411861419678\n",
      "Iteration 430: Loss 0.09275814890861511\n",
      "Iteration 440: Loss 0.08442043513059616\n",
      "Iteration 450: Loss 0.08687815070152283\n",
      "Iteration 460: Loss 0.09381721913814545\n",
      "Iteration 470: Loss 0.07890928536653519\n",
      "Iteration 480: Loss 0.07686758786439896\n",
      "Iteration 490: Loss 0.07564699649810791\n",
      "Iteration 500: Loss 0.07121787220239639\n",
      "Iteration 510: Loss 0.07930966466665268\n",
      "Iteration 520: Loss 0.07429254055023193\n",
      "Iteration 530: Loss 0.0905265212059021\n",
      "Iteration 540: Loss 0.0777793675661087\n",
      "Iteration 550: Loss 0.07667393982410431\n",
      "Iteration 560: Loss 0.07700259238481522\n",
      "Iteration 570: Loss 0.0852598026394844\n",
      "Iteration 580: Loss 0.06374134123325348\n",
      "Iteration 590: Loss 0.07507453113794327\n",
      "Iteration 600: Loss 0.06931717693805695\n",
      "Iteration 610: Loss 0.07425454258918762\n",
      "Iteration 620: Loss 0.06352488696575165\n",
      "Iteration 630: Loss 0.07729538530111313\n",
      "Iteration 640: Loss 0.0713762491941452\n",
      "Iteration 650: Loss 0.07135943323373795\n",
      "Iteration 660: Loss 0.07738283276557922\n",
      "Iteration 670: Loss 0.07481921464204788\n",
      "Iteration 680: Loss 0.06828062981367111\n",
      "Iteration 690: Loss 0.07362581789493561\n",
      "Iteration 700: Loss 0.08252920210361481\n",
      "Iteration 710: Loss 0.07207024097442627\n",
      "Iteration 720: Loss 0.06572679430246353\n",
      "Iteration 730: Loss 0.06437742710113525\n",
      "Iteration 740: Loss 0.09060120582580566\n",
      "Iteration 750: Loss 0.0693129152059555\n",
      "Iteration 760: Loss 0.07269838452339172\n",
      "Iteration 770: Loss 0.07817251235246658\n",
      "Iteration 780: Loss 0.08474316447973251\n",
      "Iteration 790: Loss 0.07958539575338364\n",
      "Iteration 800: Loss 0.06675190478563309\n",
      "Iteration 810: Loss 0.07786896079778671\n",
      "Iteration 820: Loss 0.06976048648357391\n",
      "Iteration 830: Loss 0.08038827031850815\n",
      "Iteration 840: Loss 0.07068272680044174\n",
      "Iteration 850: Loss 0.08390366286039352\n",
      "Iteration 860: Loss 0.08654345571994781\n",
      "Iteration 870: Loss 0.08242689818143845\n",
      "Iteration 880: Loss 0.06675911694765091\n",
      "Iteration 890: Loss 0.08051303774118423\n",
      "Iteration 900: Loss 0.0681520402431488\n",
      "Iteration 910: Loss 0.07464674115180969\n",
      "Iteration 920: Loss 0.08381082862615585\n",
      "Iteration 930: Loss 0.07169695198535919\n",
      "Iteration 940: Loss 0.08220602571964264\n",
      "Iteration 950: Loss 0.07260115444660187\n",
      "Iteration 960: Loss 0.07560524344444275\n",
      "Iteration 970: Loss 0.0841502994298935\n",
      "Iteration 980: Loss 0.0801706537604332\n",
      "Iteration 990: Loss 0.07964102923870087\n",
      "Iteration 0: Loss 1.5228877067565918\n",
      "Iteration 10: Loss 0.5738115310668945\n",
      "Iteration 20: Loss 0.3779946565628052\n",
      "Iteration 30: Loss 0.3629748821258545\n",
      "Iteration 40: Loss 0.33319681882858276\n",
      "Iteration 50: Loss 0.2954058051109314\n",
      "Iteration 60: Loss 0.27132880687713623\n",
      "Iteration 70: Loss 0.24944093823432922\n",
      "Iteration 80: Loss 0.27315983176231384\n",
      "Iteration 90: Loss 0.25482627749443054\n",
      "Iteration 100: Loss 0.2522726058959961\n",
      "Iteration 110: Loss 0.25846877694129944\n",
      "Iteration 120: Loss 0.25791215896606445\n",
      "Iteration 130: Loss 0.2763064503669739\n",
      "Iteration 140: Loss 0.26340579986572266\n",
      "Iteration 150: Loss 0.26381343603134155\n",
      "Iteration 160: Loss 0.2749423682689667\n",
      "Iteration 170: Loss 0.25894424319267273\n",
      "Iteration 180: Loss 0.2466784119606018\n",
      "Iteration 190: Loss 0.2619773745536804\n",
      "Iteration 200: Loss 0.2405060976743698\n",
      "Iteration 210: Loss 0.24342690408229828\n",
      "Iteration 220: Loss 0.2522192895412445\n",
      "Iteration 230: Loss 0.23917147517204285\n",
      "Iteration 240: Loss 0.216142937541008\n",
      "Iteration 250: Loss 0.22455967962741852\n",
      "Iteration 260: Loss 0.2476452887058258\n",
      "Iteration 270: Loss 0.23271030187606812\n",
      "Iteration 280: Loss 0.24966537952423096\n",
      "Iteration 290: Loss 0.21014012396335602\n",
      "Iteration 300: Loss 0.23102650046348572\n",
      "Iteration 310: Loss 0.1946839988231659\n",
      "Iteration 320: Loss 0.22257231175899506\n",
      "Iteration 330: Loss 0.24652040004730225\n",
      "Iteration 340: Loss 0.21278095245361328\n",
      "Iteration 350: Loss 0.24641361832618713\n",
      "Iteration 360: Loss 0.23505109548568726\n",
      "Iteration 370: Loss 0.2529270052909851\n",
      "Iteration 380: Loss 0.21041445434093475\n",
      "Iteration 390: Loss 0.23261749744415283\n",
      "Iteration 400: Loss 0.22885389626026154\n",
      "Iteration 410: Loss 0.2112552672624588\n",
      "Iteration 420: Loss 0.2190297245979309\n",
      "Iteration 430: Loss 0.2667377293109894\n",
      "Iteration 440: Loss 0.23430633544921875\n",
      "Iteration 450: Loss 0.22757023572921753\n",
      "Iteration 460: Loss 0.25848889350891113\n",
      "Iteration 470: Loss 0.2303522825241089\n",
      "Iteration 480: Loss 0.25310951471328735\n",
      "Iteration 490: Loss 0.2546274960041046\n",
      "Iteration 500: Loss 0.2346397191286087\n",
      "Iteration 510: Loss 0.22274108231067657\n",
      "Iteration 520: Loss 0.21213380992412567\n",
      "Iteration 530: Loss 0.22342084348201752\n",
      "Iteration 540: Loss 0.20761427283287048\n",
      "Iteration 550: Loss 0.21918007731437683\n",
      "Iteration 560: Loss 0.23332196474075317\n",
      "Iteration 570: Loss 0.26287615299224854\n",
      "Iteration 580: Loss 0.2211858332157135\n",
      "Iteration 590: Loss 0.21363051235675812\n",
      "Iteration 600: Loss 0.22380399703979492\n",
      "Iteration 610: Loss 0.21814700961112976\n",
      "Iteration 620: Loss 0.2472473829984665\n",
      "Iteration 630: Loss 0.2572839856147766\n",
      "Iteration 640: Loss 0.23727470636367798\n",
      "Iteration 650: Loss 0.22061596810817719\n",
      "Iteration 660: Loss 0.22304266691207886\n",
      "Iteration 670: Loss 0.21928036212921143\n",
      "Iteration 680: Loss 0.23040783405303955\n",
      "Iteration 690: Loss 0.27266278862953186\n",
      "Iteration 700: Loss 0.23568060994148254\n",
      "Iteration 710: Loss 0.21947936713695526\n",
      "Iteration 720: Loss 0.219040185213089\n",
      "Iteration 730: Loss 0.2138061225414276\n",
      "Iteration 740: Loss 0.24833455681800842\n",
      "Iteration 750: Loss 0.24305124580860138\n",
      "Iteration 760: Loss 0.2097974568605423\n",
      "Iteration 770: Loss 0.24122551083564758\n",
      "Iteration 780: Loss 0.2112828493118286\n",
      "Iteration 790: Loss 0.2382199466228485\n",
      "Iteration 800: Loss 0.19925646483898163\n",
      "Iteration 810: Loss 0.22191327810287476\n",
      "Iteration 820: Loss 0.2575042247772217\n",
      "Iteration 830: Loss 0.22295543551445007\n",
      "Iteration 840: Loss 0.22203309834003448\n",
      "Iteration 850: Loss 0.25143635272979736\n",
      "Iteration 860: Loss 0.19851619005203247\n",
      "Iteration 870: Loss 0.27055487036705017\n",
      "Iteration 880: Loss 0.2200823873281479\n",
      "Iteration 890: Loss 0.2282969355583191\n",
      "Iteration 900: Loss 0.209436297416687\n",
      "Iteration 910: Loss 0.20048783719539642\n",
      "Iteration 920: Loss 0.23286715149879456\n",
      "Iteration 930: Loss 0.25184574723243713\n",
      "Iteration 940: Loss 0.25141510367393494\n",
      "Iteration 950: Loss 0.22955185174942017\n",
      "Iteration 960: Loss 0.235390767455101\n",
      "Iteration 970: Loss 0.2815735638141632\n",
      "Iteration 980: Loss 0.2251928597688675\n",
      "Iteration 990: Loss 0.251132994890213\n",
      "Iteration 0: Loss 1.5548827648162842\n",
      "Iteration 10: Loss 0.8467433452606201\n",
      "Iteration 20: Loss 0.699941873550415\n",
      "Iteration 30: Loss 0.5647012591362\n",
      "Iteration 40: Loss 0.5417848229408264\n",
      "Iteration 50: Loss 0.49066394567489624\n",
      "Iteration 60: Loss 0.5016049742698669\n",
      "Iteration 70: Loss 0.4862988293170929\n",
      "Iteration 80: Loss 0.4948810338973999\n",
      "Iteration 90: Loss 0.47700268030166626\n",
      "Iteration 100: Loss 0.4737226366996765\n",
      "Iteration 110: Loss 0.4683685302734375\n",
      "Iteration 120: Loss 0.4758749008178711\n",
      "Iteration 130: Loss 0.5483400821685791\n",
      "Iteration 140: Loss 0.5080726146697998\n",
      "Iteration 150: Loss 0.539393424987793\n",
      "Iteration 160: Loss 0.47651833295822144\n",
      "Iteration 170: Loss 0.5109513401985168\n",
      "Iteration 180: Loss 0.48594334721565247\n",
      "Iteration 190: Loss 0.4860629141330719\n",
      "Iteration 200: Loss 0.46219226717948914\n",
      "Iteration 210: Loss 0.45605334639549255\n",
      "Iteration 220: Loss 0.45870956778526306\n",
      "Iteration 230: Loss 0.41483625769615173\n",
      "Iteration 240: Loss 0.42001640796661377\n",
      "Iteration 250: Loss 0.4252042770385742\n",
      "Iteration 260: Loss 0.4606640934944153\n",
      "Iteration 270: Loss 0.48619598150253296\n",
      "Iteration 280: Loss 0.45232605934143066\n",
      "Iteration 290: Loss 0.4701145887374878\n",
      "Iteration 300: Loss 0.3972020745277405\n",
      "Iteration 310: Loss 0.4466821551322937\n",
      "Iteration 320: Loss 0.4534326195716858\n",
      "Iteration 330: Loss 0.44143378734588623\n",
      "Iteration 340: Loss 0.407064288854599\n",
      "Iteration 350: Loss 0.4744870066642761\n",
      "Iteration 360: Loss 0.4105486273765564\n",
      "Iteration 370: Loss 0.4857986569404602\n",
      "Iteration 380: Loss 0.4783400297164917\n",
      "Iteration 390: Loss 0.44882693886756897\n",
      "Iteration 400: Loss 0.4407981336116791\n",
      "Iteration 410: Loss 0.4157618582248688\n",
      "Iteration 420: Loss 0.46392011642456055\n",
      "Iteration 430: Loss 0.4461829364299774\n",
      "Iteration 440: Loss 0.4297046661376953\n",
      "Iteration 450: Loss 0.4560985267162323\n",
      "Iteration 460: Loss 0.5043092370033264\n",
      "Iteration 470: Loss 0.43716248869895935\n",
      "Iteration 480: Loss 0.45316848158836365\n",
      "Iteration 490: Loss 0.4184885621070862\n",
      "Iteration 500: Loss 0.44568660855293274\n",
      "Iteration 510: Loss 0.4181845188140869\n",
      "Iteration 520: Loss 0.39341893792152405\n",
      "Iteration 530: Loss 0.4405645430088043\n",
      "Iteration 540: Loss 0.4547596573829651\n",
      "Iteration 550: Loss 0.4343982934951782\n",
      "Iteration 560: Loss 0.42905667424201965\n",
      "Iteration 570: Loss 0.4314056932926178\n",
      "Iteration 580: Loss 0.4934121072292328\n",
      "Iteration 590: Loss 0.4730643332004547\n",
      "Iteration 600: Loss 0.42058151960372925\n",
      "Iteration 610: Loss 0.4359007775783539\n",
      "Iteration 620: Loss 0.44972309470176697\n",
      "Iteration 630: Loss 0.43974295258522034\n",
      "Iteration 640: Loss 0.4665988087654114\n",
      "Iteration 650: Loss 0.43117180466651917\n",
      "Iteration 660: Loss 0.47766873240470886\n",
      "Iteration 670: Loss 0.4056489169597626\n",
      "Iteration 680: Loss 0.4158630669116974\n",
      "Iteration 690: Loss 0.4956584870815277\n",
      "Iteration 700: Loss 0.44160526990890503\n",
      "Iteration 710: Loss 0.4343482255935669\n",
      "Iteration 720: Loss 0.42508989572525024\n",
      "Iteration 730: Loss 0.4142119884490967\n",
      "Iteration 740: Loss 0.47720208764076233\n",
      "Iteration 750: Loss 0.398210734128952\n",
      "Iteration 760: Loss 0.4524518549442291\n",
      "Iteration 770: Loss 0.44810375571250916\n",
      "Iteration 780: Loss 0.4451488256454468\n",
      "Iteration 790: Loss 0.4826323986053467\n",
      "Iteration 800: Loss 0.3956236243247986\n",
      "Iteration 810: Loss 0.36452504992485046\n",
      "Iteration 820: Loss 0.4668431580066681\n",
      "Iteration 830: Loss 0.4650261700153351\n",
      "Iteration 840: Loss 0.4501258134841919\n",
      "Iteration 850: Loss 0.43212148547172546\n",
      "Iteration 860: Loss 0.4457624852657318\n",
      "Iteration 870: Loss 0.42566797137260437\n",
      "Iteration 880: Loss 0.4316018521785736\n",
      "Iteration 890: Loss 0.45461562275886536\n",
      "Iteration 900: Loss 0.4059808850288391\n",
      "Iteration 910: Loss 0.46283039450645447\n",
      "Iteration 920: Loss 0.4664969742298126\n",
      "Iteration 930: Loss 0.43008846044540405\n",
      "Iteration 940: Loss 0.43828848004341125\n",
      "Iteration 950: Loss 0.4151642918586731\n",
      "Iteration 960: Loss 0.41065695881843567\n",
      "Iteration 970: Loss 0.4338274300098419\n",
      "Iteration 980: Loss 0.405361145734787\n",
      "Iteration 990: Loss 0.43179988861083984\n",
      "Iteration 0: Loss 1.8097606897354126\n",
      "Iteration 10: Loss 1.003398060798645\n",
      "Iteration 20: Loss 0.9438754916191101\n",
      "Iteration 30: Loss 0.8266098499298096\n",
      "Iteration 40: Loss 0.7959581613540649\n",
      "Iteration 50: Loss 0.8033169507980347\n",
      "Iteration 60: Loss 0.7543764710426331\n",
      "Iteration 70: Loss 0.7539592385292053\n",
      "Iteration 80: Loss 0.812965989112854\n",
      "Iteration 90: Loss 0.7377364039421082\n",
      "Iteration 100: Loss 0.7854463458061218\n",
      "Iteration 110: Loss 0.7779839634895325\n",
      "Iteration 120: Loss 0.7277932167053223\n",
      "Iteration 130: Loss 0.7838537693023682\n",
      "Iteration 140: Loss 0.748351514339447\n",
      "Iteration 150: Loss 0.7916806936264038\n",
      "Iteration 160: Loss 0.7070890665054321\n",
      "Iteration 170: Loss 0.6412902474403381\n",
      "Iteration 180: Loss 0.6832936406135559\n",
      "Iteration 190: Loss 0.6460650563240051\n",
      "Iteration 200: Loss 0.6328502893447876\n",
      "Iteration 210: Loss 0.6790147423744202\n",
      "Iteration 220: Loss 0.7002315521240234\n",
      "Iteration 230: Loss 0.6991638541221619\n",
      "Iteration 240: Loss 0.6845810413360596\n",
      "Iteration 250: Loss 0.6658096313476562\n",
      "Iteration 260: Loss 0.6457698345184326\n",
      "Iteration 270: Loss 0.6247972249984741\n",
      "Iteration 280: Loss 0.683383584022522\n",
      "Iteration 290: Loss 0.6632761359214783\n",
      "Iteration 300: Loss 0.6612659096717834\n",
      "Iteration 310: Loss 0.6958494186401367\n",
      "Iteration 320: Loss 0.6732993125915527\n",
      "Iteration 330: Loss 0.6728920340538025\n",
      "Iteration 340: Loss 0.6690340638160706\n",
      "Iteration 350: Loss 0.6879847049713135\n",
      "Iteration 360: Loss 0.7182742953300476\n",
      "Iteration 370: Loss 0.7181545495986938\n",
      "Iteration 380: Loss 0.6349400281906128\n",
      "Iteration 390: Loss 0.6100195646286011\n",
      "Iteration 400: Loss 0.6651133894920349\n",
      "Iteration 410: Loss 0.7193036079406738\n",
      "Iteration 420: Loss 0.6699351668357849\n",
      "Iteration 430: Loss 0.6188700199127197\n",
      "Iteration 440: Loss 0.6887375712394714\n",
      "Iteration 450: Loss 0.67972731590271\n",
      "Iteration 460: Loss 0.7160166501998901\n",
      "Iteration 470: Loss 0.7318456768989563\n",
      "Iteration 480: Loss 0.7225006818771362\n",
      "Iteration 490: Loss 0.7030429244041443\n",
      "Iteration 500: Loss 0.7273940443992615\n",
      "Iteration 510: Loss 0.653514564037323\n",
      "Iteration 520: Loss 0.6470299363136292\n",
      "Iteration 530: Loss 0.6585338711738586\n",
      "Iteration 540: Loss 0.684984028339386\n",
      "Iteration 550: Loss 0.7030456066131592\n",
      "Iteration 560: Loss 0.6714027523994446\n",
      "Iteration 570: Loss 0.7214089632034302\n",
      "Iteration 580: Loss 0.6427338123321533\n",
      "Iteration 590: Loss 0.7025225162506104\n",
      "Iteration 600: Loss 0.6327979564666748\n",
      "Iteration 610: Loss 0.6896830201148987\n",
      "Iteration 620: Loss 0.6715536713600159\n",
      "Iteration 630: Loss 0.6889135241508484\n",
      "Iteration 640: Loss 0.631909191608429\n",
      "Iteration 650: Loss 0.68702632188797\n",
      "Iteration 660: Loss 0.6861901879310608\n",
      "Iteration 670: Loss 0.6944877505302429\n",
      "Iteration 680: Loss 0.6771424412727356\n",
      "Iteration 690: Loss 0.7303547859191895\n",
      "Iteration 700: Loss 0.6911228895187378\n",
      "Iteration 710: Loss 0.6147139668464661\n",
      "Iteration 720: Loss 0.6379510164260864\n",
      "Iteration 730: Loss 0.664875864982605\n",
      "Iteration 740: Loss 0.6651094555854797\n",
      "Iteration 750: Loss 0.6567369699478149\n",
      "Iteration 760: Loss 0.6838769316673279\n",
      "Iteration 770: Loss 0.6171849370002747\n",
      "Iteration 780: Loss 0.7021984457969666\n",
      "Iteration 790: Loss 0.7090219259262085\n",
      "Iteration 800: Loss 0.7129891514778137\n",
      "Iteration 810: Loss 0.6596484780311584\n",
      "Iteration 820: Loss 0.7122400403022766\n",
      "Iteration 830: Loss 0.6862104535102844\n",
      "Iteration 840: Loss 0.6294259428977966\n",
      "Iteration 850: Loss 0.6818055510520935\n",
      "Iteration 860: Loss 0.6765291094779968\n",
      "Iteration 870: Loss 0.7420339584350586\n",
      "Iteration 880: Loss 0.6298483610153198\n",
      "Iteration 890: Loss 0.6924389600753784\n",
      "Iteration 900: Loss 0.6211010813713074\n",
      "Iteration 910: Loss 0.7015343308448792\n",
      "Iteration 920: Loss 0.6685072183609009\n",
      "Iteration 930: Loss 0.6400753855705261\n",
      "Iteration 940: Loss 0.7536692023277283\n",
      "Iteration 950: Loss 0.6527151465415955\n",
      "Iteration 960: Loss 0.6612254977226257\n",
      "Iteration 970: Loss 0.7301479578018188\n",
      "Iteration 980: Loss 0.6662743091583252\n",
      "Iteration 990: Loss 0.675754964351654\n",
      "Iteration 0: Loss 2.1198744773864746\n",
      "Iteration 10: Loss 1.4054489135742188\n",
      "Iteration 20: Loss 1.2072409391403198\n",
      "Iteration 30: Loss 1.0611194372177124\n",
      "Iteration 40: Loss 1.1398390531539917\n",
      "Iteration 50: Loss 1.0287195444107056\n",
      "Iteration 60: Loss 1.0146048069000244\n",
      "Iteration 70: Loss 1.0668379068374634\n",
      "Iteration 80: Loss 1.084478497505188\n",
      "Iteration 90: Loss 1.0932855606079102\n",
      "Iteration 100: Loss 1.0641165971755981\n",
      "Iteration 110: Loss 0.9574946165084839\n",
      "Iteration 120: Loss 1.0790472030639648\n",
      "Iteration 130: Loss 1.025524377822876\n",
      "Iteration 140: Loss 0.9900323748588562\n",
      "Iteration 150: Loss 0.9541806578636169\n",
      "Iteration 160: Loss 0.8805314302444458\n",
      "Iteration 170: Loss 0.8378187417984009\n",
      "Iteration 180: Loss 0.8633776903152466\n",
      "Iteration 190: Loss 0.881449282169342\n",
      "Iteration 200: Loss 0.9155508279800415\n",
      "Iteration 210: Loss 1.0057095289230347\n",
      "Iteration 220: Loss 0.9131977558135986\n",
      "Iteration 230: Loss 0.888196587562561\n",
      "Iteration 240: Loss 0.9042609333992004\n",
      "Iteration 250: Loss 0.9239964485168457\n",
      "Iteration 260: Loss 0.9331852197647095\n",
      "Iteration 270: Loss 0.9120975136756897\n",
      "Iteration 280: Loss 0.9839169383049011\n",
      "Iteration 290: Loss 0.9128366112709045\n",
      "Iteration 300: Loss 0.9557858109474182\n",
      "Iteration 310: Loss 0.9754734039306641\n",
      "Iteration 320: Loss 0.9137709736824036\n",
      "Iteration 330: Loss 0.9247426390647888\n",
      "Iteration 340: Loss 1.0005847215652466\n",
      "Iteration 350: Loss 0.9314236044883728\n",
      "Iteration 360: Loss 0.9480628371238708\n",
      "Iteration 370: Loss 0.9622007608413696\n",
      "Iteration 380: Loss 0.9701291918754578\n",
      "Iteration 390: Loss 0.8876468539237976\n",
      "Iteration 400: Loss 0.9932366013526917\n",
      "Iteration 410: Loss 0.8788778185844421\n",
      "Iteration 420: Loss 0.9657350182533264\n",
      "Iteration 430: Loss 0.9860821962356567\n",
      "Iteration 440: Loss 0.9844076633453369\n",
      "Iteration 450: Loss 0.8844773173332214\n",
      "Iteration 460: Loss 1.0154039859771729\n",
      "Iteration 470: Loss 0.9211263656616211\n",
      "Iteration 480: Loss 0.9029431343078613\n",
      "Iteration 490: Loss 0.9777507781982422\n",
      "Iteration 500: Loss 0.8956277370452881\n",
      "Iteration 510: Loss 0.8675050735473633\n",
      "Iteration 520: Loss 0.9420999884605408\n",
      "Iteration 530: Loss 0.9830862879753113\n",
      "Iteration 540: Loss 0.9135312438011169\n",
      "Iteration 550: Loss 0.9620904922485352\n",
      "Iteration 560: Loss 0.9715927243232727\n",
      "Iteration 570: Loss 0.8901275396347046\n",
      "Iteration 580: Loss 0.9264189004898071\n",
      "Iteration 590: Loss 0.9992626905441284\n",
      "Iteration 600: Loss 0.903732419013977\n",
      "Iteration 610: Loss 0.9704331755638123\n",
      "Iteration 620: Loss 0.9576124548912048\n",
      "Iteration 630: Loss 0.9622932076454163\n",
      "Iteration 640: Loss 0.9948732256889343\n",
      "Iteration 650: Loss 0.9127402901649475\n",
      "Iteration 660: Loss 0.913301944732666\n",
      "Iteration 670: Loss 0.8914821743965149\n",
      "Iteration 680: Loss 0.9202790856361389\n",
      "Iteration 690: Loss 0.9489973187446594\n",
      "Iteration 700: Loss 0.8769779801368713\n",
      "Iteration 710: Loss 0.9976678490638733\n",
      "Iteration 720: Loss 0.8584815263748169\n",
      "Iteration 730: Loss 0.9088253378868103\n",
      "Iteration 740: Loss 0.9407564997673035\n",
      "Iteration 750: Loss 0.9461272358894348\n",
      "Iteration 760: Loss 0.9042701125144958\n",
      "Iteration 770: Loss 0.9610236287117004\n",
      "Iteration 780: Loss 0.9507091641426086\n",
      "Iteration 790: Loss 0.9697449207305908\n",
      "Iteration 800: Loss 0.9218248724937439\n",
      "Iteration 810: Loss 0.9335870742797852\n",
      "Iteration 820: Loss 0.9774078130722046\n",
      "Iteration 830: Loss 0.8527389168739319\n",
      "Iteration 840: Loss 0.9386569857597351\n",
      "Iteration 850: Loss 0.8990350365638733\n",
      "Iteration 860: Loss 0.8848134279251099\n",
      "Iteration 870: Loss 1.0192296504974365\n",
      "Iteration 880: Loss 0.9868274331092834\n",
      "Iteration 890: Loss 0.9963105916976929\n",
      "Iteration 900: Loss 0.8224995136260986\n",
      "Iteration 910: Loss 0.9066886305809021\n",
      "Iteration 920: Loss 1.0251507759094238\n",
      "Iteration 930: Loss 0.9713049530982971\n",
      "Iteration 940: Loss 0.9695786833763123\n",
      "Iteration 950: Loss 0.9466058611869812\n",
      "Iteration 960: Loss 0.8763282895088196\n",
      "Iteration 970: Loss 0.8906547427177429\n",
      "Iteration 980: Loss 0.8951826095581055\n",
      "Iteration 990: Loss 0.9493193030357361\n",
      "Iteration 0: Loss 2.311629295349121\n",
      "Iteration 10: Loss 1.8534132242202759\n",
      "Iteration 20: Loss 1.5387097597122192\n",
      "Iteration 30: Loss 1.4243872165679932\n",
      "Iteration 40: Loss 1.3901972770690918\n",
      "Iteration 50: Loss 1.3415981531143188\n",
      "Iteration 60: Loss 1.3712433576583862\n",
      "Iteration 70: Loss 1.3792136907577515\n",
      "Iteration 80: Loss 1.3378863334655762\n",
      "Iteration 90: Loss 1.327938199043274\n",
      "Iteration 100: Loss 1.4375407695770264\n",
      "Iteration 110: Loss 1.331330418586731\n",
      "Iteration 120: Loss 1.3070634603500366\n",
      "Iteration 130: Loss 1.2736010551452637\n",
      "Iteration 140: Loss 1.211037039756775\n",
      "Iteration 150: Loss 1.1234458684921265\n",
      "Iteration 160: Loss 1.204738974571228\n",
      "Iteration 170: Loss 1.12224543094635\n",
      "Iteration 180: Loss 1.091776967048645\n",
      "Iteration 190: Loss 1.3068604469299316\n",
      "Iteration 200: Loss 1.1901339292526245\n",
      "Iteration 210: Loss 1.2362185716629028\n",
      "Iteration 220: Loss 1.197297215461731\n",
      "Iteration 230: Loss 1.282943606376648\n",
      "Iteration 240: Loss 1.229079008102417\n",
      "Iteration 250: Loss 1.2322319746017456\n",
      "Iteration 260: Loss 1.1637089252471924\n",
      "Iteration 270: Loss 1.2357497215270996\n",
      "Iteration 280: Loss 1.2375032901763916\n",
      "Iteration 290: Loss 1.3360540866851807\n",
      "Iteration 300: Loss 1.2057366371154785\n",
      "Iteration 310: Loss 1.2244172096252441\n",
      "Iteration 320: Loss 1.2142359018325806\n",
      "Iteration 330: Loss 1.2545406818389893\n",
      "Iteration 340: Loss 1.1503171920776367\n",
      "Iteration 350: Loss 1.2280099391937256\n",
      "Iteration 360: Loss 1.1993939876556396\n",
      "Iteration 370: Loss 1.2479122877120972\n",
      "Iteration 380: Loss 1.2053192853927612\n",
      "Iteration 390: Loss 1.1494815349578857\n",
      "Iteration 400: Loss 1.1527152061462402\n",
      "Iteration 410: Loss 1.23691987991333\n",
      "Iteration 420: Loss 1.1836204528808594\n",
      "Iteration 430: Loss 1.1893794536590576\n",
      "Iteration 440: Loss 1.2215523719787598\n",
      "Iteration 450: Loss 1.180035948753357\n",
      "Iteration 460: Loss 1.2791894674301147\n",
      "Iteration 470: Loss 1.1929289102554321\n",
      "Iteration 480: Loss 1.1430542469024658\n",
      "Iteration 490: Loss 1.1106622219085693\n",
      "Iteration 500: Loss 1.1711359024047852\n",
      "Iteration 510: Loss 1.2669755220413208\n",
      "Iteration 520: Loss 1.1864198446273804\n",
      "Iteration 530: Loss 1.2167562246322632\n",
      "Iteration 540: Loss 1.208292007446289\n",
      "Iteration 550: Loss 1.1602128744125366\n",
      "Iteration 560: Loss 1.0962462425231934\n",
      "Iteration 570: Loss 1.3020445108413696\n",
      "Iteration 580: Loss 1.2723931074142456\n",
      "Iteration 590: Loss 1.128843069076538\n",
      "Iteration 600: Loss 1.1196870803833008\n",
      "Iteration 610: Loss 1.1720243692398071\n",
      "Iteration 620: Loss 1.1309049129486084\n",
      "Iteration 630: Loss 1.2001619338989258\n",
      "Iteration 640: Loss 1.2140822410583496\n",
      "Iteration 650: Loss 1.177141547203064\n",
      "Iteration 660: Loss 1.1934281587600708\n",
      "Iteration 670: Loss 1.178935170173645\n",
      "Iteration 680: Loss 1.2590551376342773\n",
      "Iteration 690: Loss 1.2652565240859985\n",
      "Iteration 700: Loss 1.19502592086792\n",
      "Iteration 710: Loss 1.254523754119873\n",
      "Iteration 720: Loss 1.187406301498413\n",
      "Iteration 730: Loss 1.0804874897003174\n",
      "Iteration 740: Loss 1.1512089967727661\n",
      "Iteration 750: Loss 1.100335955619812\n",
      "Iteration 760: Loss 1.193896770477295\n",
      "Iteration 770: Loss 1.2862151861190796\n",
      "Iteration 780: Loss 1.08198082447052\n",
      "Iteration 790: Loss 1.1362231969833374\n",
      "Iteration 800: Loss 1.147023320198059\n",
      "Iteration 810: Loss 1.1785516738891602\n",
      "Iteration 820: Loss 1.1555935144424438\n",
      "Iteration 830: Loss 1.1723979711532593\n",
      "Iteration 840: Loss 1.1788828372955322\n",
      "Iteration 850: Loss 1.3195407390594482\n",
      "Iteration 860: Loss 1.1708022356033325\n",
      "Iteration 870: Loss 1.1585545539855957\n",
      "Iteration 880: Loss 1.1034603118896484\n",
      "Iteration 890: Loss 1.1421829462051392\n",
      "Iteration 900: Loss 1.1545227766036987\n",
      "Iteration 910: Loss 1.2346370220184326\n",
      "Iteration 920: Loss 1.255504846572876\n",
      "Iteration 930: Loss 1.2380955219268799\n",
      "Iteration 940: Loss 1.2573659420013428\n",
      "Iteration 950: Loss 1.086401343345642\n",
      "Iteration 960: Loss 1.1577001810073853\n",
      "Iteration 970: Loss 1.248577356338501\n",
      "Iteration 980: Loss 1.184898853302002\n",
      "Iteration 990: Loss 1.2131630182266235\n",
      "Iteration 0: Loss 2.5743284225463867\n",
      "Iteration 10: Loss 1.9015440940856934\n",
      "Iteration 20: Loss 1.7812851667404175\n",
      "Iteration 30: Loss 1.7045058012008667\n",
      "Iteration 40: Loss 1.6958675384521484\n",
      "Iteration 50: Loss 1.7291600704193115\n",
      "Iteration 60: Loss 1.6957169771194458\n",
      "Iteration 70: Loss 1.7146661281585693\n",
      "Iteration 80: Loss 1.6901791095733643\n",
      "Iteration 90: Loss 1.6474015712738037\n",
      "Iteration 100: Loss 1.5974591970443726\n",
      "Iteration 110: Loss 1.5330253839492798\n",
      "Iteration 120: Loss 1.5129824876785278\n",
      "Iteration 130: Loss 1.4702863693237305\n",
      "Iteration 140: Loss 1.5406779050827026\n",
      "Iteration 150: Loss 1.5175588130950928\n",
      "Iteration 160: Loss 1.4813926219940186\n",
      "Iteration 170: Loss 1.4294695854187012\n",
      "Iteration 180: Loss 1.5236499309539795\n",
      "Iteration 190: Loss 1.4299917221069336\n",
      "Iteration 200: Loss 1.4840190410614014\n",
      "Iteration 210: Loss 1.4252562522888184\n",
      "Iteration 220: Loss 1.517183780670166\n",
      "Iteration 230: Loss 1.3985933065414429\n",
      "Iteration 240: Loss 1.473162293434143\n",
      "Iteration 250: Loss 1.4669771194458008\n",
      "Iteration 260: Loss 1.3967760801315308\n",
      "Iteration 270: Loss 1.469983696937561\n",
      "Iteration 280: Loss 1.5270835161209106\n",
      "Iteration 290: Loss 1.462475061416626\n",
      "Iteration 300: Loss 1.4989053010940552\n",
      "Iteration 310: Loss 1.4597795009613037\n",
      "Iteration 320: Loss 1.4089040756225586\n",
      "Iteration 330: Loss 1.4778327941894531\n",
      "Iteration 340: Loss 1.4401148557662964\n",
      "Iteration 350: Loss 1.4968494176864624\n",
      "Iteration 360: Loss 1.4545329809188843\n",
      "Iteration 370: Loss 1.540107011795044\n",
      "Iteration 380: Loss 1.4579522609710693\n",
      "Iteration 390: Loss 1.4302269220352173\n",
      "Iteration 400: Loss 1.5236783027648926\n",
      "Iteration 410: Loss 1.4389593601226807\n",
      "Iteration 420: Loss 1.3501322269439697\n",
      "Iteration 430: Loss 1.4128217697143555\n",
      "Iteration 440: Loss 1.4015178680419922\n",
      "Iteration 450: Loss 1.4938533306121826\n",
      "Iteration 460: Loss 1.429168462753296\n",
      "Iteration 470: Loss 1.3819981813430786\n",
      "Iteration 480: Loss 1.3493837118148804\n",
      "Iteration 490: Loss 1.447706937789917\n",
      "Iteration 500: Loss 1.383225917816162\n",
      "Iteration 510: Loss 1.4132804870605469\n",
      "Iteration 520: Loss 1.517377257347107\n",
      "Iteration 530: Loss 1.4042218923568726\n",
      "Iteration 540: Loss 1.478495478630066\n",
      "Iteration 550: Loss 1.4880561828613281\n",
      "Iteration 560: Loss 1.4565986394882202\n",
      "Iteration 570: Loss 1.48479425907135\n",
      "Iteration 580: Loss 1.3641189336776733\n",
      "Iteration 590: Loss 1.5035320520401\n",
      "Iteration 600: Loss 1.453332781791687\n",
      "Iteration 610: Loss 1.5155541896820068\n",
      "Iteration 620: Loss 1.4592807292938232\n",
      "Iteration 630: Loss 1.4600635766983032\n",
      "Iteration 640: Loss 1.3621701002120972\n",
      "Iteration 650: Loss 1.4241228103637695\n",
      "Iteration 660: Loss 1.5532493591308594\n",
      "Iteration 670: Loss 1.415872573852539\n",
      "Iteration 680: Loss 1.4733773469924927\n",
      "Iteration 690: Loss 1.5502665042877197\n",
      "Iteration 700: Loss 1.4044530391693115\n",
      "Iteration 710: Loss 1.524113655090332\n",
      "Iteration 720: Loss 1.3978402614593506\n",
      "Iteration 730: Loss 1.4384571313858032\n",
      "Iteration 740: Loss 1.462019681930542\n",
      "Iteration 750: Loss 1.463750958442688\n",
      "Iteration 760: Loss 1.4510908126831055\n",
      "Iteration 770: Loss 1.472044587135315\n",
      "Iteration 780: Loss 1.5386073589324951\n",
      "Iteration 790: Loss 1.417857050895691\n",
      "Iteration 800: Loss 1.3693944215774536\n",
      "Iteration 810: Loss 1.3895243406295776\n",
      "Iteration 820: Loss 1.4937506914138794\n",
      "Iteration 830: Loss 1.3733408451080322\n",
      "Iteration 840: Loss 1.4697293043136597\n",
      "Iteration 850: Loss 1.4305812120437622\n",
      "Iteration 860: Loss 1.4921150207519531\n",
      "Iteration 870: Loss 1.5226727724075317\n",
      "Iteration 880: Loss 1.3974426984786987\n",
      "Iteration 890: Loss 1.4315710067749023\n",
      "Iteration 900: Loss 1.4888668060302734\n",
      "Iteration 910: Loss 1.4545543193817139\n",
      "Iteration 920: Loss 1.4474096298217773\n",
      "Iteration 930: Loss 1.4696552753448486\n",
      "Iteration 940: Loss 1.3635727167129517\n",
      "Iteration 950: Loss 1.4752081632614136\n",
      "Iteration 960: Loss 1.3897149562835693\n",
      "Iteration 970: Loss 1.546139121055603\n",
      "Iteration 980: Loss 1.512678861618042\n",
      "Iteration 990: Loss 1.5156004428863525\n",
      "Iteration 0: Loss 2.831116199493408\n",
      "Iteration 10: Loss 2.2397825717926025\n",
      "Iteration 20: Loss 2.0388073921203613\n",
      "Iteration 30: Loss 1.954471468925476\n",
      "Iteration 40: Loss 2.098660945892334\n",
      "Iteration 50: Loss 2.070953369140625\n",
      "Iteration 60: Loss 1.9220987558364868\n",
      "Iteration 70: Loss 1.9581079483032227\n",
      "Iteration 80: Loss 1.8829848766326904\n",
      "Iteration 90: Loss 1.8045835494995117\n",
      "Iteration 100: Loss 1.7757776975631714\n",
      "Iteration 110: Loss 1.7190049886703491\n",
      "Iteration 120: Loss 1.7713234424591064\n",
      "Iteration 130: Loss 1.7500288486480713\n",
      "Iteration 140: Loss 1.752855658531189\n",
      "Iteration 150: Loss 1.7525659799575806\n",
      "Iteration 160: Loss 1.7306933403015137\n",
      "Iteration 170: Loss 1.7279194593429565\n",
      "Iteration 180: Loss 1.8071681261062622\n",
      "Iteration 190: Loss 1.6303848028182983\n",
      "Iteration 200: Loss 1.7095131874084473\n",
      "Iteration 210: Loss 1.7497682571411133\n",
      "Iteration 220: Loss 1.7914204597473145\n",
      "Iteration 230: Loss 1.7148934602737427\n",
      "Iteration 240: Loss 1.7065224647521973\n",
      "Iteration 250: Loss 1.760524034500122\n",
      "Iteration 260: Loss 1.6689003705978394\n",
      "Iteration 270: Loss 1.7347257137298584\n",
      "Iteration 280: Loss 1.7003921270370483\n",
      "Iteration 290: Loss 1.6889389753341675\n",
      "Iteration 300: Loss 1.6440387964248657\n",
      "Iteration 310: Loss 1.7174972295761108\n",
      "Iteration 320: Loss 1.6439870595932007\n",
      "Iteration 330: Loss 1.7539883852005005\n",
      "Iteration 340: Loss 1.7198054790496826\n",
      "Iteration 350: Loss 1.7684191465377808\n",
      "Iteration 360: Loss 1.666319727897644\n",
      "Iteration 370: Loss 1.7371394634246826\n",
      "Iteration 380: Loss 1.6668323278427124\n",
      "Iteration 390: Loss 1.64248788356781\n",
      "Iteration 400: Loss 1.8744874000549316\n",
      "Iteration 410: Loss 1.7414820194244385\n",
      "Iteration 420: Loss 1.569836974143982\n",
      "Iteration 430: Loss 1.7536076307296753\n",
      "Iteration 440: Loss 1.6466612815856934\n",
      "Iteration 450: Loss 1.6653058528900146\n",
      "Iteration 460: Loss 1.7313584089279175\n",
      "Iteration 470: Loss 1.785854697227478\n",
      "Iteration 480: Loss 1.7581279277801514\n",
      "Iteration 490: Loss 1.709046483039856\n",
      "Iteration 500: Loss 1.734490990638733\n",
      "Iteration 510: Loss 1.7679722309112549\n",
      "Iteration 520: Loss 1.662499189376831\n",
      "Iteration 530: Loss 1.6380982398986816\n",
      "Iteration 540: Loss 1.734545111656189\n",
      "Iteration 550: Loss 1.8213558197021484\n",
      "Iteration 560: Loss 1.6099803447723389\n",
      "Iteration 570: Loss 1.704172968864441\n",
      "Iteration 580: Loss 1.7974648475646973\n",
      "Iteration 590: Loss 1.6180464029312134\n",
      "Iteration 600: Loss 1.6608346700668335\n",
      "Iteration 610: Loss 1.7716386318206787\n",
      "Iteration 620: Loss 1.7001091241836548\n",
      "Iteration 630: Loss 1.6901596784591675\n",
      "Iteration 640: Loss 1.560074806213379\n",
      "Iteration 650: Loss 1.7214548587799072\n",
      "Iteration 660: Loss 1.6516677141189575\n",
      "Iteration 670: Loss 1.7063288688659668\n",
      "Iteration 680: Loss 1.7596739530563354\n",
      "Iteration 690: Loss 1.8145315647125244\n",
      "Iteration 700: Loss 1.626751184463501\n",
      "Iteration 710: Loss 1.6004984378814697\n",
      "Iteration 720: Loss 1.6872320175170898\n",
      "Iteration 730: Loss 1.7469433546066284\n",
      "Iteration 740: Loss 1.6372548341751099\n",
      "Iteration 750: Loss 1.7747327089309692\n",
      "Iteration 760: Loss 1.7519612312316895\n",
      "Iteration 770: Loss 1.6792917251586914\n",
      "Iteration 780: Loss 1.7291538715362549\n",
      "Iteration 790: Loss 1.7370089292526245\n",
      "Iteration 800: Loss 1.6328173875808716\n",
      "Iteration 810: Loss 1.680740237236023\n",
      "Iteration 820: Loss 1.8086390495300293\n",
      "Iteration 830: Loss 1.7287464141845703\n",
      "Iteration 840: Loss 1.6719589233398438\n",
      "Iteration 850: Loss 1.6824162006378174\n",
      "Iteration 860: Loss 1.7820411920547485\n",
      "Iteration 870: Loss 1.6593413352966309\n",
      "Iteration 880: Loss 1.6665364503860474\n",
      "Iteration 890: Loss 1.8009964227676392\n",
      "Iteration 900: Loss 1.6967957019805908\n",
      "Iteration 910: Loss 1.6822054386138916\n",
      "Iteration 920: Loss 1.7758113145828247\n",
      "Iteration 930: Loss 1.6092948913574219\n",
      "Iteration 940: Loss 1.6943681240081787\n",
      "Iteration 950: Loss 1.673810362815857\n",
      "Iteration 960: Loss 1.621166706085205\n",
      "Iteration 970: Loss 1.7073731422424316\n",
      "Iteration 980: Loss 1.6659746170043945\n",
      "Iteration 990: Loss 1.8309385776519775\n",
      "Iteration 0: Loss 3.057802677154541\n",
      "Iteration 10: Loss 2.6425437927246094\n",
      "Iteration 20: Loss 2.455099105834961\n",
      "Iteration 30: Loss 2.301443576812744\n",
      "Iteration 40: Loss 2.3480420112609863\n",
      "Iteration 50: Loss 2.3805787563323975\n",
      "Iteration 60: Loss 2.3077099323272705\n",
      "Iteration 70: Loss 2.2759904861450195\n",
      "Iteration 80: Loss 2.183048725128174\n",
      "Iteration 90: Loss 2.167422294616699\n",
      "Iteration 100: Loss 1.8780173063278198\n",
      "Iteration 110: Loss 1.9336051940917969\n",
      "Iteration 120: Loss 1.8845882415771484\n",
      "Iteration 130: Loss 2.100598096847534\n",
      "Iteration 140: Loss 2.0357258319854736\n",
      "Iteration 150: Loss 1.9417192935943604\n",
      "Iteration 160: Loss 2.054668664932251\n",
      "Iteration 170: Loss 1.8114877939224243\n",
      "Iteration 180: Loss 2.002807855606079\n",
      "Iteration 190: Loss 1.8549902439117432\n",
      "Iteration 200: Loss 1.9975923299789429\n",
      "Iteration 210: Loss 1.9543273448944092\n",
      "Iteration 220: Loss 2.069608211517334\n",
      "Iteration 230: Loss 2.0888285636901855\n",
      "Iteration 240: Loss 1.9687126874923706\n",
      "Iteration 250: Loss 1.883194088935852\n",
      "Iteration 260: Loss 1.9379721879959106\n",
      "Iteration 270: Loss 1.8457257747650146\n",
      "Iteration 280: Loss 1.997607707977295\n",
      "Iteration 290: Loss 1.8677761554718018\n",
      "Iteration 300: Loss 1.9676518440246582\n",
      "Iteration 310: Loss 1.9301437139511108\n",
      "Iteration 320: Loss 1.8857429027557373\n",
      "Iteration 330: Loss 2.014909029006958\n",
      "Iteration 340: Loss 2.0122764110565186\n",
      "Iteration 350: Loss 1.9420325756072998\n",
      "Iteration 360: Loss 1.9821655750274658\n",
      "Iteration 370: Loss 1.8943605422973633\n",
      "Iteration 380: Loss 2.0786690711975098\n",
      "Iteration 390: Loss 1.9766157865524292\n",
      "Iteration 400: Loss 1.9010167121887207\n",
      "Iteration 410: Loss 2.033775568008423\n",
      "Iteration 420: Loss 1.952813982963562\n",
      "Iteration 430: Loss 2.137967824935913\n",
      "Iteration 440: Loss 1.9569517374038696\n",
      "Iteration 450: Loss 1.9910911321640015\n",
      "Iteration 460: Loss 2.075493812561035\n",
      "Iteration 470: Loss 1.954203486442566\n",
      "Iteration 480: Loss 2.0076711177825928\n",
      "Iteration 490: Loss 1.9808993339538574\n",
      "Iteration 500: Loss 1.8590608835220337\n",
      "Iteration 510: Loss 1.869523048400879\n",
      "Iteration 520: Loss 1.7559432983398438\n",
      "Iteration 530: Loss 2.0198488235473633\n",
      "Iteration 540: Loss 1.993377685546875\n",
      "Iteration 550: Loss 1.9938430786132812\n",
      "Iteration 560: Loss 1.940935492515564\n",
      "Iteration 570: Loss 1.9725455045700073\n",
      "Iteration 580: Loss 1.8457069396972656\n",
      "Iteration 590: Loss 2.0244147777557373\n",
      "Iteration 600: Loss 1.90742027759552\n",
      "Iteration 610: Loss 2.0350840091705322\n",
      "Iteration 620: Loss 1.9869366884231567\n",
      "Iteration 630: Loss 1.962462067604065\n",
      "Iteration 640: Loss 1.9500564336776733\n",
      "Iteration 650: Loss 2.0661137104034424\n",
      "Iteration 660: Loss 1.9459608793258667\n",
      "Iteration 670: Loss 1.8771636486053467\n",
      "Iteration 680: Loss 1.8410890102386475\n",
      "Iteration 690: Loss 2.0035643577575684\n",
      "Iteration 700: Loss 2.0236687660217285\n",
      "Iteration 710: Loss 1.9074983596801758\n",
      "Iteration 720: Loss 1.9177565574645996\n",
      "Iteration 730: Loss 1.8583909273147583\n",
      "Iteration 740: Loss 2.104095220565796\n",
      "Iteration 750: Loss 2.0271835327148438\n",
      "Iteration 760: Loss 1.9228593111038208\n",
      "Iteration 770: Loss 2.0670578479766846\n",
      "Iteration 780: Loss 2.046565294265747\n",
      "Iteration 790: Loss 1.9531553983688354\n",
      "Iteration 800: Loss 1.9588061571121216\n",
      "Iteration 810: Loss 1.898606538772583\n",
      "Iteration 820: Loss 1.978959083557129\n",
      "Iteration 830: Loss 2.025200366973877\n",
      "Iteration 840: Loss 2.097683906555176\n",
      "Iteration 850: Loss 1.984620213508606\n",
      "Iteration 860: Loss 1.924173355102539\n",
      "Iteration 870: Loss 1.9864169359207153\n",
      "Iteration 880: Loss 2.059394121170044\n",
      "Iteration 890: Loss 2.031222105026245\n",
      "Iteration 900: Loss 1.918840765953064\n",
      "Iteration 910: Loss 1.9358323812484741\n",
      "Iteration 920: Loss 1.8846876621246338\n",
      "Iteration 930: Loss 1.8609042167663574\n",
      "Iteration 940: Loss 1.9931995868682861\n",
      "Iteration 950: Loss 2.0817108154296875\n",
      "Iteration 960: Loss 2.028350830078125\n",
      "Iteration 970: Loss 2.0756516456604004\n",
      "Iteration 980: Loss 2.0648741722106934\n",
      "Iteration 990: Loss 2.047091245651245\n",
      "Iteration 0: Loss 3.337999105453491\n",
      "Iteration 10: Loss 2.76741623878479\n",
      "Iteration 20: Loss 2.785674571990967\n",
      "Iteration 30: Loss 2.5587375164031982\n",
      "Iteration 40: Loss 2.697019577026367\n",
      "Iteration 50: Loss 2.626765251159668\n",
      "Iteration 60: Loss 2.6446986198425293\n",
      "Iteration 70: Loss 2.5200531482696533\n",
      "Iteration 80: Loss 2.3527865409851074\n",
      "Iteration 90: Loss 2.169034719467163\n",
      "Iteration 100: Loss 2.2396135330200195\n",
      "Iteration 110: Loss 2.2085752487182617\n",
      "Iteration 120: Loss 2.2931740283966064\n",
      "Iteration 130: Loss 2.2530064582824707\n",
      "Iteration 140: Loss 2.253587245941162\n",
      "Iteration 150: Loss 2.278836965560913\n",
      "Iteration 160: Loss 2.2436227798461914\n",
      "Iteration 170: Loss 2.3156917095184326\n",
      "Iteration 180: Loss 2.252689838409424\n",
      "Iteration 190: Loss 2.3491926193237305\n",
      "Iteration 200: Loss 2.2570176124572754\n",
      "Iteration 210: Loss 2.2767558097839355\n",
      "Iteration 220: Loss 2.271362066268921\n",
      "Iteration 230: Loss 2.2430741786956787\n",
      "Iteration 240: Loss 2.139089345932007\n",
      "Iteration 250: Loss 2.128688335418701\n",
      "Iteration 260: Loss 2.2470688819885254\n",
      "Iteration 270: Loss 2.2231764793395996\n",
      "Iteration 280: Loss 2.3177011013031006\n",
      "Iteration 290: Loss 2.160792112350464\n",
      "Iteration 300: Loss 2.2112841606140137\n",
      "Iteration 310: Loss 2.259380340576172\n",
      "Iteration 320: Loss 2.3507578372955322\n",
      "Iteration 330: Loss 2.1836953163146973\n",
      "Iteration 340: Loss 2.2017157077789307\n",
      "Iteration 350: Loss 2.14841628074646\n",
      "Iteration 360: Loss 2.1399784088134766\n",
      "Iteration 370: Loss 2.1576735973358154\n",
      "Iteration 380: Loss 2.1318109035491943\n",
      "Iteration 390: Loss 2.179058074951172\n",
      "Iteration 400: Loss 2.261634588241577\n",
      "Iteration 410: Loss 2.2778494358062744\n",
      "Iteration 420: Loss 2.118030309677124\n",
      "Iteration 430: Loss 2.150968313217163\n",
      "Iteration 440: Loss 2.3400838375091553\n",
      "Iteration 450: Loss 2.1177430152893066\n",
      "Iteration 460: Loss 2.4406938552856445\n",
      "Iteration 470: Loss 2.213496208190918\n",
      "Iteration 480: Loss 2.156874418258667\n",
      "Iteration 490: Loss 2.128239870071411\n",
      "Iteration 500: Loss 2.2419867515563965\n",
      "Iteration 510: Loss 2.344989538192749\n",
      "Iteration 520: Loss 2.2435293197631836\n",
      "Iteration 530: Loss 2.1239562034606934\n",
      "Iteration 540: Loss 2.170444965362549\n",
      "Iteration 550: Loss 2.3010671138763428\n",
      "Iteration 560: Loss 2.122774124145508\n",
      "Iteration 570: Loss 2.0343081951141357\n",
      "Iteration 580: Loss 2.18208909034729\n",
      "Iteration 590: Loss 2.210369825363159\n",
      "Iteration 600: Loss 2.282119035720825\n",
      "Iteration 610: Loss 2.2164385318756104\n",
      "Iteration 620: Loss 2.1358892917633057\n",
      "Iteration 630: Loss 2.268460988998413\n",
      "Iteration 640: Loss 2.1094517707824707\n",
      "Iteration 650: Loss 2.245579957962036\n",
      "Iteration 660: Loss 2.313506603240967\n",
      "Iteration 670: Loss 2.118673324584961\n",
      "Iteration 680: Loss 2.1325414180755615\n",
      "Iteration 690: Loss 2.2437033653259277\n",
      "Iteration 700: Loss 2.1611642837524414\n",
      "Iteration 710: Loss 2.129615068435669\n",
      "Iteration 720: Loss 2.167790651321411\n",
      "Iteration 730: Loss 2.294157028198242\n",
      "Iteration 740: Loss 2.289857864379883\n",
      "Iteration 750: Loss 2.2161433696746826\n",
      "Iteration 760: Loss 2.1466574668884277\n",
      "Iteration 770: Loss 2.180727243423462\n",
      "Iteration 780: Loss 2.2638113498687744\n",
      "Iteration 790: Loss 2.1712965965270996\n",
      "Iteration 800: Loss 2.061471462249756\n",
      "Iteration 810: Loss 2.140009641647339\n",
      "Iteration 820: Loss 2.2008934020996094\n",
      "Iteration 830: Loss 2.1963050365448\n",
      "Iteration 840: Loss 2.245962142944336\n",
      "Iteration 850: Loss 2.308696985244751\n",
      "Iteration 860: Loss 2.2010769844055176\n",
      "Iteration 870: Loss 2.1028339862823486\n",
      "Iteration 880: Loss 2.169743299484253\n",
      "Iteration 890: Loss 2.3127634525299072\n",
      "Iteration 900: Loss 2.1247613430023193\n",
      "Iteration 910: Loss 2.236260175704956\n",
      "Iteration 920: Loss 2.186377763748169\n",
      "Iteration 930: Loss 2.115835428237915\n",
      "Iteration 940: Loss 2.3191006183624268\n",
      "Iteration 950: Loss 2.1546146869659424\n",
      "Iteration 960: Loss 2.0579724311828613\n",
      "Iteration 970: Loss 2.2735211849212646\n",
      "Iteration 980: Loss 2.1433889865875244\n",
      "Iteration 990: Loss 2.1378061771392822\n",
      "Iteration 0: Loss 3.583817481994629\n",
      "Iteration 10: Loss 3.234297037124634\n",
      "Iteration 20: Loss 2.9127485752105713\n",
      "Iteration 30: Loss 2.947976589202881\n",
      "Iteration 40: Loss 2.8988752365112305\n",
      "Iteration 50: Loss 2.729275941848755\n",
      "Iteration 60: Loss 2.8031277656555176\n",
      "Iteration 70: Loss 2.669797897338867\n",
      "Iteration 80: Loss 2.547231912612915\n",
      "Iteration 90: Loss 2.459625244140625\n",
      "Iteration 100: Loss 2.45206356048584\n",
      "Iteration 110: Loss 2.5122318267822266\n",
      "Iteration 120: Loss 2.4712867736816406\n",
      "Iteration 130: Loss 2.609405040740967\n",
      "Iteration 140: Loss 2.4990971088409424\n",
      "Iteration 150: Loss 2.366849660873413\n",
      "Iteration 160: Loss 2.5076844692230225\n",
      "Iteration 170: Loss 2.354203462600708\n",
      "Iteration 180: Loss 2.5404536724090576\n",
      "Iteration 190: Loss 2.3292558193206787\n",
      "Iteration 200: Loss 2.4574596881866455\n",
      "Iteration 210: Loss 2.462002754211426\n",
      "Iteration 220: Loss 2.4669764041900635\n",
      "Iteration 230: Loss 2.4108262062072754\n",
      "Iteration 240: Loss 2.4794766902923584\n",
      "Iteration 250: Loss 2.505105495452881\n",
      "Iteration 260: Loss 2.5031847953796387\n",
      "Iteration 270: Loss 2.496155023574829\n",
      "Iteration 280: Loss 2.3697566986083984\n",
      "Iteration 290: Loss 2.4535844326019287\n",
      "Iteration 300: Loss 2.384896755218506\n",
      "Iteration 310: Loss 2.4194416999816895\n",
      "Iteration 320: Loss 2.33096981048584\n",
      "Iteration 330: Loss 2.628927707672119\n",
      "Iteration 340: Loss 2.29258131980896\n",
      "Iteration 350: Loss 2.4484951496124268\n",
      "Iteration 360: Loss 2.4384570121765137\n",
      "Iteration 370: Loss 2.567591667175293\n",
      "Iteration 380: Loss 2.3695945739746094\n",
      "Iteration 390: Loss 2.412492513656616\n",
      "Iteration 400: Loss 2.5709948539733887\n",
      "Iteration 410: Loss 2.4033167362213135\n",
      "Iteration 420: Loss 2.5669467449188232\n",
      "Iteration 430: Loss 2.4911253452301025\n",
      "Iteration 440: Loss 2.5681440830230713\n",
      "Iteration 450: Loss 2.458505868911743\n",
      "Iteration 460: Loss 2.3835926055908203\n",
      "Iteration 470: Loss 2.2877562046051025\n",
      "Iteration 480: Loss 2.3960914611816406\n",
      "Iteration 490: Loss 2.4067678451538086\n",
      "Iteration 500: Loss 2.513237476348877\n",
      "Iteration 510: Loss 2.3636627197265625\n",
      "Iteration 520: Loss 2.252105474472046\n",
      "Iteration 530: Loss 2.4258248805999756\n",
      "Iteration 540: Loss 2.413987159729004\n",
      "Iteration 550: Loss 2.388601779937744\n",
      "Iteration 560: Loss 2.401202917098999\n",
      "Iteration 570: Loss 2.4716010093688965\n",
      "Iteration 580: Loss 2.3873441219329834\n",
      "Iteration 590: Loss 2.3776156902313232\n",
      "Iteration 600: Loss 2.541961193084717\n",
      "Iteration 610: Loss 2.424142360687256\n",
      "Iteration 620: Loss 2.385324716567993\n",
      "Iteration 630: Loss 2.5474607944488525\n",
      "Iteration 640: Loss 2.4193904399871826\n",
      "Iteration 650: Loss 2.4006764888763428\n",
      "Iteration 660: Loss 2.374427318572998\n",
      "Iteration 670: Loss 2.3818490505218506\n",
      "Iteration 680: Loss 2.547529935836792\n",
      "Iteration 690: Loss 2.4350686073303223\n",
      "Iteration 700: Loss 2.389498472213745\n",
      "Iteration 710: Loss 2.3431670665740967\n",
      "Iteration 720: Loss 2.5256869792938232\n",
      "Iteration 730: Loss 2.471499443054199\n",
      "Iteration 740: Loss 2.4741878509521484\n",
      "Iteration 750: Loss 2.453549861907959\n",
      "Iteration 760: Loss 2.5584137439727783\n",
      "Iteration 770: Loss 2.5708682537078857\n",
      "Iteration 780: Loss 2.3063318729400635\n",
      "Iteration 790: Loss 2.3908393383026123\n",
      "Iteration 800: Loss 2.4043726921081543\n",
      "Iteration 810: Loss 2.486098289489746\n",
      "Iteration 820: Loss 2.3413140773773193\n",
      "Iteration 830: Loss 2.3434038162231445\n",
      "Iteration 840: Loss 2.4585015773773193\n",
      "Iteration 850: Loss 2.458989381790161\n",
      "Iteration 860: Loss 2.3832297325134277\n",
      "Iteration 870: Loss 2.383772134780884\n",
      "Iteration 880: Loss 2.373223066329956\n",
      "Iteration 890: Loss 2.4533722400665283\n",
      "Iteration 900: Loss 2.3305978775024414\n",
      "Iteration 910: Loss 2.3379108905792236\n",
      "Iteration 920: Loss 2.476412534713745\n",
      "Iteration 930: Loss 2.472029209136963\n",
      "Iteration 940: Loss 2.464984178543091\n",
      "Iteration 950: Loss 2.6175246238708496\n",
      "Iteration 960: Loss 2.518402338027954\n",
      "Iteration 970: Loss 2.4731757640838623\n",
      "Iteration 980: Loss 2.377208948135376\n",
      "Iteration 990: Loss 2.3378853797912598\n",
      "Iteration 0: Loss 3.833756923675537\n",
      "Iteration 10: Loss 3.473820686340332\n",
      "Iteration 20: Loss 3.1691043376922607\n",
      "Iteration 30: Loss 3.1804075241088867\n",
      "Iteration 40: Loss 3.102975606918335\n",
      "Iteration 50: Loss 3.2173049449920654\n",
      "Iteration 60: Loss 3.0104386806488037\n",
      "Iteration 70: Loss 2.976487874984741\n",
      "Iteration 80: Loss 3.00293231010437\n",
      "Iteration 90: Loss 2.8150086402893066\n",
      "Iteration 100: Loss 2.9616641998291016\n",
      "Iteration 110: Loss 2.8863234519958496\n",
      "Iteration 120: Loss 2.9945685863494873\n",
      "Iteration 130: Loss 2.7897026538848877\n",
      "Iteration 140: Loss 2.7861437797546387\n",
      "Iteration 150: Loss 2.8149776458740234\n",
      "Iteration 160: Loss 2.7098476886749268\n",
      "Iteration 170: Loss 2.689204216003418\n",
      "Iteration 180: Loss 2.774111032485962\n",
      "Iteration 190: Loss 2.6210339069366455\n",
      "Iteration 200: Loss 2.630795478820801\n",
      "Iteration 210: Loss 2.7061009407043457\n",
      "Iteration 220: Loss 2.7547943592071533\n",
      "Iteration 230: Loss 2.6678223609924316\n",
      "Iteration 240: Loss 2.5669655799865723\n",
      "Iteration 250: Loss 2.6844677925109863\n",
      "Iteration 260: Loss 2.587163209915161\n",
      "Iteration 270: Loss 2.6536905765533447\n",
      "Iteration 280: Loss 2.5146994590759277\n",
      "Iteration 290: Loss 2.7486865520477295\n",
      "Iteration 300: Loss 2.5527236461639404\n",
      "Iteration 310: Loss 2.5256600379943848\n",
      "Iteration 320: Loss 2.687685012817383\n",
      "Iteration 330: Loss 2.6534218788146973\n",
      "Iteration 340: Loss 2.692638635635376\n",
      "Iteration 350: Loss 2.5246734619140625\n",
      "Iteration 360: Loss 2.667881488800049\n",
      "Iteration 370: Loss 2.6157584190368652\n",
      "Iteration 380: Loss 2.5698082447052\n",
      "Iteration 390: Loss 2.3797295093536377\n",
      "Iteration 400: Loss 2.6435422897338867\n",
      "Iteration 410: Loss 2.5532662868499756\n",
      "Iteration 420: Loss 2.637648820877075\n",
      "Iteration 430: Loss 2.6469104290008545\n",
      "Iteration 440: Loss 2.609415054321289\n",
      "Iteration 450: Loss 2.631643533706665\n",
      "Iteration 460: Loss 2.7885313034057617\n",
      "Iteration 470: Loss 2.7155754566192627\n",
      "Iteration 480: Loss 2.6302847862243652\n",
      "Iteration 490: Loss 2.641425132751465\n",
      "Iteration 500: Loss 2.7179200649261475\n",
      "Iteration 510: Loss 2.488983631134033\n",
      "Iteration 520: Loss 2.6489944458007812\n",
      "Iteration 530: Loss 2.873347282409668\n",
      "Iteration 540: Loss 2.606424331665039\n",
      "Iteration 550: Loss 2.7701117992401123\n",
      "Iteration 560: Loss 2.616854190826416\n",
      "Iteration 570: Loss 2.6261188983917236\n",
      "Iteration 580: Loss 2.585055112838745\n",
      "Iteration 590: Loss 2.6425132751464844\n",
      "Iteration 600: Loss 2.6129016876220703\n",
      "Iteration 610: Loss 2.563542127609253\n",
      "Iteration 620: Loss 2.4703426361083984\n",
      "Iteration 630: Loss 2.8033409118652344\n",
      "Iteration 640: Loss 2.532914161682129\n",
      "Iteration 650: Loss 2.670851469039917\n",
      "Iteration 660: Loss 2.7652735710144043\n",
      "Iteration 670: Loss 2.609041452407837\n",
      "Iteration 680: Loss 2.4366142749786377\n",
      "Iteration 690: Loss 2.5768136978149414\n",
      "Iteration 700: Loss 2.6560795307159424\n",
      "Iteration 710: Loss 2.6145849227905273\n",
      "Iteration 720: Loss 2.6854515075683594\n",
      "Iteration 730: Loss 2.5405662059783936\n",
      "Iteration 740: Loss 2.633756637573242\n",
      "Iteration 750: Loss 2.586212158203125\n",
      "Iteration 760: Loss 2.7112488746643066\n",
      "Iteration 770: Loss 2.6829416751861572\n",
      "Iteration 780: Loss 2.6776812076568604\n",
      "Iteration 790: Loss 2.6903738975524902\n",
      "Iteration 800: Loss 2.5195693969726562\n",
      "Iteration 810: Loss 2.6473920345306396\n",
      "Iteration 820: Loss 2.6533594131469727\n",
      "Iteration 830: Loss 2.6273789405822754\n",
      "Iteration 840: Loss 2.7518322467803955\n",
      "Iteration 850: Loss 2.7294912338256836\n",
      "Iteration 860: Loss 2.6111037731170654\n",
      "Iteration 870: Loss 2.6980459690093994\n",
      "Iteration 880: Loss 2.738452196121216\n",
      "Iteration 890: Loss 2.7819879055023193\n",
      "Iteration 900: Loss 2.6325347423553467\n",
      "Iteration 910: Loss 2.6179299354553223\n",
      "Iteration 920: Loss 2.7379770278930664\n",
      "Iteration 930: Loss 2.6169795989990234\n",
      "Iteration 940: Loss 2.530817747116089\n",
      "Iteration 950: Loss 2.6785213947296143\n",
      "Iteration 960: Loss 2.608905792236328\n",
      "Iteration 970: Loss 2.7556865215301514\n",
      "Iteration 980: Loss 2.5508627891540527\n",
      "Iteration 990: Loss 2.7051684856414795\n",
      "Iteration 0: Loss 4.014076232910156\n",
      "Iteration 10: Loss 3.5620858669281006\n",
      "Iteration 20: Loss 3.5474166870117188\n",
      "Iteration 30: Loss 3.590999126434326\n",
      "Iteration 40: Loss 3.4804842472076416\n",
      "Iteration 50: Loss 3.406191825866699\n",
      "Iteration 60: Loss 3.3133199214935303\n",
      "Iteration 70: Loss 3.189972162246704\n",
      "Iteration 80: Loss 3.060857057571411\n",
      "Iteration 90: Loss 3.075533866882324\n",
      "Iteration 100: Loss 2.9757297039031982\n",
      "Iteration 110: Loss 3.076638698577881\n",
      "Iteration 120: Loss 2.848369836807251\n",
      "Iteration 130: Loss 2.9875307083129883\n",
      "Iteration 140: Loss 2.8183252811431885\n",
      "Iteration 150: Loss 2.8589131832122803\n",
      "Iteration 160: Loss 2.7633471488952637\n",
      "Iteration 170: Loss 2.916508436203003\n",
      "Iteration 180: Loss 2.8194937705993652\n",
      "Iteration 190: Loss 2.7992682456970215\n",
      "Iteration 200: Loss 2.9124627113342285\n",
      "Iteration 210: Loss 2.799708127975464\n",
      "Iteration 220: Loss 2.941728353500366\n",
      "Iteration 230: Loss 2.784895896911621\n",
      "Iteration 240: Loss 2.7637546062469482\n",
      "Iteration 250: Loss 2.792684316635132\n",
      "Iteration 260: Loss 2.897127628326416\n",
      "Iteration 270: Loss 2.8994836807250977\n",
      "Iteration 280: Loss 2.9712250232696533\n",
      "Iteration 290: Loss 2.835207223892212\n",
      "Iteration 300: Loss 2.8468549251556396\n",
      "Iteration 310: Loss 2.898249864578247\n",
      "Iteration 320: Loss 2.7229421138763428\n",
      "Iteration 330: Loss 2.8521056175231934\n",
      "Iteration 340: Loss 2.962468385696411\n",
      "Iteration 350: Loss 2.796937942504883\n",
      "Iteration 360: Loss 2.617650032043457\n",
      "Iteration 370: Loss 3.144767999649048\n",
      "Iteration 380: Loss 2.7661123275756836\n",
      "Iteration 390: Loss 2.8051741123199463\n",
      "Iteration 400: Loss 2.9283077716827393\n",
      "Iteration 410: Loss 2.8489859104156494\n",
      "Iteration 420: Loss 2.974757671356201\n",
      "Iteration 430: Loss 2.8884620666503906\n",
      "Iteration 440: Loss 2.669970989227295\n",
      "Iteration 450: Loss 3.05849552154541\n",
      "Iteration 460: Loss 2.828403949737549\n",
      "Iteration 470: Loss 2.8650245666503906\n",
      "Iteration 480: Loss 2.8783459663391113\n",
      "Iteration 490: Loss 2.859938859939575\n",
      "Iteration 500: Loss 2.941781520843506\n",
      "Iteration 510: Loss 2.8416831493377686\n",
      "Iteration 520: Loss 2.911081075668335\n",
      "Iteration 530: Loss 2.8798139095306396\n",
      "Iteration 540: Loss 2.878335952758789\n",
      "Iteration 550: Loss 2.8106651306152344\n",
      "Iteration 560: Loss 2.845224380493164\n",
      "Iteration 570: Loss 3.032419204711914\n",
      "Iteration 580: Loss 2.7977256774902344\n",
      "Iteration 590: Loss 2.6788980960845947\n",
      "Iteration 600: Loss 2.8881726264953613\n",
      "Iteration 610: Loss 2.776949644088745\n",
      "Iteration 620: Loss 2.870518207550049\n",
      "Iteration 630: Loss 2.8265318870544434\n",
      "Iteration 640: Loss 2.8188424110412598\n",
      "Iteration 650: Loss 3.015578031539917\n",
      "Iteration 660: Loss 2.702625036239624\n",
      "Iteration 670: Loss 2.7704222202301025\n",
      "Iteration 680: Loss 3.0508522987365723\n",
      "Iteration 690: Loss 2.8756282329559326\n",
      "Iteration 700: Loss 2.886596441268921\n",
      "Iteration 710: Loss 2.889294147491455\n",
      "Iteration 720: Loss 2.829678535461426\n",
      "Iteration 730: Loss 2.9435884952545166\n",
      "Iteration 740: Loss 2.9930357933044434\n",
      "Iteration 750: Loss 2.887395143508911\n",
      "Iteration 760: Loss 2.73061203956604\n",
      "Iteration 770: Loss 2.7736446857452393\n",
      "Iteration 780: Loss 3.0063843727111816\n",
      "Iteration 790: Loss 2.7488832473754883\n",
      "Iteration 800: Loss 2.9015607833862305\n",
      "Iteration 810: Loss 2.8060197830200195\n",
      "Iteration 820: Loss 2.9002127647399902\n",
      "Iteration 830: Loss 2.9055209159851074\n",
      "Iteration 840: Loss 2.833871364593506\n",
      "Iteration 850: Loss 2.8337156772613525\n",
      "Iteration 860: Loss 2.9555747509002686\n",
      "Iteration 870: Loss 2.8796865940093994\n",
      "Iteration 880: Loss 2.7351484298706055\n",
      "Iteration 890: Loss 2.872727394104004\n",
      "Iteration 900: Loss 2.8983731269836426\n",
      "Iteration 910: Loss 2.7683873176574707\n",
      "Iteration 920: Loss 2.82503080368042\n",
      "Iteration 930: Loss 2.8541476726531982\n",
      "Iteration 940: Loss 2.899854898452759\n",
      "Iteration 950: Loss 2.7092339992523193\n",
      "Iteration 960: Loss 2.7401375770568848\n",
      "Iteration 970: Loss 3.026057481765747\n",
      "Iteration 980: Loss 2.7934463024139404\n",
      "Iteration 990: Loss 2.877816677093506\n",
      "Iteration 0: Loss 4.280882358551025\n",
      "Iteration 10: Loss 3.835113525390625\n",
      "Iteration 20: Loss 3.727031946182251\n",
      "Iteration 30: Loss 3.7337100505828857\n",
      "Iteration 40: Loss 3.847985029220581\n",
      "Iteration 50: Loss 3.6226370334625244\n",
      "Iteration 60: Loss 3.439624071121216\n",
      "Iteration 70: Loss 3.484372138977051\n",
      "Iteration 80: Loss 3.433238983154297\n",
      "Iteration 90: Loss 3.4274091720581055\n",
      "Iteration 100: Loss 3.283190965652466\n",
      "Iteration 110: Loss 3.2676656246185303\n",
      "Iteration 120: Loss 3.193556070327759\n",
      "Iteration 130: Loss 3.097393274307251\n",
      "Iteration 140: Loss 3.1912779808044434\n",
      "Iteration 150: Loss 3.234886646270752\n",
      "Iteration 160: Loss 3.010617971420288\n",
      "Iteration 170: Loss 3.0427329540252686\n",
      "Iteration 180: Loss 3.1350760459899902\n",
      "Iteration 190: Loss 3.074787139892578\n",
      "Iteration 200: Loss 3.1112115383148193\n",
      "Iteration 210: Loss 2.9436445236206055\n",
      "Iteration 220: Loss 3.019657850265503\n",
      "Iteration 230: Loss 2.926571846008301\n",
      "Iteration 240: Loss 2.8713631629943848\n",
      "Iteration 250: Loss 3.010462522506714\n",
      "Iteration 260: Loss 3.1683850288391113\n",
      "Iteration 270: Loss 3.0161190032958984\n",
      "Iteration 280: Loss 3.0629019737243652\n",
      "Iteration 290: Loss 3.018014430999756\n",
      "Iteration 300: Loss 2.8989901542663574\n",
      "Iteration 310: Loss 3.062946319580078\n",
      "Iteration 320: Loss 2.8619625568389893\n",
      "Iteration 330: Loss 3.244755268096924\n",
      "Iteration 340: Loss 3.107069969177246\n",
      "Iteration 350: Loss 2.9388928413391113\n",
      "Iteration 360: Loss 3.0577611923217773\n",
      "Iteration 370: Loss 3.0323541164398193\n",
      "Iteration 380: Loss 2.9485549926757812\n",
      "Iteration 390: Loss 3.1634886264801025\n",
      "Iteration 400: Loss 2.9490301609039307\n",
      "Iteration 410: Loss 2.981337308883667\n",
      "Iteration 420: Loss 3.021214485168457\n",
      "Iteration 430: Loss 3.0721957683563232\n",
      "Iteration 440: Loss 2.9701340198516846\n",
      "Iteration 450: Loss 3.1133716106414795\n",
      "Iteration 460: Loss 3.1045291423797607\n",
      "Iteration 470: Loss 2.8344991207122803\n",
      "Iteration 480: Loss 3.0276193618774414\n",
      "Iteration 490: Loss 3.13112211227417\n",
      "Iteration 500: Loss 2.979774236679077\n",
      "Iteration 510: Loss 3.1775078773498535\n",
      "Iteration 520: Loss 3.0193135738372803\n",
      "Iteration 530: Loss 3.036519765853882\n",
      "Iteration 540: Loss 3.2077887058258057\n",
      "Iteration 550: Loss 2.9715287685394287\n",
      "Iteration 560: Loss 3.0026726722717285\n",
      "Iteration 570: Loss 3.0874295234680176\n",
      "Iteration 580: Loss 3.000262975692749\n",
      "Iteration 590: Loss 3.062605381011963\n",
      "Iteration 600: Loss 3.050442695617676\n",
      "Iteration 610: Loss 3.146329879760742\n",
      "Iteration 620: Loss 2.9771225452423096\n",
      "Iteration 630: Loss 3.212219715118408\n",
      "Iteration 640: Loss 3.188387870788574\n",
      "Iteration 650: Loss 3.0835933685302734\n",
      "Iteration 660: Loss 3.071451187133789\n",
      "Iteration 670: Loss 3.1155898571014404\n",
      "Iteration 680: Loss 2.968271493911743\n",
      "Iteration 690: Loss 2.9980578422546387\n",
      "Iteration 700: Loss 3.0174593925476074\n",
      "Iteration 710: Loss 3.1081154346466064\n",
      "Iteration 720: Loss 3.141099452972412\n",
      "Iteration 730: Loss 3.129864454269409\n",
      "Iteration 740: Loss 3.1101136207580566\n",
      "Iteration 750: Loss 3.018505811691284\n",
      "Iteration 760: Loss 2.941920280456543\n",
      "Iteration 770: Loss 3.0227596759796143\n",
      "Iteration 780: Loss 3.04093337059021\n",
      "Iteration 790: Loss 2.940721035003662\n",
      "Iteration 800: Loss 3.2431869506835938\n",
      "Iteration 810: Loss 2.9571022987365723\n",
      "Iteration 820: Loss 3.0742945671081543\n",
      "Iteration 830: Loss 2.812624454498291\n",
      "Iteration 840: Loss 2.9642930030822754\n",
      "Iteration 850: Loss 3.042858123779297\n",
      "Iteration 860: Loss 2.8843791484832764\n",
      "Iteration 870: Loss 3.2020132541656494\n",
      "Iteration 880: Loss 3.0356922149658203\n",
      "Iteration 890: Loss 3.110149621963501\n",
      "Iteration 900: Loss 2.9614789485931396\n",
      "Iteration 910: Loss 2.992539405822754\n",
      "Iteration 920: Loss 3.082996368408203\n",
      "Iteration 930: Loss 2.954942226409912\n",
      "Iteration 940: Loss 3.1149439811706543\n",
      "Iteration 950: Loss 3.1438326835632324\n",
      "Iteration 960: Loss 2.9495439529418945\n",
      "Iteration 970: Loss 3.051043748855591\n",
      "Iteration 980: Loss 2.901892900466919\n",
      "Iteration 990: Loss 3.0500707626342773\n",
      "Iteration 0: Loss 4.492621898651123\n",
      "Iteration 10: Loss 3.9746615886688232\n",
      "Iteration 20: Loss 3.9313857555389404\n",
      "Iteration 30: Loss 4.019829750061035\n",
      "Iteration 40: Loss 3.948996067047119\n",
      "Iteration 50: Loss 3.7065677642822266\n",
      "Iteration 60: Loss 3.444918155670166\n",
      "Iteration 70: Loss 3.5234076976776123\n",
      "Iteration 80: Loss 3.5356814861297607\n",
      "Iteration 90: Loss 3.408719539642334\n",
      "Iteration 100: Loss 3.4124069213867188\n",
      "Iteration 110: Loss 3.1869804859161377\n",
      "Iteration 120: Loss 3.369107246398926\n",
      "Iteration 130: Loss 3.261422872543335\n",
      "Iteration 140: Loss 3.397202968597412\n",
      "Iteration 150: Loss 3.1097328662872314\n",
      "Iteration 160: Loss 3.2018914222717285\n",
      "Iteration 170: Loss 3.0604326725006104\n",
      "Iteration 180: Loss 3.234039306640625\n",
      "Iteration 190: Loss 3.0794432163238525\n",
      "Iteration 200: Loss 3.2581570148468018\n",
      "Iteration 210: Loss 3.228163719177246\n",
      "Iteration 220: Loss 3.2727243900299072\n",
      "Iteration 230: Loss 3.2626943588256836\n",
      "Iteration 240: Loss 3.1151556968688965\n",
      "Iteration 250: Loss 3.209693193435669\n",
      "Iteration 260: Loss 3.2507219314575195\n",
      "Iteration 270: Loss 3.158719539642334\n",
      "Iteration 280: Loss 3.1254003047943115\n",
      "Iteration 290: Loss 3.377103805541992\n",
      "Iteration 300: Loss 3.107728958129883\n",
      "Iteration 310: Loss 3.2592663764953613\n",
      "Iteration 320: Loss 3.249884605407715\n",
      "Iteration 330: Loss 3.2221546173095703\n",
      "Iteration 340: Loss 3.350806474685669\n",
      "Iteration 350: Loss 3.1158316135406494\n",
      "Iteration 360: Loss 3.228457450866699\n",
      "Iteration 370: Loss 3.1101107597351074\n",
      "Iteration 380: Loss 2.9878990650177\n",
      "Iteration 390: Loss 3.422767162322998\n",
      "Iteration 400: Loss 3.3013432025909424\n",
      "Iteration 410: Loss 3.1809213161468506\n",
      "Iteration 420: Loss 3.2797632217407227\n",
      "Iteration 430: Loss 3.2652807235717773\n",
      "Iteration 440: Loss 3.3941092491149902\n",
      "Iteration 450: Loss 3.355583906173706\n",
      "Iteration 460: Loss 3.30060076713562\n",
      "Iteration 470: Loss 3.081557512283325\n",
      "Iteration 480: Loss 3.199650287628174\n",
      "Iteration 490: Loss 3.387977361679077\n",
      "Iteration 500: Loss 3.2005980014801025\n",
      "Iteration 510: Loss 3.129962921142578\n",
      "Iteration 520: Loss 3.267512321472168\n",
      "Iteration 530: Loss 3.1909780502319336\n",
      "Iteration 540: Loss 3.20969557762146\n",
      "Iteration 550: Loss 3.278120994567871\n",
      "Iteration 560: Loss 3.2369213104248047\n",
      "Iteration 570: Loss 3.3199663162231445\n",
      "Iteration 580: Loss 3.038586139678955\n",
      "Iteration 590: Loss 3.222362756729126\n",
      "Iteration 600: Loss 3.1446497440338135\n",
      "Iteration 610: Loss 3.2421038150787354\n",
      "Iteration 620: Loss 3.0630409717559814\n",
      "Iteration 630: Loss 3.321458578109741\n",
      "Iteration 640: Loss 3.0654773712158203\n",
      "Iteration 650: Loss 3.2457098960876465\n",
      "Iteration 660: Loss 3.1377196311950684\n",
      "Iteration 670: Loss 3.201719284057617\n",
      "Iteration 680: Loss 3.3162550926208496\n",
      "Iteration 690: Loss 3.2265589237213135\n",
      "Iteration 700: Loss 3.185800790786743\n",
      "Iteration 710: Loss 3.204036235809326\n",
      "Iteration 720: Loss 3.2174203395843506\n",
      "Iteration 730: Loss 3.3146281242370605\n",
      "Iteration 740: Loss 3.17507266998291\n",
      "Iteration 750: Loss 3.27724552154541\n",
      "Iteration 760: Loss 3.24836802482605\n",
      "Iteration 770: Loss 3.2432634830474854\n",
      "Iteration 780: Loss 3.3377513885498047\n",
      "Iteration 790: Loss 3.3188886642456055\n",
      "Iteration 800: Loss 3.2970402240753174\n",
      "Iteration 810: Loss 3.1274147033691406\n",
      "Iteration 820: Loss 3.401555299758911\n",
      "Iteration 830: Loss 3.221635580062866\n",
      "Iteration 840: Loss 3.1758244037628174\n",
      "Iteration 850: Loss 3.3012940883636475\n",
      "Iteration 860: Loss 3.2662289142608643\n",
      "Iteration 870: Loss 3.286051034927368\n",
      "Iteration 880: Loss 3.097726583480835\n",
      "Iteration 890: Loss 3.4487006664276123\n",
      "Iteration 900: Loss 3.219416856765747\n",
      "Iteration 910: Loss 3.1603143215179443\n",
      "Iteration 920: Loss 3.1553027629852295\n",
      "Iteration 930: Loss 3.273904323577881\n",
      "Iteration 940: Loss 3.204401969909668\n",
      "Iteration 950: Loss 3.173760414123535\n",
      "Iteration 960: Loss 3.1984171867370605\n",
      "Iteration 970: Loss 3.3726460933685303\n",
      "Iteration 980: Loss 3.1264967918395996\n",
      "Iteration 990: Loss 3.1238183975219727\n",
      "Iteration 0: Loss 4.691826820373535\n",
      "Iteration 10: Loss 4.248108386993408\n",
      "Iteration 20: Loss 4.26756477355957\n",
      "Iteration 30: Loss 4.003489971160889\n",
      "Iteration 40: Loss 4.045285701751709\n",
      "Iteration 50: Loss 4.02733039855957\n",
      "Iteration 60: Loss 4.0054755210876465\n",
      "Iteration 70: Loss 3.9445207118988037\n",
      "Iteration 80: Loss 3.919248342514038\n",
      "Iteration 90: Loss 3.8733952045440674\n",
      "Iteration 100: Loss 3.7752771377563477\n",
      "Iteration 110: Loss 3.798015832901001\n",
      "Iteration 120: Loss 3.6570889949798584\n",
      "Iteration 130: Loss 3.5926530361175537\n",
      "Iteration 140: Loss 3.5240273475646973\n",
      "Iteration 150: Loss 3.575756549835205\n",
      "Iteration 160: Loss 3.5456912517547607\n",
      "Iteration 170: Loss 3.477628707885742\n",
      "Iteration 180: Loss 3.4155397415161133\n",
      "Iteration 190: Loss 3.408465623855591\n",
      "Iteration 200: Loss 3.6747632026672363\n",
      "Iteration 210: Loss 3.2675743103027344\n",
      "Iteration 220: Loss 3.3551506996154785\n",
      "Iteration 230: Loss 3.362006664276123\n",
      "Iteration 240: Loss 3.4607784748077393\n",
      "Iteration 250: Loss 3.465510129928589\n",
      "Iteration 260: Loss 3.2928545475006104\n",
      "Iteration 270: Loss 3.3968050479888916\n",
      "Iteration 280: Loss 3.2769203186035156\n",
      "Iteration 290: Loss 3.5344746112823486\n",
      "Iteration 300: Loss 3.457690477371216\n",
      "Iteration 310: Loss 3.4451100826263428\n",
      "Iteration 320: Loss 3.132030487060547\n",
      "Iteration 330: Loss 3.3802690505981445\n",
      "Iteration 340: Loss 3.367201805114746\n",
      "Iteration 350: Loss 3.2565314769744873\n",
      "Iteration 360: Loss 3.3605589866638184\n",
      "Iteration 370: Loss 3.209735155105591\n",
      "Iteration 380: Loss 3.421592950820923\n",
      "Iteration 390: Loss 3.404531240463257\n",
      "Iteration 400: Loss 3.3023476600646973\n",
      "Iteration 410: Loss 3.5380468368530273\n",
      "Iteration 420: Loss 3.3728535175323486\n",
      "Iteration 430: Loss 3.4672627449035645\n",
      "Iteration 440: Loss 3.3840579986572266\n",
      "Iteration 450: Loss 3.351181983947754\n",
      "Iteration 460: Loss 3.478073835372925\n",
      "Iteration 470: Loss 3.304196834564209\n",
      "Iteration 480: Loss 3.2459888458251953\n",
      "Iteration 490: Loss 3.3359429836273193\n",
      "Iteration 500: Loss 3.222637891769409\n",
      "Iteration 510: Loss 3.2410197257995605\n",
      "Iteration 520: Loss 3.3280959129333496\n",
      "Iteration 530: Loss 3.2457363605499268\n",
      "Iteration 540: Loss 3.4328174591064453\n",
      "Iteration 550: Loss 3.3138389587402344\n",
      "Iteration 560: Loss 3.5688953399658203\n",
      "Iteration 570: Loss 3.4555163383483887\n",
      "Iteration 580: Loss 3.4540843963623047\n",
      "Iteration 590: Loss 3.396611213684082\n",
      "Iteration 600: Loss 3.2330873012542725\n",
      "Iteration 610: Loss 3.3345320224761963\n",
      "Iteration 620: Loss 3.243947982788086\n",
      "Iteration 630: Loss 3.5454671382904053\n",
      "Iteration 640: Loss 3.399549961090088\n",
      "Iteration 650: Loss 3.462934970855713\n",
      "Iteration 660: Loss 3.4686331748962402\n",
      "Iteration 670: Loss 3.3749568462371826\n",
      "Iteration 680: Loss 3.357555389404297\n",
      "Iteration 690: Loss 3.534961700439453\n",
      "Iteration 700: Loss 3.6194233894348145\n",
      "Iteration 710: Loss 3.412137746810913\n",
      "Iteration 720: Loss 3.3414924144744873\n",
      "Iteration 730: Loss 3.3255507946014404\n",
      "Iteration 740: Loss 3.5105764865875244\n",
      "Iteration 750: Loss 3.4931983947753906\n",
      "Iteration 760: Loss 3.4365596771240234\n",
      "Iteration 770: Loss 3.4170310497283936\n",
      "Iteration 780: Loss 3.3377654552459717\n",
      "Iteration 790: Loss 3.6287546157836914\n",
      "Iteration 800: Loss 3.4165611267089844\n",
      "Iteration 810: Loss 3.3967092037200928\n",
      "Iteration 820: Loss 3.462231159210205\n",
      "Iteration 830: Loss 3.352513313293457\n",
      "Iteration 840: Loss 3.3247265815734863\n",
      "Iteration 850: Loss 3.3904154300689697\n",
      "Iteration 860: Loss 3.4846909046173096\n",
      "Iteration 870: Loss 3.154167413711548\n",
      "Iteration 880: Loss 3.2197365760803223\n",
      "Iteration 890: Loss 3.4532902240753174\n",
      "Iteration 900: Loss 3.36932110786438\n",
      "Iteration 910: Loss 3.2218408584594727\n",
      "Iteration 920: Loss 3.2628817558288574\n",
      "Iteration 930: Loss 3.4385874271392822\n",
      "Iteration 940: Loss 3.555392026901245\n",
      "Iteration 950: Loss 3.48046875\n",
      "Iteration 960: Loss 3.314938545227051\n",
      "Iteration 970: Loss 3.410806894302368\n",
      "Iteration 980: Loss 3.417926549911499\n",
      "Iteration 990: Loss 3.463191270828247\n",
      "Iteration 0: Loss 4.885461330413818\n",
      "Iteration 10: Loss 4.501086235046387\n",
      "Iteration 20: Loss 4.584790229797363\n",
      "Iteration 30: Loss 4.517124176025391\n",
      "Iteration 40: Loss 4.359648704528809\n",
      "Iteration 50: Loss 4.052056312561035\n",
      "Iteration 60: Loss 3.858806848526001\n",
      "Iteration 70: Loss 3.9821112155914307\n",
      "Iteration 80: Loss 3.8497776985168457\n",
      "Iteration 90: Loss 3.6147241592407227\n",
      "Iteration 100: Loss 3.8119513988494873\n",
      "Iteration 110: Loss 3.6222918033599854\n",
      "Iteration 120: Loss 3.610261917114258\n",
      "Iteration 130: Loss 3.7124288082122803\n",
      "Iteration 140: Loss 3.5782229900360107\n",
      "Iteration 150: Loss 3.5083138942718506\n",
      "Iteration 160: Loss 3.5591139793395996\n",
      "Iteration 170: Loss 3.6836273670196533\n",
      "Iteration 180: Loss 3.5734710693359375\n",
      "Iteration 190: Loss 3.6019084453582764\n",
      "Iteration 200: Loss 3.5837607383728027\n",
      "Iteration 210: Loss 3.5642006397247314\n",
      "Iteration 220: Loss 3.4284608364105225\n",
      "Iteration 230: Loss 3.3152921199798584\n",
      "Iteration 240: Loss 3.611510753631592\n",
      "Iteration 250: Loss 3.4840941429138184\n",
      "Iteration 260: Loss 3.6380019187927246\n",
      "Iteration 270: Loss 3.501450777053833\n",
      "Iteration 280: Loss 3.668856143951416\n",
      "Iteration 290: Loss 3.5926880836486816\n",
      "Iteration 300: Loss 3.6819350719451904\n",
      "Iteration 310: Loss 3.49558424949646\n",
      "Iteration 320: Loss 3.5256874561309814\n",
      "Iteration 330: Loss 3.418802261352539\n",
      "Iteration 340: Loss 3.4081456661224365\n",
      "Iteration 350: Loss 3.5752804279327393\n",
      "Iteration 360: Loss 3.4027600288391113\n",
      "Iteration 370: Loss 3.7725131511688232\n",
      "Iteration 380: Loss 3.5908334255218506\n",
      "Iteration 390: Loss 3.5228271484375\n",
      "Iteration 400: Loss 3.5099008083343506\n",
      "Iteration 410: Loss 3.536860942840576\n",
      "Iteration 420: Loss 3.6780803203582764\n",
      "Iteration 430: Loss 3.5863659381866455\n",
      "Iteration 440: Loss 3.6807522773742676\n",
      "Iteration 450: Loss 3.483593225479126\n",
      "Iteration 460: Loss 3.743051767349243\n",
      "Iteration 470: Loss 3.6698343753814697\n",
      "Iteration 480: Loss 3.4261467456817627\n",
      "Iteration 490: Loss 3.2325968742370605\n",
      "Iteration 500: Loss 3.616497278213501\n",
      "Iteration 510: Loss 3.4143857955932617\n",
      "Iteration 520: Loss 3.7796218395233154\n",
      "Iteration 530: Loss 3.3960304260253906\n",
      "Iteration 540: Loss 3.471529483795166\n",
      "Iteration 550: Loss 3.5142624378204346\n",
      "Iteration 560: Loss 3.4870967864990234\n",
      "Iteration 570: Loss 3.535768985748291\n",
      "Iteration 580: Loss 3.671800374984741\n",
      "Iteration 590: Loss 3.6130616664886475\n",
      "Iteration 600: Loss 3.677865505218506\n",
      "Iteration 610: Loss 3.4291062355041504\n",
      "Iteration 620: Loss 3.560713291168213\n",
      "Iteration 630: Loss 3.5817296504974365\n",
      "Iteration 640: Loss 3.5134100914001465\n",
      "Iteration 650: Loss 3.464085817337036\n",
      "Iteration 660: Loss 3.518542766571045\n",
      "Iteration 670: Loss 3.5388638973236084\n",
      "Iteration 680: Loss 3.5973994731903076\n",
      "Iteration 690: Loss 3.633117914199829\n",
      "Iteration 700: Loss 3.501802682876587\n",
      "Iteration 710: Loss 3.465219020843506\n",
      "Iteration 720: Loss 3.5360002517700195\n",
      "Iteration 730: Loss 3.353071689605713\n",
      "Iteration 740: Loss 3.7468013763427734\n",
      "Iteration 750: Loss 3.5741841793060303\n",
      "Iteration 760: Loss 3.5149617195129395\n",
      "Iteration 770: Loss 3.6020944118499756\n",
      "Iteration 780: Loss 3.5647881031036377\n",
      "Iteration 790: Loss 3.6757044792175293\n",
      "Iteration 800: Loss 3.467397451400757\n",
      "Iteration 810: Loss 3.385544538497925\n",
      "Iteration 820: Loss 3.7463889122009277\n",
      "Iteration 830: Loss 3.678091287612915\n",
      "Iteration 840: Loss 3.677031993865967\n",
      "Iteration 850: Loss 3.5021421909332275\n",
      "Iteration 860: Loss 3.491826057434082\n",
      "Iteration 870: Loss 3.3002893924713135\n",
      "Iteration 880: Loss 3.529237747192383\n",
      "Iteration 890: Loss 3.6598289012908936\n",
      "Iteration 900: Loss 3.599233388900757\n",
      "Iteration 910: Loss 3.454582691192627\n",
      "Iteration 920: Loss 3.6963014602661133\n",
      "Iteration 930: Loss 3.6786394119262695\n",
      "Iteration 940: Loss 3.644782781600952\n",
      "Iteration 950: Loss 3.5913097858428955\n",
      "Iteration 960: Loss 3.492713689804077\n",
      "Iteration 970: Loss 3.630969524383545\n",
      "Iteration 980: Loss 3.4339849948883057\n",
      "Iteration 990: Loss 3.439314842224121\n",
      "Iteration 0: Loss 5.097804546356201\n",
      "Iteration 10: Loss 4.886498928070068\n",
      "Iteration 20: Loss 4.622579097747803\n",
      "Iteration 30: Loss 4.531295299530029\n",
      "Iteration 40: Loss 4.357928276062012\n",
      "Iteration 50: Loss 4.194681167602539\n",
      "Iteration 60: Loss 4.12859582901001\n",
      "Iteration 70: Loss 4.378367900848389\n",
      "Iteration 80: Loss 4.204296588897705\n",
      "Iteration 90: Loss 4.010514736175537\n",
      "Iteration 100: Loss 4.078832149505615\n",
      "Iteration 110: Loss 4.138437271118164\n",
      "Iteration 120: Loss 4.005119323730469\n",
      "Iteration 130: Loss 3.803499460220337\n",
      "Iteration 140: Loss 3.8538882732391357\n",
      "Iteration 150: Loss 3.7638444900512695\n",
      "Iteration 160: Loss 3.660707712173462\n",
      "Iteration 170: Loss 3.7097816467285156\n",
      "Iteration 180: Loss 3.6999683380126953\n",
      "Iteration 190: Loss 3.8521835803985596\n",
      "Iteration 200: Loss 3.6160006523132324\n",
      "Iteration 210: Loss 3.680521011352539\n",
      "Iteration 220: Loss 3.6929514408111572\n",
      "Iteration 230: Loss 3.753321409225464\n",
      "Iteration 240: Loss 3.6600794792175293\n",
      "Iteration 250: Loss 3.5799100399017334\n",
      "Iteration 260: Loss 3.4470877647399902\n",
      "Iteration 270: Loss 3.7847414016723633\n",
      "Iteration 280: Loss 3.703174591064453\n",
      "Iteration 290: Loss 3.5371179580688477\n",
      "Iteration 300: Loss 3.6809751987457275\n",
      "Iteration 310: Loss 3.7120347023010254\n",
      "Iteration 320: Loss 3.6228039264678955\n",
      "Iteration 330: Loss 3.6499154567718506\n",
      "Iteration 340: Loss 3.5168776512145996\n",
      "Iteration 350: Loss 3.752713918685913\n",
      "Iteration 360: Loss 3.687238931655884\n",
      "Iteration 370: Loss 3.8656437397003174\n",
      "Iteration 380: Loss 3.6842527389526367\n",
      "Iteration 390: Loss 3.6966748237609863\n",
      "Iteration 400: Loss 3.6741976737976074\n",
      "Iteration 410: Loss 3.759675979614258\n",
      "Iteration 420: Loss 3.8619420528411865\n",
      "Iteration 430: Loss 3.7899515628814697\n",
      "Iteration 440: Loss 3.8307366371154785\n",
      "Iteration 450: Loss 3.66104793548584\n",
      "Iteration 460: Loss 3.686051368713379\n",
      "Iteration 470: Loss 3.814107656478882\n",
      "Iteration 480: Loss 3.719984292984009\n",
      "Iteration 490: Loss 3.916452169418335\n",
      "Iteration 500: Loss 3.5999484062194824\n",
      "Iteration 510: Loss 3.6154425144195557\n",
      "Iteration 520: Loss 3.6599361896514893\n",
      "Iteration 530: Loss 3.7020936012268066\n",
      "Iteration 540: Loss 3.788210391998291\n",
      "Iteration 550: Loss 3.6374123096466064\n",
      "Iteration 560: Loss 3.873060464859009\n",
      "Iteration 570: Loss 3.6232733726501465\n",
      "Iteration 580: Loss 3.7881624698638916\n",
      "Iteration 590: Loss 3.5761072635650635\n",
      "Iteration 600: Loss 3.738816261291504\n",
      "Iteration 610: Loss 3.7381203174591064\n",
      "Iteration 620: Loss 3.6742022037506104\n",
      "Iteration 630: Loss 3.628267526626587\n",
      "Iteration 640: Loss 3.694976806640625\n",
      "Iteration 650: Loss 3.7490921020507812\n",
      "Iteration 660: Loss 3.721820116043091\n",
      "Iteration 670: Loss 3.5848569869995117\n",
      "Iteration 680: Loss 3.7116634845733643\n",
      "Iteration 690: Loss 3.571916341781616\n",
      "Iteration 700: Loss 3.667032241821289\n",
      "Iteration 710: Loss 3.6201670169830322\n",
      "Iteration 720: Loss 3.704770565032959\n",
      "Iteration 730: Loss 3.662961721420288\n",
      "Iteration 740: Loss 3.882049798965454\n",
      "Iteration 750: Loss 3.709138870239258\n",
      "Iteration 760: Loss 3.528163194656372\n",
      "Iteration 770: Loss 3.715208053588867\n",
      "Iteration 780: Loss 3.5779943466186523\n",
      "Iteration 790: Loss 3.7148678302764893\n",
      "Iteration 800: Loss 3.693826913833618\n",
      "Iteration 810: Loss 3.5788466930389404\n",
      "Iteration 820: Loss 3.808026075363159\n",
      "Iteration 830: Loss 3.5058302879333496\n",
      "Iteration 840: Loss 3.7194976806640625\n",
      "Iteration 850: Loss 3.8059372901916504\n",
      "Iteration 860: Loss 3.6895201206207275\n",
      "Iteration 870: Loss 3.7027995586395264\n",
      "Iteration 880: Loss 3.7351982593536377\n",
      "Iteration 890: Loss 3.692950963973999\n",
      "Iteration 900: Loss 3.5803799629211426\n",
      "Iteration 910: Loss 3.6003990173339844\n",
      "Iteration 920: Loss 3.712310552597046\n",
      "Iteration 930: Loss 3.6172187328338623\n",
      "Iteration 940: Loss 3.7173802852630615\n",
      "Iteration 950: Loss 3.758894920349121\n",
      "Iteration 960: Loss 3.6475582122802734\n",
      "Iteration 970: Loss 3.793450355529785\n",
      "Iteration 980: Loss 3.8568084239959717\n",
      "Iteration 990: Loss 3.729156255722046\n",
      "Iteration 0: Loss 5.2531843185424805\n",
      "Iteration 10: Loss 5.014229774475098\n",
      "Iteration 20: Loss 4.946583271026611\n",
      "Iteration 30: Loss 4.797061920166016\n",
      "Iteration 40: Loss 4.765112400054932\n",
      "Iteration 50: Loss 4.410673141479492\n",
      "Iteration 60: Loss 4.511904239654541\n",
      "Iteration 70: Loss 4.301840305328369\n",
      "Iteration 80: Loss 4.138341426849365\n",
      "Iteration 90: Loss 3.9319756031036377\n",
      "Iteration 100: Loss 4.177276611328125\n",
      "Iteration 110: Loss 3.962038993835449\n",
      "Iteration 120: Loss 4.00337028503418\n",
      "Iteration 130: Loss 4.010707855224609\n",
      "Iteration 140: Loss 3.957158088684082\n",
      "Iteration 150: Loss 3.7800862789154053\n",
      "Iteration 160: Loss 3.848090410232544\n",
      "Iteration 170: Loss 3.895782709121704\n",
      "Iteration 180: Loss 4.010066986083984\n",
      "Iteration 190: Loss 3.726850748062134\n",
      "Iteration 200: Loss 3.8344366550445557\n",
      "Iteration 210: Loss 3.821176767349243\n",
      "Iteration 220: Loss 3.7942874431610107\n",
      "Iteration 230: Loss 3.6671226024627686\n",
      "Iteration 240: Loss 3.766925573348999\n",
      "Iteration 250: Loss 3.977530002593994\n",
      "Iteration 260: Loss 3.812192916870117\n",
      "Iteration 270: Loss 3.78509783744812\n",
      "Iteration 280: Loss 3.7818350791931152\n",
      "Iteration 290: Loss 3.809910535812378\n",
      "Iteration 300: Loss 3.607741355895996\n",
      "Iteration 310: Loss 3.725142002105713\n",
      "Iteration 320: Loss 3.875192880630493\n",
      "Iteration 330: Loss 3.922239065170288\n",
      "Iteration 340: Loss 3.8675968647003174\n",
      "Iteration 350: Loss 3.901435136795044\n",
      "Iteration 360: Loss 3.841282606124878\n",
      "Iteration 370: Loss 3.9689950942993164\n",
      "Iteration 380: Loss 3.880242109298706\n",
      "Iteration 390: Loss 3.9784350395202637\n",
      "Iteration 400: Loss 3.787217855453491\n",
      "Iteration 410: Loss 3.77875018119812\n",
      "Iteration 420: Loss 3.872255325317383\n",
      "Iteration 430: Loss 4.071143627166748\n",
      "Iteration 440: Loss 3.9593608379364014\n",
      "Iteration 450: Loss 3.8952443599700928\n",
      "Iteration 460: Loss 3.7524232864379883\n",
      "Iteration 470: Loss 3.8253448009490967\n",
      "Iteration 480: Loss 3.8958287239074707\n",
      "Iteration 490: Loss 3.76580548286438\n",
      "Iteration 500: Loss 3.836392879486084\n",
      "Iteration 510: Loss 3.7978873252868652\n",
      "Iteration 520: Loss 3.682537078857422\n",
      "Iteration 530: Loss 3.8114845752716064\n",
      "Iteration 540: Loss 3.7562506198883057\n",
      "Iteration 550: Loss 4.053459167480469\n",
      "Iteration 560: Loss 3.731459379196167\n",
      "Iteration 570: Loss 3.876884698867798\n",
      "Iteration 580: Loss 3.7155537605285645\n",
      "Iteration 590: Loss 3.7734720706939697\n",
      "Iteration 600: Loss 3.8625638484954834\n",
      "Iteration 610: Loss 3.852170944213867\n",
      "Iteration 620: Loss 3.6473355293273926\n",
      "Iteration 630: Loss 3.9339003562927246\n",
      "Iteration 640: Loss 3.866813898086548\n",
      "Iteration 650: Loss 3.6695780754089355\n",
      "Iteration 660: Loss 3.973172903060913\n",
      "Iteration 670: Loss 3.9382834434509277\n",
      "Iteration 680: Loss 3.7540109157562256\n",
      "Iteration 690: Loss 3.805708408355713\n",
      "Iteration 700: Loss 3.9721920490264893\n",
      "Iteration 710: Loss 3.9848904609680176\n",
      "Iteration 720: Loss 3.6367034912109375\n",
      "Iteration 730: Loss 3.904496431350708\n",
      "Iteration 740: Loss 3.7527804374694824\n",
      "Iteration 750: Loss 3.752213954925537\n",
      "Iteration 760: Loss 3.705859661102295\n",
      "Iteration 770: Loss 3.8379037380218506\n",
      "Iteration 780: Loss 3.910996675491333\n",
      "Iteration 790: Loss 3.743229627609253\n",
      "Iteration 800: Loss 3.912329912185669\n",
      "Iteration 810: Loss 3.8639814853668213\n",
      "Iteration 820: Loss 3.7678351402282715\n",
      "Iteration 830: Loss 3.6851882934570312\n",
      "Iteration 840: Loss 3.9892327785491943\n",
      "Iteration 850: Loss 3.905578136444092\n",
      "Iteration 860: Loss 3.8591413497924805\n",
      "Iteration 870: Loss 3.8847784996032715\n",
      "Iteration 880: Loss 3.67353892326355\n",
      "Iteration 890: Loss 3.833587408065796\n",
      "Iteration 900: Loss 3.8665599822998047\n",
      "Iteration 910: Loss 3.8572490215301514\n",
      "Iteration 920: Loss 3.8145039081573486\n",
      "Iteration 930: Loss 3.771865129470825\n",
      "Iteration 940: Loss 3.7501373291015625\n",
      "Iteration 950: Loss 3.6980347633361816\n",
      "Iteration 960: Loss 3.622333526611328\n",
      "Iteration 970: Loss 3.9311976432800293\n",
      "Iteration 980: Loss 3.975041627883911\n",
      "Iteration 990: Loss 3.905036211013794\n",
      "Iteration 0: Loss 5.4259538650512695\n",
      "Iteration 10: Loss 5.187954902648926\n",
      "Iteration 20: Loss 5.021471977233887\n",
      "Iteration 30: Loss 4.903907775878906\n",
      "Iteration 40: Loss 4.8468241691589355\n",
      "Iteration 50: Loss 4.692928314208984\n",
      "Iteration 60: Loss 4.603384494781494\n",
      "Iteration 70: Loss 4.521308422088623\n",
      "Iteration 80: Loss 4.456267833709717\n",
      "Iteration 90: Loss 4.460425853729248\n",
      "Iteration 100: Loss 4.434225559234619\n",
      "Iteration 110: Loss 4.268501281738281\n",
      "Iteration 120: Loss 4.160891056060791\n",
      "Iteration 130: Loss 4.189199924468994\n",
      "Iteration 140: Loss 4.076919078826904\n",
      "Iteration 150: Loss 4.075479030609131\n",
      "Iteration 160: Loss 4.239617824554443\n",
      "Iteration 170: Loss 4.001796245574951\n",
      "Iteration 180: Loss 3.8970553874969482\n",
      "Iteration 190: Loss 3.9481828212738037\n",
      "Iteration 200: Loss 4.0535125732421875\n",
      "Iteration 210: Loss 3.845853328704834\n",
      "Iteration 220: Loss 4.160409927368164\n",
      "Iteration 230: Loss 4.199182033538818\n",
      "Iteration 240: Loss 4.074723720550537\n",
      "Iteration 250: Loss 3.9238369464874268\n",
      "Iteration 260: Loss 4.122574329376221\n",
      "Iteration 270: Loss 3.9659435749053955\n",
      "Iteration 280: Loss 3.8553202152252197\n",
      "Iteration 290: Loss 3.897144317626953\n",
      "Iteration 300: Loss 3.995408535003662\n",
      "Iteration 310: Loss 3.950673818588257\n",
      "Iteration 320: Loss 3.8529443740844727\n",
      "Iteration 330: Loss 4.102034091949463\n",
      "Iteration 340: Loss 3.9154646396636963\n",
      "Iteration 350: Loss 3.8911592960357666\n",
      "Iteration 360: Loss 3.93652606010437\n",
      "Iteration 370: Loss 4.151159763336182\n",
      "Iteration 380: Loss 3.8946640491485596\n",
      "Iteration 390: Loss 3.9550046920776367\n",
      "Iteration 400: Loss 3.8182241916656494\n",
      "Iteration 410: Loss 3.898019313812256\n",
      "Iteration 420: Loss 3.961212396621704\n",
      "Iteration 430: Loss 4.0305962562561035\n",
      "Iteration 440: Loss 3.883413076400757\n",
      "Iteration 450: Loss 3.8583104610443115\n",
      "Iteration 460: Loss 3.9854226112365723\n",
      "Iteration 470: Loss 4.132986068725586\n",
      "Iteration 480: Loss 3.836247444152832\n",
      "Iteration 490: Loss 3.967456579208374\n",
      "Iteration 500: Loss 3.8764779567718506\n",
      "Iteration 510: Loss 4.141639709472656\n",
      "Iteration 520: Loss 4.078620910644531\n",
      "Iteration 530: Loss 3.8752760887145996\n",
      "Iteration 540: Loss 4.064009189605713\n",
      "Iteration 550: Loss 3.957531213760376\n",
      "Iteration 560: Loss 4.218415260314941\n",
      "Iteration 570: Loss 4.029943466186523\n",
      "Iteration 580: Loss 4.058677673339844\n",
      "Iteration 590: Loss 4.09354305267334\n",
      "Iteration 600: Loss 4.110574722290039\n",
      "Iteration 610: Loss 3.972608804702759\n",
      "Iteration 620: Loss 3.929757595062256\n",
      "Iteration 630: Loss 3.970730781555176\n",
      "Iteration 640: Loss 4.038529396057129\n",
      "Iteration 650: Loss 3.7074058055877686\n",
      "Iteration 660: Loss 3.8913931846618652\n",
      "Iteration 670: Loss 3.9480061531066895\n",
      "Iteration 680: Loss 3.9120397567749023\n",
      "Iteration 690: Loss 3.917926549911499\n",
      "Iteration 700: Loss 3.9538533687591553\n",
      "Iteration 710: Loss 3.96209716796875\n",
      "Iteration 720: Loss 3.942991018295288\n",
      "Iteration 730: Loss 3.9337124824523926\n",
      "Iteration 740: Loss 3.984081983566284\n",
      "Iteration 750: Loss 3.96834135055542\n",
      "Iteration 760: Loss 4.058701992034912\n",
      "Iteration 770: Loss 3.9077975749969482\n",
      "Iteration 780: Loss 4.0218634605407715\n",
      "Iteration 790: Loss 4.088785648345947\n",
      "Iteration 800: Loss 4.084548473358154\n",
      "Iteration 810: Loss 3.8467588424682617\n",
      "Iteration 820: Loss 4.027830123901367\n",
      "Iteration 830: Loss 4.002474308013916\n",
      "Iteration 840: Loss 4.031489849090576\n",
      "Iteration 850: Loss 3.967327356338501\n",
      "Iteration 860: Loss 3.9200639724731445\n",
      "Iteration 870: Loss 4.070695877075195\n",
      "Iteration 880: Loss 3.766632318496704\n",
      "Iteration 890: Loss 4.141232013702393\n",
      "Iteration 900: Loss 3.947582721710205\n",
      "Iteration 910: Loss 3.930828809738159\n",
      "Iteration 920: Loss 3.857633590698242\n",
      "Iteration 930: Loss 3.8788132667541504\n",
      "Iteration 940: Loss 3.984017848968506\n",
      "Iteration 950: Loss 3.897185802459717\n",
      "Iteration 960: Loss 3.961561679840088\n",
      "Iteration 970: Loss 3.9980592727661133\n",
      "Iteration 980: Loss 4.105252265930176\n",
      "Iteration 990: Loss 3.9799365997314453\n"
     ]
    }
   ],
   "source": [
    "base = 10000\n",
    "\n",
    "Ts = [1, 2, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50] + [55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "\n",
    "times = np.zeros(len(Ts)) + np.nan\n",
    "\n",
    "for i_t, t in enumerate(Ts):\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    model = Model(N=base, T=t) # create model object\n",
    "    \n",
    "    train_policy(model) # train policy\n",
    "\n",
    "    times[i_t] = time.perf_counter() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "aa7d0d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots()\n",
    "\n",
    "ax.plot(Ts, times, label='JAX', color='blue')\n",
    "ax.plot(Ts, times_pytorch, label='PyTorch', color='red')\n",
    "ax.set_ylabel('Seconds')\n",
    "ax.set_xlabel('Periods')\n",
    "ax.vlines(60, 0, 60, color='red', linestyles='--', label ='PyTorch start printing nans')\n",
    "ax.legend()\n",
    "f.suptitle('Time for 1000 iterations with 10000 agents');\n",
    "\n",
    "f.savefig('JAX_vs_PyTorch_number_of_periods.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
