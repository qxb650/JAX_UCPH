{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8dae2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "from types import SimpleNamespace\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f12893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flax version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import flax\n",
    "\n",
    "print(\"Flax version:\", flax.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451cec89",
   "metadata": {},
   "source": [
    "## 1. Create neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5792d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nnx.Module):\n",
    "  def __init__(self, din: int, dout: int, neurons: list, rngs: nnx.Rngs):\n",
    "\n",
    "    # 1st layer\n",
    "    self.layer1 = nnx.Linear(din, neurons[0], rngs=rngs)\n",
    "    \n",
    "    # hidden layers\n",
    "    for layer in range(len(neurons)-1):\n",
    "      \n",
    "      setattr(self, f\"layer{layer+2}\", nnx.Linear(neurons[layer], neurons[layer+1], rngs=rngs))\n",
    "\n",
    "    # last layer\n",
    "    self.layer_out = nnx.Linear(neurons[-1], dout, rngs=rngs)\n",
    "\n",
    "    self.din, self.dout, self.hidden_layers = din, dout, neurons\n",
    "  \n",
    "  def __call__(self, x: jax.Array):\n",
    "\n",
    "    # 1st + hiden layers\n",
    "    for i in range(len(self.hidden_layers)):\n",
    "\n",
    "      # unpack layer\n",
    "      layer = getattr(self, f\"layer{i+1}\")\n",
    "\n",
    "      # forward x\n",
    "      x = nnx.relu(layer(x))\n",
    "\n",
    "    # last layer\n",
    "    layer = self.layer_out\n",
    "\n",
    "    y = jax.nn.sigmoid(layer(x))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f761e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_nn(model):\n",
    "\n",
    "  par = model.par\n",
    "  train = model.train\n",
    "\n",
    "  T = par.T\n",
    "  Nstates = par.Nstates\n",
    "  Nactions = par.Nactions\n",
    "\n",
    "  din = Nstates + T\n",
    "  dout = Nactions\n",
    "  neurons = train.neurons\n",
    "\n",
    "  nn = Policy(din, dout, neurons, rngs=nnx.Rngs(params=0))\n",
    "\n",
    "  return nn\n",
    "\n",
    "def eval_policy(model,nn,x,t):\n",
    "\n",
    "  par = model.par\n",
    "\n",
    "  # time dummies\n",
    "  Nx = x.shape[0]\n",
    "  T = par.T\n",
    "    \n",
    "  time_dummies = jax.nn.one_hot(t, T)           # shape (T,)\n",
    "  time_dummies = jnp.broadcast_to(time_dummies, (Nx, T))\n",
    "  # concatenate\n",
    "  \n",
    "  x = jnp.concatenate((x,time_dummies),axis=-1)\n",
    "\n",
    "  # evaluate\n",
    "  action = nn(x)\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f32a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.5598746]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# direcly evaluate Policy network\n",
    "hidden_layers = [5,5,10]\n",
    "din = 10\n",
    "dout = 1\n",
    "\n",
    "model = Policy(din, dout, hidden_layers, rngs=nnx.Rngs(params=0))\n",
    "model(x=jnp.ones((1, din)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588f904",
   "metadata": {},
   "source": [
    "## 2. Create neural network in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc76a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.par = SimpleNamespace()\n",
    "        self.train = SimpleNamespace()\n",
    "        self.sim = SimpleNamespace()\n",
    "\n",
    "        self.dtype = jnp.float32\n",
    "        self.device = jax.devices(\"cpu\")[0]\n",
    "    \n",
    "        self.setup() # Setup model parameters\n",
    "        self.allocate() # Allocate model objects\n",
    "        self.setup_train()  # Setup training parameters\n",
    "        self.allocate_train() # Allocate training objects\n",
    "    \n",
    "    # setup empty functions to be overwritten\n",
    "\n",
    "    # Setup and allocate\n",
    "    def setup(self): pass\n",
    "    def allocate(self): pass\n",
    "    def setup_train(self): pass\n",
    "    def allocate_train(self): pass\n",
    "\n",
    "    # Draw\n",
    "    def draw_initial_states(self): pass\n",
    "    def draw_shocks(self): pass\n",
    "\n",
    "    # Transition\n",
    "    def state_trans(self): pass  # Post-decision states, shocks -> next-period states\n",
    "\n",
    "    # Reward\n",
    "    def reward(self): pass  # Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295df8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(self,full=None):\n",
    "    \"\"\" choose parameters \"\"\"\n",
    "\n",
    "    par = self.par\n",
    "    sim = self.sim\n",
    "    \n",
    "    # a. model\n",
    "    par.T = 5 # number of periods\n",
    "\n",
    "    # preferences\n",
    "    par.beta = 1/1.01 # discount factor\n",
    "\n",
    "    # income\n",
    "    par.kappa_base = 1.0\n",
    "    par.rho_p= 0.95 # shock, persistenc\n",
    "    par.sigma_xi = 0.1 # shock, permanent\n",
    "    par.sigma_psi = 0.1 # shock, transitory std\n",
    "\n",
    "    # return\n",
    "    par.R = 1.01 # gross return\n",
    "\n",
    "    # b. solver settings\n",
    "    par.Nstates = 2 # number of states variables\n",
    "    par.Nactions = 1 # number of actions variables\n",
    "    par.Nshocks = 2 # number of shocks\n",
    "\n",
    "    # c. simulation \n",
    "    sim.N = 50_000 # number of agents\n",
    "\n",
    "    # initial states\n",
    "    par.mu_m0 = 1.0 # initial cash-on-hand, mean\n",
    "    par.sigma_m0 = 0.1 # initial cash-on-hand, std\n",
    "\n",
    "    # initial permanent income\n",
    "    par.mu_p0 = 1.0 # initial durable, mean\n",
    "    par.sigma_p0 = 0.1 # initial durable, std\n",
    "\n",
    "\n",
    "    sim.N = 10_000 # number of simulated agents\n",
    "\n",
    "def setup_train(model):\n",
    "    \"\"\" default parameters for training \"\"\"\n",
    "    \n",
    "    train = model.train\n",
    "\n",
    "    # a. neural network\n",
    "    train.neurons = [100,100] # number of neurons in hidden layers\n",
    "\n",
    "    train.N = 3000 # number of agents for training\n",
    "    train.seed = 0\n",
    "\n",
    "    train.learning_rate_policy = 1e-3 # learning rate for policy\n",
    "\n",
    "def allocate_train(model):\n",
    "    \"\"\" allocate memory training \"\"\"\n",
    "\n",
    "    par = model.par\n",
    "    train = model.train\n",
    "    device = model.device\n",
    "\n",
    "    # a. neural network\n",
    "    model.Policy_NN = setup_nn(model) # policy neural network\n",
    "    model.create_opt() # create optimizer for policy neural network\n",
    "\n",
    "def create_opt(model):\n",
    "\n",
    "    train = model.train\n",
    "\n",
    "    lr = train.learning_rate_policy\n",
    "\n",
    "    model.policy_opt = nnx.ModelAndOptimizer(model.Policy_NN, optax.adam(learning_rate=train.learning_rate_policy))\n",
    "\n",
    "\n",
    "Model.allocate_train = allocate_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbac8540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.9248638]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate nn in model\n",
    "Model.setup = setup\n",
    "Model.setup_train = setup_train\n",
    "Model.allocate_train = allocate_train\n",
    "Model.create_opt = create_opt\n",
    "\n",
    "model = Model()\n",
    "\n",
    "#model.nn =  setup_nn(model)\n",
    "\n",
    "x = 20*jnp.ones((1, model.par.Nstates))\n",
    "\n",
    "eval_policy(model,model.Policy_NN,x,t=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d146d1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.9248638]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_policy_jit = jax.jit(eval_policy, static_argnums=(0))\n",
    "\n",
    "eval_policy_jit(model,model.Policy_NN,x,t=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c2186",
   "metadata": {},
   "source": [
    "## 3. Allocate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7aaa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate(model):\n",
    "    \"\"\" allocate arrays  \"\"\"\n",
    "\n",
    "    # unpack\n",
    "    par = model.par\n",
    "    sim = model.sim\n",
    "\n",
    "    # b. simulation (same across models)\n",
    "    sim.states = jnp.zeros((par.T,sim.N,par.Nstates)) # State-vector\n",
    "    sim.shocks = jnp.zeros((par.T,sim.N,par.Nshocks)) # Shock-vector\n",
    "    sim.actions = jnp.zeros((par.T,sim.N,par.Nactions))  # actions array\n",
    "    sim.reward = jnp.zeros((par.T,sim.N)) # array for utility rewards\n",
    "    sim.outcomes = jnp.zeros((par.T,sim.N,1)) # array for outcomes - will just be consumption\n",
    "    sim.R = jnp.nan # initialize average discounted utility\n",
    "\n",
    "Model.allocate = allocate\n",
    "\n",
    "def draw_initial_states(model,N):\n",
    "    \"\"\" draw initial state (m,p,t) \"\"\"\n",
    "\n",
    "    par = model.par\n",
    "    train = model.train\n",
    "\n",
    "    sigma_m0 = par.sigma_m0\n",
    "    sigma_p0 = par.sigma_p0\n",
    "\n",
    "    # a. draw cash-on-hand:\t\n",
    "    m0 = par.mu_m0*np.exp(np.random.normal(-0.5*sigma_m0**2,sigma_m0,size=(N,)))\n",
    "    m0 = jnp.array(m0,dtype=model.dtype,device=model.device)\n",
    "    \n",
    "    # b. draw permanent income\n",
    "    p0 = par.mu_p0*np.exp(np.random.normal(-0.5*sigma_p0**2,sigma_p0,size=(N,)))\n",
    "    p0 = jnp.array(p0,dtype=model.dtype,device=model.device)\n",
    "\n",
    "    # c. store\n",
    "    return jnp.stack((m0,p0),axis=1)\n",
    "\n",
    "Model.draw_initial_states = draw_initial_states\n",
    "\n",
    "def draw_shocks(model,N):\n",
    "    \"\"\" draw shocks \"\"\"\n",
    "\n",
    "    par = model.par\n",
    "\n",
    "    # xi \n",
    "    xi_loc = -0.5*par.sigma_xi**2\n",
    "    xi = np.exp(np.random.normal(xi_loc,par.sigma_xi,size=(par.T,N,)))\n",
    "    xi = jnp.array(xi,dtype=model.dtype,device=model.device)\n",
    "\n",
    "    # psi\n",
    "    psi_loc = -0.5*par.sigma_psi**2\n",
    "    psi = np.exp(np.random.normal(psi_loc,par.sigma_psi,size=(par.T,N,)))\n",
    "    psi = jnp.array(psi,dtype=model.dtype,device=model.device)\n",
    "\n",
    "\n",
    "    return jnp.stack((xi,psi),axis=-1)\n",
    "\n",
    "Model.draw_shocks = draw_shocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "824ba0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.8558104 , 0.8778358 ],\n",
       "       [1.0766381 , 1.019488  ],\n",
       "       [0.9568096 , 0.93885803],\n",
       "       [1.1297044 , 0.9903299 ],\n",
       "       [1.0857869 , 1.0999461 ],\n",
       "       [1.0134012 , 0.8934043 ],\n",
       "       [0.91807693, 0.9612195 ],\n",
       "       [1.3216639 , 1.004609  ],\n",
       "       [1.0485314 , 0.81817067],\n",
       "       [0.96520007, 1.0397788 ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.draw_initial_states(model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ec3c6",
   "metadata": {},
   "source": [
    "## 3. Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e12f8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcomes(model,states,actions,t=None):\n",
    "\t\"\"\" outcomes - just consumption here \"\"\"\n",
    "\n",
    "\tm = states[...,0] # cash-on-hand\n",
    "\ta = actions[...,0] # savings rate\n",
    "\tc = m*(1-a) # consumption\n",
    "\n",
    "\treturn jnp.stack((c,),axis=-1) # (T,N,Noutcomes)\n",
    "\n",
    "Model.outcomes = outcomes\n",
    "\n",
    "def state_trans(model,states,actions,outcomes,shocks,t=None):\n",
    "\t\"\"\" transition to future state \"\"\"\n",
    "\n",
    "\t# a. unpack\n",
    "\tpar = model.par\n",
    "\txi = shocks[...,0] # permanent income shock\n",
    "\tpsi = shocks[...,1] # transitory income shock\n",
    "\tm = states[...,0]\n",
    "\tp = states[...,1]\n",
    "\tc = outcomes[...,0]\n",
    "\n",
    "\t# c. post-decision\n",
    "\tm_pd = m-c\n",
    "\t\n",
    "    # d. persistent income\n",
    "\tp_plus = p**par.rho_p * xi # permanent income\n",
    "\t\n",
    "    # d. income\n",
    "\tincome = par.kappa_base * p_plus * psi # income\n",
    "\t\n",
    "    # e. future cash-on-hand\n",
    "\tm_plus = par.R * m_pd + income # future cash-on-hand\n",
    "\n",
    "\t# d. finalize\n",
    "\tstates_pd = jnp.stack((m_plus,p_plus),axis=-1)\n",
    "\treturn states_pd\n",
    "\n",
    "Model.state_trans = state_trans\n",
    "\n",
    "def utility(par,c):\n",
    "\t\"\"\" utility \"\"\"\n",
    "\n",
    "\treturn jnp.log(c)\n",
    "\n",
    "def reward(model,states,actions,outcomes,t0=0,t=None):\n",
    "\t\"\"\" reward \"\"\"\n",
    "\n",
    "\t# a. unpack\n",
    "\tpar = model.par\n",
    "\n",
    "\t# b. consumption\n",
    "\tc = outcomes[...,0]\n",
    "\n",
    "\t# c. utility\n",
    "\tu = utility(par,c)\n",
    "\n",
    "\treturn u \n",
    "\n",
    "Model.reward = reward\n",
    "\n",
    "def simulate(model):\n",
    "    \"\"\" Simulate to get loss in DeepSimulate \"\"\"\n",
    "\n",
    "    # a. unpack\n",
    "    par = model.par\n",
    "    sim = model.sim\n",
    "    dtype = model.dtype\n",
    "    device = model.device\n",
    "\n",
    "    states = sim.states\n",
    "    actions = sim.actions\n",
    "    rewards = sim.reward\n",
    "    outcomes = sim.outcomes\n",
    "\n",
    "    # b. draw initial states\n",
    "    states = states.at[0].set(model.draw_initial_states(N=sim.N))\n",
    "    shocks = model.draw_shocks(N=sim.N)\n",
    "\n",
    "    # c. simulate\n",
    "    for t in range(par.T):\n",
    "\n",
    "        # i. compute actions\n",
    "        actions = actions.at[t].set(eval_policy(model, model.Policy_NN, states[t], t=t))\n",
    "\n",
    "        # ii. compute outcomes\n",
    "        outcomes = outcomes.at[t].set(model.outcomes(states[t], actions[t], t=t))\n",
    "\n",
    "        # iii. compute rewards\n",
    "        rewards = rewards.at[t].set(model.reward(states[t], actions[t], outcomes[t], t=t))\n",
    "\n",
    "        # iv. transition\n",
    "        if t < par.T - 1:\n",
    "            states = states.at[t + 1].set(model.state_trans(states[t], actions[t], outcomes[t], shocks[t + 1], t=t))\n",
    "\n",
    "    # d. compute discounted utility\n",
    "    discount_factor = jnp.zeros((par.T, sim.N), dtype=dtype)\n",
    "    for t in range(par.T):\n",
    "        discount_factor = discount_factor.at[t].set(par.beta ** t)\n",
    "\n",
    "    R = jnp.sum(discount_factor * rewards) / sim.N\n",
    "\n",
    "    return R\n",
    "\n",
    "Model.simulate = simulate\n",
    "\n",
    "def simulate_loss(model,policy_NN,initial_states,shocks):\n",
    "\t\"\"\" Simulate to get objective function for policy optimization\"\"\"\n",
    "\n",
    "\t# a. unpack\n",
    "\tpar = model.par\n",
    "\ttrain = model.train\n",
    "\tdtype = model.dtype\n",
    "\tdevice = model.device\n",
    "\n",
    "\tjax.lax.stop_gradient(initial_states)\n",
    "\tjax.lax.stop_gradient(shocks)\n",
    "\n",
    "\t# b. allocate\n",
    "\tN = initial_states.shape[0]\n",
    "\tdiscount_factor = jnp.zeros((par.T,N),dtype=dtype,device=device)\t\n",
    "\treward = jnp.zeros((par.T,N),dtype=dtype,device=device)\n",
    "\tnew_states_t = jnp.zeros((N,par.Nstates),dtype=dtype,device=device)\n",
    "\n",
    "\tdef scan_step(carry, t):\n",
    "\t\tstates_t = carry\n",
    "\n",
    "\t\t# actions\n",
    "\t\tactions_t = eval_policy(model, policy_NN, states_t, t)\n",
    "\n",
    "\t\t# outcomes\n",
    "\t\toutcomes_t = model.outcomes(states_t, actions_t, t)\n",
    "\n",
    "\t\t# reward\n",
    "\t\treward_t = model.reward(states_t, actions_t, outcomes_t, t)\n",
    "\n",
    "\t\t# transition\n",
    "\t\tnext_states = model.state_trans(states_t, actions_t, outcomes_t, shocks[t], t)\n",
    "\n",
    "\t\treturn next_states, (actions_t, outcomes_t, reward_t)\n",
    "\n",
    "\tT = par.T\n",
    "\tts = jnp.arange(T)\n",
    "\n",
    "\tfinal_states, (actions_seq, outcomes_seq, rewards_seq) = jax.lax.scan(\n",
    "\t\tscan_step,\n",
    "\t\tinitial_states,   # carry\n",
    "\t\tts                # loop variable\n",
    "\t)\n",
    "\t\n",
    "\tif False:\n",
    "\t\t# c. simulate\n",
    "\t\tfor t in range(par.T):\n",
    "\n",
    "\t\t\t# i. states in period t \n",
    "\t\t\tif t > 0:\n",
    "\t\t\t\tstates_t = jnp.array(new_states_t) # states_t then doesn't share memory with new_states_t\n",
    "\t\t\telse:\n",
    "\t\t\t\tstates_t = initial_states\n",
    "\t\t\t\n",
    "\t\t\t# ii. endogenous actions\n",
    "\t\t\tactions_t = eval_policy_jit(model,policy_NN,states_t,t=t)\n",
    "\n",
    "\t\t\t# iii. reward and discount factor\n",
    "\t\t\toutcomes_t = model.outcomes(states_t,actions_t,t=t)\n",
    "\t\t\treward = reward.at[t].set(model.reward(states_t,actions_t,outcomes_t,t=t))\n",
    "\t\t\tdiscount_factor = discount_factor.at[t].set(par.beta**t)\n",
    "\n",
    "\t\t\t# iv. transition\n",
    "\t\t\tif t < par.T-1:\n",
    "\t\t\t\tnew_states_t = new_states_t.at[:,:].set(model.state_trans(states_t,actions_t,outcomes_t,shocks[t+1],t=t))\n",
    "\t\t\t\n",
    "\t# d. compute discounted utility\n",
    "\tprint(rewards_seq)\n",
    "\tR = jnp.sum(discount_factor*rewards_seq)/train.N\n",
    "\tloss = -R # - because we minimize negative reward\n",
    "\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1186aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_loss(model, policy_NN, initial_states, shocks):\n",
    "    \"\"\"Fast, JIT+SCAN version of simulate_loss\"\"\"\n",
    "\n",
    "    par = model.par\n",
    "    train = model.train\n",
    "    beta = par.beta\n",
    "    T = par.T\n",
    "\n",
    "    # Helpers (faster to bind locally)\n",
    "    eval_pol = eval_policy_jit   # Or eval_policy if not jitted\n",
    "    state_trans = model.state_trans\n",
    "    outcomes_fn = model.outcomes\n",
    "    reward_fn = model.reward\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Define scan step\n",
    "    # ------------------------------------------\n",
    "    def scan_step(states_t, t):\n",
    "        \"\"\"One period transition in simulation.\"\"\"\n",
    "\n",
    "        # Policy\n",
    "        actions_t = eval_pol(model, policy_NN, states_t, t)\n",
    "\n",
    "        # Outcomes & reward\n",
    "        outcomes_t = outcomes_fn(states_t, actions_t, t)\n",
    "        reward_t = reward_fn(states_t, actions_t, outcomes_t, t)\n",
    "\n",
    "        # Transition to next state\n",
    "        next_states = state_trans(states_t, actions_t, outcomes_t, shocks[t], t)\n",
    "\n",
    "        return next_states, (reward_t,)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Run scan over all T periods\n",
    "    # ------------------------------------------\n",
    "    ts = jnp.arange(T)\n",
    "    final_states, (reward_seq,) = jax.lax.scan(\n",
    "        scan_step, \n",
    "        initial_states,   # carry\n",
    "        ts                # loop variable\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Discount rewards\n",
    "    # ------------------------------------------\n",
    "    discounts = beta ** jnp.arange(T)\n",
    "    discounted_sum = jnp.sum(discounts[:, None] * reward_seq, axis=0)\n",
    "\n",
    "    # Average objective over training samples\n",
    "    loss = -jnp.mean(discounted_sum)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6bad1411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-1.4446996, dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.simulate = simulate\n",
    "Model.simulate_loss = simulate_loss \n",
    "\n",
    "model = Model()\n",
    "model.simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5115cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_update_step(model,loss_fn):\n",
    "\n",
    "    policy_opt = model.policy_opt\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(model.Policy_NN)\n",
    "    \n",
    "    policy_opt.update(grads)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_policy(model, K=100):\n",
    "    \"\"\" train policy function with simulation approach \"\"\"\n",
    "\n",
    "    \"\"\" \n",
    "    K: number of iterations to train policy\n",
    "    \"\"\"\n",
    "    train = model.train\n",
    "\n",
    "    np.random.seed(model.train.seed)\n",
    "    \n",
    "    # training loop\n",
    "    for k in range(K):\n",
    "\n",
    "        # i. draw initial states and shocks\n",
    "        initial_states = model.draw_initial_states(N=train.N)\n",
    "        shocks = model.draw_shocks(N=train.N)\n",
    "\n",
    "        simulate_loss_jit = jax.jit(simulate_loss, static_argnums=(0,))\n",
    "        \n",
    "        # ii. simulate loss\n",
    "        loss_fn = lambda nn: simulate_loss_jit(model, nn, initial_states, shocks)\n",
    "\n",
    "        # iii. update policy parameters\n",
    "        loss = policy_update_step(model,loss_fn)\n",
    "\n",
    "        # iv. print progress\n",
    "        if k % 10 == 0:\n",
    "            print(f\"Iteration {k}: Loss {loss.item()}\")\n",
    "\n",
    "Model.train_policy = train_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c997ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6bd475aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss 1.468570590019226\n",
      "Iteration 10: Loss 0.319608598947525\n",
      "Iteration 20: Loss 0.18654939532279968\n",
      "Iteration 30: Loss 0.12268534302711487\n",
      "Iteration 40: Loss 0.10773950070142746\n",
      "Iteration 50: Loss 0.09452074021100998\n",
      "Iteration 60: Loss 0.09294816851615906\n",
      "Iteration 70: Loss 0.09860627353191376\n",
      "Iteration 80: Loss 0.0831630602478981\n",
      "Iteration 90: Loss 0.08901738375425339\n",
      "Iteration 100: Loss 0.07833657413721085\n",
      "Iteration 110: Loss 0.08447569608688354\n",
      "Iteration 120: Loss 0.08388619124889374\n",
      "Iteration 130: Loss 0.08444564789533615\n",
      "Iteration 140: Loss 0.0844244733452797\n",
      "Iteration 150: Loss 0.07709135860204697\n",
      "Iteration 160: Loss 0.0725313350558281\n",
      "Iteration 170: Loss 0.10389026999473572\n",
      "Iteration 180: Loss 0.09708646684885025\n",
      "Iteration 190: Loss 0.09744958579540253\n",
      "Iteration 200: Loss 0.09046687930822372\n",
      "Iteration 210: Loss 0.09070216864347458\n",
      "Iteration 220: Loss 0.08374987542629242\n",
      "Iteration 230: Loss 0.0709710344672203\n",
      "Iteration 240: Loss 0.07275847345590591\n",
      "Iteration 250: Loss 0.10449371486902237\n",
      "Iteration 260: Loss 0.0861271321773529\n",
      "Iteration 270: Loss 0.09201149642467499\n",
      "Iteration 280: Loss 0.09987355768680573\n",
      "Iteration 290: Loss 0.08863455802202225\n",
      "Iteration 300: Loss 0.09574634581804276\n",
      "Iteration 310: Loss 0.09225840866565704\n",
      "Iteration 320: Loss 0.07721894234418869\n",
      "Iteration 330: Loss 0.07550168037414551\n",
      "Iteration 340: Loss 0.09252255409955978\n",
      "Iteration 350: Loss 0.10309180617332458\n",
      "Iteration 360: Loss 0.09783849865198135\n",
      "Iteration 370: Loss 0.08312990516424179\n",
      "Iteration 380: Loss 0.10196622461080551\n",
      "Iteration 390: Loss 0.07012510299682617\n",
      "Iteration 400: Loss 0.09416315704584122\n",
      "Iteration 410: Loss 0.0904296264052391\n",
      "Iteration 420: Loss 0.08209946751594543\n",
      "Iteration 430: Loss 0.08509233593940735\n",
      "Iteration 440: Loss 0.0728052631020546\n",
      "Iteration 450: Loss 0.0871468260884285\n",
      "Iteration 460: Loss 0.08477900177240372\n",
      "Iteration 470: Loss 0.10868754237890244\n",
      "Iteration 480: Loss 0.07037018984556198\n",
      "Iteration 490: Loss 0.076060950756073\n",
      "Iteration 500: Loss 0.06604958325624466\n",
      "Iteration 510: Loss 0.08032047003507614\n",
      "Iteration 520: Loss 0.06356821954250336\n",
      "Iteration 530: Loss 0.07994573563337326\n",
      "Iteration 540: Loss 0.0933828130364418\n",
      "Iteration 550: Loss 0.061472803354263306\n",
      "Iteration 560: Loss 0.07419198751449585\n",
      "Iteration 570: Loss 0.09193053841590881\n",
      "Iteration 580: Loss 0.0932074561715126\n",
      "Iteration 590: Loss 0.06575844436883926\n",
      "Iteration 600: Loss 0.07538951188325882\n",
      "Iteration 610: Loss 0.055982451885938644\n",
      "Iteration 620: Loss 0.09132281690835953\n",
      "Iteration 630: Loss 0.07147705554962158\n",
      "Iteration 640: Loss 0.10264807939529419\n",
      "Iteration 650: Loss 0.06359080225229263\n",
      "Iteration 660: Loss 0.07307484745979309\n",
      "Iteration 670: Loss 0.0845835730433464\n",
      "Iteration 680: Loss 0.09114671498537064\n",
      "Iteration 690: Loss 0.0704944059252739\n",
      "Iteration 700: Loss 0.08734964579343796\n",
      "Iteration 710: Loss 0.08198074996471405\n",
      "Iteration 720: Loss 0.06938064843416214\n",
      "Iteration 730: Loss 0.09094669669866562\n",
      "Iteration 740: Loss 0.08736314624547958\n",
      "Iteration 750: Loss 0.0595787949860096\n",
      "Iteration 760: Loss 0.0739750787615776\n",
      "Iteration 770: Loss 0.07461535185575485\n",
      "Iteration 780: Loss 0.07297877222299576\n",
      "Iteration 790: Loss 0.07572682201862335\n",
      "Iteration 800: Loss 0.08024121820926666\n",
      "Iteration 810: Loss 0.07348562031984329\n",
      "Iteration 820: Loss 0.08788829296827316\n",
      "Iteration 830: Loss 0.08378192782402039\n",
      "Iteration 840: Loss 0.08622020483016968\n",
      "Iteration 850: Loss 0.042686041444540024\n",
      "Iteration 860: Loss 0.08302237093448639\n",
      "Iteration 870: Loss 0.06177625432610512\n",
      "Iteration 880: Loss 0.068193219602108\n",
      "Iteration 890: Loss 0.07255606353282928\n",
      "Iteration 900: Loss 0.08671023696660995\n",
      "Iteration 910: Loss 0.07643895596265793\n",
      "Iteration 920: Loss 0.09792439639568329\n",
      "Iteration 930: Loss 0.07526122778654099\n",
      "Iteration 940: Loss 0.057622671127319336\n",
      "Iteration 950: Loss 0.06933081895112991\n",
      "Iteration 960: Loss 0.09272576868534088\n",
      "Iteration 970: Loss 0.06165299564599991\n",
      "Iteration 980: Loss 0.09081120043992996\n",
      "Iteration 990: Loss 0.07419034838676453\n"
     ]
    }
   ],
   "source": [
    "model.train_policy(K=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed092be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
